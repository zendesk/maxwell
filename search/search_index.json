{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is Maxwell's daemon, an application that reads MySQL binlogs and writes row updates as JSON to Kafka, Kinesis, or other streaming platforms. Maxwell has low operational overhead, requiring nothing but mysql and a place to write to. Its common use cases include ETL, cache building/expiring, metrics collection, search indexing and inter-service communication. Maxwell gives you some of the benefits of event sourcing without having to re-architect your entire platform. Download: https://github.com/zendesk/maxwell/releases/download/v1.38.0/maxwell-1.38.0.tar.gz Source: https://github.com/zendesk/maxwell mysql> insert into `test`.`maxwell` set id = 1, daemon = 'Stanislaw Lem'; maxwell: { \"database\": \"test\", \"table\": \"maxwell\", \"type\": \"insert\", \"ts\": 1449786310, \"xid\": 940752, \"commit\": true, \"data\": { \"id\":1, \"daemon\": \"Stanislaw Lem\" } } mysql> update test.maxwell set daemon = 'firebus! firebus!' where id = 1; maxwell: { \"database\": \"test\", \"table\": \"maxwell\", \"type\": \"update\", \"ts\": 1449786341, \"xid\": 940786, \"commit\": true, \"data\": {\"id\":1, \"daemon\": \"Firebus! Firebus!\"}, \"old\": {\"daemon\": \"Stanislaw Lem\"} }","title":"Home"},{"location":"bootstrapping/","text":"Bootstrapping Maxwell allows you to \"bootstrap\" data into your stream. This will perform a select * from table and output the results into your stream, allowing you to recreate your entire dataset by playing the stream from the start. Using the maxwell-bootstrap utility You can use the maxwell-bootstrap utility to begin boostrap operations from the command-line. option description --log_level LOG_LEVEL log level (DEBUG, INFO, WARN or ERROR) --user USER mysql username --password PASSWORD mysql password --host HOST mysql host --port PORT mysql port --database DATABASE mysql database containing the table to bootstrap --table TABLE mysql table to bootstrap --where WHERE_CLAUSE where clause to restrict the rows bootstrapped from the specified table --client_id CLIENT_ID specify which maxwell instance should perform the bootstrap operation --comment COMMENT arbitrary comment to be added to every bootstrap row record Starting a table bootstrap You can start a bootstrap using: bin/maxwell-bootstrap --database fooDB --table barTable Optionally, you can include a where clause to replay part of the data. bin/maxwell-bootstrap --database fooDB --table barTable --where \"my_date >= '2017-01-07 00:00:00'\" Alternatively you can insert a row in the maxwell.bootstrap table to trigger a bootstrap. mysql> insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable'); Note that if a Maxwell client_id has been set you should specify the client id. mysql> insert into maxwell.bootstrap (database_name, table_name, client_id) values ('fooDB', 'barTable', 'custom_maxwell_client_id'); You can schedule bootstrap tasks to be run in the future by setting the started_at column. Maxwell will wait until this time to start the bootstrap. mysql> insert into maxwell.bootstrap (database_name, table_name, client_id, started_at) values ('fooDB', 'barTable', 'custom_maxwell_client_id', '2020-05-18 12:30:00'); Async vs Sync bootstrapping The Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time. When running Maxwell with --bootstrapper=sync , the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete. Running Maxwell with --bootstrapper=async however, will make Maxwell spawn a separate thread for bootstrapping. In this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process. Bootstrapping Data Format a bootstrap starts with an event of type = \"bootstrap-start\" then events with type = \"bootstrap-insert\" (one per row in the table) then one event per INSERT , UPDATE or DELETE with standard event types i.e. type = \"insert\" , type = \"update\" or type = \"delete\" that occurred since the beginning of bootstrap finally an event with type = \"bootstrap-complete\" Here's a complete example: mysql> create table fooDB.barTable(txt varchar(255)); mysql> insert into fooDB.barTable (txt) values (\"hello\"), (\"bootstrap!\"); mysql> insert into maxwell.bootstrap (database_name, table_name) values (\"fooDB\", \"barTable\"); Corresponding replication stream output of table fooDB.barTable : {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"insert\",\"ts\":1450557598,\"xid\":13,\"data\":{\"txt\":\"hello\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"insert\",\"ts\":1450557598,\"xid\":13,\"data\":{\"txt\":\"bootstrap!\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-start\",\"ts\":1450557744,\"data\":{}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-insert\",\"ts\":1450557744,\"data\":{\"txt\":\"hello\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-insert\",\"ts\":1450557744,\"data\":{\"txt\":\"bootstrap!\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-complete\",\"ts\":1450557744,\"data\":{}} Failure Scenarios If Maxwell crashes during bootstrapping the next time it runs it will rerun the bootstrap in its entirety - regardless of previous progress. If this behavior is not desired, manual updates to the bootstrap table are required. Specifically, marking the unfinished bootstrap row as 'complete' ( is_complete = 1) or deleting the row. jQuery(document).ready(function () { jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\"); });","title":"Bootstrapping"},{"location":"bootstrapping/#bootstrapping","text":"Maxwell allows you to \"bootstrap\" data into your stream. This will perform a select * from table and output the results into your stream, allowing you to recreate your entire dataset by playing the stream from the start.","title":"Bootstrapping"},{"location":"bootstrapping/#using-the-maxwell-bootstrap-utility","text":"You can use the maxwell-bootstrap utility to begin boostrap operations from the command-line. option description --log_level LOG_LEVEL log level (DEBUG, INFO, WARN or ERROR) --user USER mysql username --password PASSWORD mysql password --host HOST mysql host --port PORT mysql port --database DATABASE mysql database containing the table to bootstrap --table TABLE mysql table to bootstrap --where WHERE_CLAUSE where clause to restrict the rows bootstrapped from the specified table --client_id CLIENT_ID specify which maxwell instance should perform the bootstrap operation --comment COMMENT arbitrary comment to be added to every bootstrap row record","title":"Using the maxwell-bootstrap utility"},{"location":"bootstrapping/#starting-a-table-bootstrap","text":"You can start a bootstrap using: bin/maxwell-bootstrap --database fooDB --table barTable Optionally, you can include a where clause to replay part of the data. bin/maxwell-bootstrap --database fooDB --table barTable --where \"my_date >= '2017-01-07 00:00:00'\" Alternatively you can insert a row in the maxwell.bootstrap table to trigger a bootstrap. mysql> insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable'); Note that if a Maxwell client_id has been set you should specify the client id. mysql> insert into maxwell.bootstrap (database_name, table_name, client_id) values ('fooDB', 'barTable', 'custom_maxwell_client_id'); You can schedule bootstrap tasks to be run in the future by setting the started_at column. Maxwell will wait until this time to start the bootstrap. mysql> insert into maxwell.bootstrap (database_name, table_name, client_id, started_at) values ('fooDB', 'barTable', 'custom_maxwell_client_id', '2020-05-18 12:30:00');","title":"Starting a table bootstrap"},{"location":"bootstrapping/#async-vs-sync-bootstrapping","text":"The Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time. When running Maxwell with --bootstrapper=sync , the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete. Running Maxwell with --bootstrapper=async however, will make Maxwell spawn a separate thread for bootstrapping. In this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process.","title":"Async vs Sync bootstrapping"},{"location":"bootstrapping/#bootstrapping-data-format","text":"a bootstrap starts with an event of type = \"bootstrap-start\" then events with type = \"bootstrap-insert\" (one per row in the table) then one event per INSERT , UPDATE or DELETE with standard event types i.e. type = \"insert\" , type = \"update\" or type = \"delete\" that occurred since the beginning of bootstrap finally an event with type = \"bootstrap-complete\" Here's a complete example: mysql> create table fooDB.barTable(txt varchar(255)); mysql> insert into fooDB.barTable (txt) values (\"hello\"), (\"bootstrap!\"); mysql> insert into maxwell.bootstrap (database_name, table_name) values (\"fooDB\", \"barTable\"); Corresponding replication stream output of table fooDB.barTable : {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"insert\",\"ts\":1450557598,\"xid\":13,\"data\":{\"txt\":\"hello\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"insert\",\"ts\":1450557598,\"xid\":13,\"data\":{\"txt\":\"bootstrap!\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-start\",\"ts\":1450557744,\"data\":{}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-insert\",\"ts\":1450557744,\"data\":{\"txt\":\"hello\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-insert\",\"ts\":1450557744,\"data\":{\"txt\":\"bootstrap!\"}} {\"database\":\"fooDB\",\"table\":\"barTable\",\"type\":\"bootstrap-complete\",\"ts\":1450557744,\"data\":{}}","title":"Bootstrapping Data Format"},{"location":"bootstrapping/#failure-scenarios","text":"If Maxwell crashes during bootstrapping the next time it runs it will rerun the bootstrap in its entirety - regardless of previous progress. If this behavior is not desired, manual updates to the bootstrap table are required. Specifically, marking the unfinished bootstrap row as 'complete' ( is_complete = 1) or deleting the row. jQuery(document).ready(function () { jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\"); });","title":"Failure Scenarios"},{"location":"changelog/","text":"Maxwell changelog v1.38.0 Maxwell gets the ability to talk to bigtable! I have no idea how well it'll work. I hope it works for you! upgrade protobuf to fix a rabbitmq issue with booleans, I think. rabbitMQ timeouts on connection other fixes. I can't imagine the security department cares about my naming what with what's going on inside 1019. I guess we'll see. - Released 2022-07-29 v1.37.7 Bump viafoura/metrics-datadog 2.0.0-RC3 Released 2022-06-21 v1.37.6 In non-GTID mode, Verify that the master's server hasn't changed out from underneath us. thanks Tamin Khan Released 2022-05-12 v1.37.5 Upgrade binlog-replicator. pulls in some minor fixes. Released 2022-04-16 v1.37.4 configure custom producer via environment sns and sqs producers take output config properly Released 2022-04-08 v1.37.3 fixes for mariadb Released 2022-03-25 v1.37.2 configurable binlog event queue size Released 2022-03-14 v1.37.1 list changes Released 2022-03-07 v1.37.0 Change max size of RowMap buffer to unblock high-efficiency producers Released 2022-01-26 v1.36.0 fix bug where the millionth binlog would kinda sort \"overflow\" and the binlog positions would stop moving. My benefactor here asked that I stopped creating cute release names. The security department, mysteriously. Released 2022-01-23 v1.35.5 log4j, again and agian. Released 2021-12-29 v1.35.4 log4j turns 2.17.0, happy birthday Released 2021-12-18 v1.35.3 log4j vulnerability #2 Released 2021-12-15 v1.35.2 better logging when we can't connect on startup Released 2021-12-12 v1.35.1 log4j upgrade to upgrade past the giant security hole Released 2021-12-10 v1.35.0 couple of parser fixes docker builds are now multi-platform replication_reconnection_retries configuration option quote table names in bootstrapper properly Released 2021-11-30 v1.34.1 support for mysql 8's visible/invisible columns support mariadb's if-exists/if-not-exists for partition management add an index for the http endpoint Released 2021-09-21 v1.34.0 intern a bunch of objects in our in-memory representation of schema. Saves gobs of memory in cases where one has N copies of the same database. Note that this changes the API of Columns, should any embedded Maxwell application be using that. go up to BIGINT for maxwell's auto-increment ids Released 2021-07-29 v1.33.1 properties may now be fetched from a javascript blob in the env RowMap provides access to primary keys fix an odd NPE in mariaDB init Released 2021-06-02 v1.33.0 Add HTTP endpoint for runtime reconfiguration Released 2021-03-29 v1.32.0 Amazon SNS producer added, thanks Rober Wittman kafka 2.7.0 supported stackdriver metrics logging available Released 2021-03-17 v1.31.0 Add producer for NATS streaming server Released 2021-02-11 v1.30.0 support server-sent heartbeating on the binlog connection via --binlog-heartbeat can connect to rabbitmq by URL, supports SSL connections fix parser bug with multiline SQL target JDK 11 -- we have dropped support for JDK 8 ability to send a microsecond timestamp via --output_push_timestamp fixes for odd azure mysql connection failures Released 2021-02-05 v1.29.2 fix for terrible performance regression in bootstrapping Released 2021-01-27 v1.29.1 small bugfix release, fixes binlog event type processing in mysql 8 Released 2020-12-23 v1.29.0 High Availability support via jgroups-raft rework --help text Released 2020-12-15 v1.28.2 fix for encryption parsing error on table creation some logging around memory usage in RowMapBuffer Released 2020-12-02 v1.28.1 fix http server issue in 1.28.0 Released 2020-11-25 v1.28.0 schema compaction! with the new --max_schemas option, maxwell will periodically roll up the maxwell . schemas table, preventing it from growing infinitely long. fix metricsAgeSloMS calculation support SRID columns fix parsing of complex INDEX(CAST()) statements various dependency bumps Released 2020-11-19 v1.27.1 redis producer gets sentinal support fix a double-reconnect race condition file producer honors javascript row-suppression better error messaging when we lack REPLICATION SLAVE privs miscellaneous dependency bumps Released 2020-08-07 v1.27.0 better support for empty/null passwords allow bootstrap utility to query replication_host a few library upgrades, notably pubsub and kinesis library bootstrap connection uses jdbc_options properly add logging for when we hit out of sync schema exceptions allow for partitioning by thread_id, thx @gogov fresh and clean documentation Released 2020-06-30 v1.26.4 support now() function with precision Released 2020-06-08 v1.26.3 use pooled redis connections, fixes corruption when redis was accessed from multiple threads (bootstrap/producer), thanks @lucastex fix date handling of '0000-01-01' fix race condition in binlog reconnect logic Released 2020-05-26 v1.26.2 bootstraps can be scheduled in the future by setting the started_at column, thanks @lucastex two mysql 8 fixes; one for a DEFAULT(function()) parse error, one for supporting DEFAULT ENCRYPTION Released 2020-05-18 v1.26.1 fixes for redis re-connection login, thanks much @lucastex Released 2020-05-07 v1.26.0 We now support mysql 8's caching_sha2_password authentication scheme support for converting JSON field names to camelCase Released 2020-05-06 v1.25.3 fixes memory leak in mysql-binlog-connector fixes exceptions that occur when a connection passes wait_timeout Released 2020-05-02 v1.25.2 Fixes for a long standing JSON bug in 8.0.19+ Released 2020-05-01 v1.25.1 issue #1457, ALTER DATABASE with implicit database name maxwell now runs on JDK 11 in docker exit with status 2 when we can't find binlog files Released 2020-04-22 v1.25.0 swap un-maintained snaq.db with C3P0. support eu datadog metrics protect against lost connections during key queries (bootstrapping, heartbeats, postition setting) Released 2020-03-29 v1.24.2 bugfix parsing errors: compressed columns, exchange partitions, parenthesis-enclosed default values, drop column foo.t . add partition-by-random feature. update jackson-databind to get security patch fix redis channel interpolation on RPUSH Released 2020-03-25 v1.24.1 allow jdbc_options on secondary connections fix a crash in bootstrapping / javascript filters fix a regression in message.publish.age metric Released 2020-01-21 v1.24.0 add comments field to bootstrapping, thanks Tom Collins fix sql bug with #comments style comments Released 2019-12-14 v1.23.5 Update bootstrap documentation Bump drop wizard metrics to support Java versions 10+ Released 2019-12-12 v1.23.4 Bump and override dependencies to fix security vulnerabilities. Update redis-key config options list changes Released 2019-12-03 v1.23.3 pubsubDelayMultiplier may now be 1.0 allow %{database} and %{topic} interpolation into redis producer docs updates setup default client_id in maxwell-bootstrap util Released 2019-11-21 v1.23.2 upgrade jackson stop passing maxwell rows through the JS filter. too dangerous. Released 2019-10-18 v1.23.1 Add option for XADD (redis streams) operation Add configuration flag for tuning transaction buffer memory sectionalize help text Released 2019-10-12 v1.23.0 Added AWS FIFO support Add retry and batch settings to pubs producer Add support for age SLO metrics Released 2019-10-08 v1.22.6 upgrade mysql-connector-java to 8.0.17 use a newer docker image as base list changes Released 2019-09-20 v1.22.5 bugfix for bootstrapping off a split replica that doesn't contain a \"maxwell\" database Fix a parser issue with db.table.column style column names Released 2019-09-06 v1.22.4 Add row type to fallback message Upgrade jackson-databind Released 2019-08-23 v1.22.3 fix issue with google pubsub in 1.22.2 Released 2019-06-20 v1.22.2 fix an issue with bootstrapping-on-replicas add --output_primary_keys and --output_primary_key_columns fix a very minor memory leak with blacklists Released 2019-06-18 v1.22.1 fix crash in rabbit-mq producer better support for maxwell + azure-mysql remove bogus different-host bootstrap check some security upgrades Released 2019-05-28 v1.22.0 Bootstrapping has been reworked and is now available in all setups, including those in which the maxwell store is split from the replicator. cleanup and fix a deadlock in the kafka fallback queue logic add .partition_string = to javascript filters Released 2019-04-16 v1.21.1 Upgrade binlog connector. Should fix issues around deserialization errors. Released 2019-03-29 v1.21.0 Bootstrapping output no longer contain binlog positions. Please update any code that relies on this. Fix 3 parser issues. Released 2019-03-23 v1.20.0 add support for partitioning by transaction ID thx @hexene add support for a kafka \"fallback\" topic to write to when a message fails to write add UJIS charset support parser bug: multiple strings concatenate to make one default string parser bug: deal with bizarre column renames which are then referenced in AFTER column statements Released 2019-02-28 v1.19.7 fix a parser error with empty sql comments interpret latin-1 as windows-1252, not iso-whatever, thx @borleaandrei Released 2019-01-25 v1.19.6 Further fixes for GTID-reconnection issues. Crash sanely when GTID-enabled maxwell is connected to clearly the wrong master, thanks @acampoh Released 2019-01-20 v1.19.5 Fixes for unreliable connections wrt to GTID events; previously we restart in any old position, now we throw away the current transaction and restart the replicator again at the head of the GTID event. Released 2019-01-15 v1.19.4 Fixes for a maxwell database not making it through the blacklist Add output_null_zerodates parameter to control how we treat '0000-00-00' Released 2019-01-12 v1.19.3 Add a universal backpressure mechanism. This should help people who were running into out-of-memory situations while bootstrapping. Released 2018-12-19 v1.19.2 Include schema_id in bootstrap events add more logging around binlog connector losing connection add retry logic to redis some aws fixes allow pushing JS hashes/arrays into data from js filters list changes Released 2018-12-02 v1.19.1 Handle mysql bit literals in DEFAULT statements blacklist out CREATE ROLE etc upgrade dependencies to pick up security issues Released 2018-11-12 v1.19.0 mysql 8 support! utf8 enum values are supported now fix #1125, bootstrapping issue for TINYINT(1) fix #1145, nasty bug around SQL blacklist and columns starting with \"begin\" only resume bootstraps that are targeted at this client_id fixes for blacklists and heartbeats. Did I ever mention blacklists are a terrible idea? Released 2018-10-27 v1.18.0 memory optimizations for large schemas (especially shareded schemas with lots of duplicates) add support for an http endpoint to support Prometheus metrics allow javascript filters to access the row query object javascript filters now run in the bootstrap process support for non-latin1 column names add --output_schema_id option better handling of packet-too-big errors from Kinesis add message.publish.age metric Released 2018-09-15 v1.17.1 fix a regression around filters + bootstrapping fix a regression around filters + database-only-ddl Released 2018-07-03 v1.17.0 v1.17.0 brings a new level of configurability by allowing you to inject a bit of javascript into maxwell's processing. Should be useful! Also: fix regression for Alibaba RDS tables Released 2018-06-28 v1.16.1 Fix Bootstrapping for JSON columns add --recapture_schema flag for when ya wanna start over add kafka 1.0 libraries, make them default Released 2018-06-21 v1.16.0 v1.16.0 brings a rewrite of Maxwell's filtering system, giving it a concise list of rules that are executed in sequence. It's now possible to exclude tables from a particular database, exclude columns matching a value, and probably some other use cases. See http://maxwells-daemon.io/config/#filtering for details. Released 2018-06-15 v1.15.0 This is a bug-fix release, but it's big enough I'm giving it a minor version. Fix a very old bug in which DDL rows were writing the start of the row into maxwell.positions , leading to chaos in some scenarios where maxwell managed to stop on the row and double-process it, as well as to a few well-meaning patches. Fix the fact that maxwell was outputting \"next-position\" instead of \"position\" of a row into JSON. Fix the master-recovery code to store schema that corresponds to the start of a row, and points the replicator at the next-position. Much thanks to Tim, Likun and others in sorting this mess out. Released 2018-06-13 v1.14.7 add RowMap#getRowQuery, thx @saimon7 revert alpine-linux docker image fiasco fix RawJSONString not serializable, thx @niuhaifeng Released 2018-06-03 v1.14.6 Fix docker image Released 2018-05-15 v1.14.5 reduce docker image footprint add benchmarking framework performance improvements for date/datetime columns fix parser error on UPGRADE PARTITIONING Released 2018-05-15 v1.14.4 Fix race condition in SchemaCapturer Released 2018-05-07 v1.14.3 Enable jvm metrics Released 2018-05-04 v1.14.2 fix regression in 1.14.1 around bootstrapping host detection fix heartbeating code around table includes Released 2018-05-02 v1.14.1 bootstraps can now take a client_id improved config validation for embedded mode Released 2018-05-01 v1.14.0 new feature --output_xoffset to uniquely identify rows within transactions, thx Jens Gyti Bug fixes around \"0000-00-00\" times. Bug fixes around dates pre 1000 AD Released 2018-04-24 v1.13.5 Support environment variable based configuration Released 2018-04-11 v1.13.4 Added possibility to do not declare the rabbitmq exchange. Released 2018-04-03 v1.13.3 Add logging for binlog errors Maven warning fix Do not include current position DDL schema to avoid processing DDL twice Always write null fields in primary key fields Bugfix: fix http_path_prefix command line option issue Released 2018-04-03 v1.13.2 fix a bug with CHARACTER SET = DEFAULT maxwell now eclipse-friendly. configurable bind-address for maxwell's http server Released 2018-03-06 v1.13.1 redis producer now supports LPUSH, thx @m-denton RowMap can now contain artbitrary attributes for embedded maxwell, thx @jkgeyti bugfix: fix jdbc option parsing when value contains = bugfix: apparently the SQS producer was disabled bugfix: fix a situation where adding a second client could cause schemas to become out of sync support for --daemon Released 2018-02-20 v1.13.0 proper SSL connection support, thanks @cadams5 support for including original SQL in insert/update/deletes, thanks @saimon7 fixes for float4, float8 and other non-mysql datatypes bump kinesis lib to 0.12.8 fix for bug when two databases share a single table Released 2018-02-01 v1.12.0 Support for injecting a custom producer, thanks @tomcollinsproject New producer for Amazon SQS, thanks @vikrant2mahajan Maxwell can now filter rows based on column values, thanks @finnplay Fixes for the Google Pubsub producer (it was really broken), thanks @finnplay DDL output can now optionally include the source SQL, thanks @sungjuly Support for double-quoted table/database/etc names rabbitmq option for persistent messages, thanks @d-babiak SQL parser bugfix for values like +1.234, thanks @hexene Released 2018-01-09 v1.11.0 - default kafka client upgrades to 0.11.0.1 - fix the encryption issue (https://github.com/zendesk/maxwell/issues/803) Released 2017-11-22 v1.10.9 We recommend all v1.10.7 and v1.10.8 users upgrade to v1.10.9. Add missing Kafka clients Listen and report on binlog connector lifecycle events for better visibility Reduce docker image size Released 2017-10-30 v1.10.8 Fix docker builds Add Google Cloud Pub/Sub producer RabbitMQ producer enhancements Released 2017-10-12 v1.10.7 Java 8 upgrade Diagnostic health check endpoint Encryption Documentation update: encryption, kinesis producer, schema storage fundamentals, etc. Released 2017-10-11 v1.10.6 Binlog-connector upgrade Bug-fix: when using literal string for an option that accepts Regex, Regex characters are no longer special If master recovery is enabled, Maxwell cleans up old positions for the same server and client id Released 2017-08-14 v1.10.5 Shyko's binlog-connector is now the default and only replication backend available for maxwell. Released 2017-07-25 v1.10.4 Notable changes: Shutdown hardening. If maxwell can't shut down (because the kafka producer is in a bad state and close() never terminates, for example), it would previously stall and process no messages. Now, shutdown is run in a separate thread and there is an additional watchdog thread which forcibly kills the maxwell process if it can't shut down within 10 seconds. Initial support for running maxwell from java, rather then as its own process. This mode of operation is still experimental, but we'll accept PRs to improve it (thanks Geoff Lywood). Fix incorrect handling of negative (pre-epoch dates) when using binlog_connector mode (thanks Geoff Lywood). Released 2017-07-10 v1.10.3 tiny release to fix a units error in the replication.lag metric (subtracting seconds from milliseconds) Released 2017-06-06 v1.10.2 added metrics: \"replication.queue.time\" and \"inflightmessages.count\" renamed \"time.overall\" metric to \"message.publish.time\" documentation updates (thanks Chintan Tank) Released 2017-06-04 v1.10.1 The observable changes in this minor release are a new configuration for Kafka/Kinesis producer to abort processing on publish errors, and support of Kafka 0.10.2. Also a bunch of good refactoring has been done for heartbeat processing. List of changes: Support Kafka 0.10.2 Stop procesing RDS hearbeats Keep maxwell heartbeat going every 10 seconds when database is quiet Allow for empty double-quoted string literals for database schema changes Ignore Kafka/Kinesis producer errors based on new configuration ignore_producer_error Released 2017-05-26 v1.10.0 This is a small release, primarily around a change to how schemas are stored. Maxwell now stores the last_heartbeat_read with each entry in the schemas table, making schema management more resilient to cases where binlog numbers are reused, but means that you must take care if you need to roll back to an earlier version. If you deploy v1.10.0, then roll back to an earlier version, you should manually update all schemas . last_heartbeat_read values to 0 before redeploying v1.10.0 or higher. Other minor changes: allow negative default numbers in columns only store final binlog position if it has changed blacklist internal aurora table `rds_heartbeat*' log4j version bump (allows for one entry per line JSON logging) Released 2017-05-09 v1.9.0 Maxwell 1.9 adds one main feature: monitoring support, contributed by Scott Ferguson. Multiple backends can be configured, read the updated docs for full details. There's also some bugfixes: filter DDL messages based on config determine newest schema from binlog order, not creation order add task manager to shutdown cleanly on error minor logging improvements Released 2017-04-26 v1.8.2 Bugfix release. maxwell would crash on a quoted partition name fixes for alters on non-string tables containing VARCHAR use seconds instead of milliseconds for DDL messages Released 2017-04-11 v1.8.1 performance improves in capturing and restoring schema, thx Joren Minnaert Allow for capturing from a separate mysql host (adds support for using Maxscale as a replication proxy), thx Adam Szkoda Released 2017-02-20 v1.8.0 In version 1.8.0 Maxwell gains alpha support for GTID-based positions! All praise due to Henry Cai. Released 2017-02-14 v1.7.2 Fix a bug found where maxwell could cache the wrong TABLE_MAP_ID for a binlog event, leading to crashes or in some cases data mismatches. Released 2017-01-30 v1.7.1 bootstrapping now can take a --where clause performance improvements in the kafka producer Released 2017-01-24 v1.7.0 Maxwell 1.7 brings 2 major new, alpha features. The first is Mysql 5.7 support, including JSON column type support and handling of 5.7 SQL, but not including GTID support yet. This is based on porting Maxwell to Stanley Shyko's binlog-connector library. Thanks to Stanley for his amazing support doing this port. The second major new feature is a producer for Amazon's Kinesis streams, This was contributed in full by the dogged and persistent Thomas Dziedzic. Check it out with --producer=kinesis . There's also some bugfixes: - Amazon RDS heartbeat events now tick maxwell's position, thx Scott Ferguson - allow CHECK() statements inside column definitions Released 2017-01-07 v1.6.0 This is mostly a bugfix release, but it gets a minor version bump due to a single change of behavior: dates and timestamps which mysql may accept, but are considered invalid (0000-00-00 is a notable example) previously had inconsistent behavior. Now we convert these to NULL. Other bugfixes: - heartbeats have moved into their own table - more fixes around alibaba rds - ignore DELETE statements that are output for MEMORY tables upon server restart - allow pointing maxwell to a pre-existing database Released 2016-12-29 v1.5.2 add support for kafka 0.10.1 @ smferguson master recovery: cleanup positions from previous master; prevent errors on flip-back. fix a bug that would trigger in certain cases when dropping a column that was part of the primary-key Released 2016-12-07 v1.5.1 This is a bugfix release. - fixes for bootstrapping with an alternative maxwell-schema name and an include_database filter, thanks Lucian Jones - fixes for kafka 0.10 with lz4 compression, thanks Scott Ferguson - ignore the RDS table mysql.ha_health_check table - Get the bootstrapping process to output NULL values. - fix a quoting issue in the bootstrap code, thanks @mylesjao. Released 2016-11-24 v1.5.0 CHANGE: Kafka producer no longer ships with hard-coded defaults. Please ensure you have \"compression.type\", \"metadata.fetch.timeout.ms\", and \"retries\" configured to your liking. bugfix: fix a regression in handling ALTER TABLE change c int after b statements warn on servers with missing server_id Released 2016-11-07 v1.4.2 kafka 0.10.0 support, as well as a re-working of the --kafka_version command line option. Released 2016-11-01 v1.4.1 support per-table topics, Thanks @smferguson and @sschatts. fix a parser issue with DROP COLUMN CASCADE, thanks @smferguson Released 2016-10-27 v1.4.0 1.4.0 brings us two nice new features: - partition-by-column: see --kafka_partition_columns. Thanks @smferguson - output schema changes as JSON: see --output_ddl. Thanks @xmlking - As well as a fix around race conditions on shutdown. Released 2016-10-21 v1.3.0 support for fractional DATETIME, TIME, TIMESTAMP columns, thanks @Dagnan support for outputting server_id & thread_id, thanks @sagiba fix a race condition in bootstrap support Released 2016-10-03 v1.2.2 Maxwell will now include by default fields with NULL values (as null fields). To disable this and restore the old functionality where fields were omitted, pass --output_nulls=false Fix an issue with multi-client support where two replicators would ping-pong heartbeats at each other Fix an issue where a client would attempt to recover a position from a mismatched client_id Fix a bug when using CHANGE COLUMN on a primary key Released 2016-09-23 v1.2.1 This is a bugfix release. - fix a parser bug around ALTER TABLE CHARACTER SET - fix bin/maxwell to pull in the proper version of the kafka-clients library Released 2016-09-15 v1.2.0 1.2.0 is a major release of Maxwell that introduces master recovery features; when a slave is promoted to master, Maxwell is now capable of recovering the position. See the --master_recovery flag for more details. It also upgrades the kafka producer library to 0.9. If you're using maxwell with a kafka 0.8 server, you must now pass the --kafka0.8 flag to maxwell. Released 2016-09-12 v1.1.6 minor bugfix in which maxwell with --replay mode was trying to write heartbeats Released 2016-09-07 v1.1.5 @dadah89 adds --output_binlog_position to optionally output the position with the row @dadah89 adds --output_commit_info to turn off xid/commit fields maxwell now supports tables with partitions maxwell now supports N maxwells per-server. see the client_id / replica_server_id options. two parser fixes, for engine= innodb and CHARSET ASCII lay the ground work for doing master recovery; we add a heartbeat into the positions table that we can co-ordinate around. Released 2016-09-04 v1.1.4 add support for a bunch more charsets (gbk, big5, notably) fix Maxwell's handling of kafka errors - previously we were trying to crash Maxwell by throwing a RuntimeException out of the Kafka Producer, but this was a failure. Now we log and skip all errors. Released 2016-08-05 v1.1.3 This is a bugfix release, which fixes: - https://github.com/zendesk/maxwell/issues/376, a problem parsing RENAME INDEX - https://github.com/zendesk/maxwell/issues/371, a problem with the SERIAL datatype - https://github.com/zendesk/maxwell/issues/362, we now preserve the original casing of columns - https://github.com/zendesk/maxwell/issues/373, we were incorrectly expecting heartbeats to work under 5.1 Released 2016-07-14 v1.1.2 pick up latest mysql-connector-j, fixes #369 fix an issue where maxwell could skip ahead positions if a leader failed. rework buffering code to be much kinder to the GC and JVM heap in case of very large transactions / rows inside transactions kinder, gentler help text when you specify an option incorrectly Released 2016-06-27 v1.1.1 fixes a race condition setting the binlog position that would get maxwell stuck Released 2016-05-23 v1.1.0 much more efficient processing of schema updates storage, especially when dealing with large schemas. @lileeyao added --exclude-columns and the --jdbc_options features @lileeyao added --jdbc_options can now blacklist entire databases new kafka key format available, using a JSON array instead of an object bugfix: unsigned integer columns were captured incorrectly. 1.1 will recapture the schema and attempt to correct the error. Released 2016-05-20 v1.1.0-pre4 Eddie McLean gives some helpful patches around bootstrapping Bugfixes for the patch-up-the-schema code around unsigned ints Released 2016-05-06 v1.1.0-pre3 forgot to include some updates that back-patch unsigned column problems Released 2016-05-05 v1.1.0-pre2 fix performance issues when capturing schema in AWS Aurora fix a bug in capturing unsigned integer columns Released 2016-05-04 v1.0.1 fixes a parsing bug with CURRENT_TIMESTAMP() Released 2016-04-12 v1.0.0 Since v0.17.0, Maxwell has gotten: - bootstrapping support - blacklisting for tables - flexible kafka partitioning - replication heartbeats - GEOMETRY columns - a whole lotta lotta bugfixes and I, Osheroff, think the damn thing is stable enough for a 1.0. So there. Released 2016-03-11 v1.0.0-RC3 pull in support for replication heartbeats. helps in the flakier network environs. Released 2016-03-08 v1.0.0-RC2 fixes the way ALTER DATABASE charset= was handled adds proper handling of ALTER TABLE CONVERT TO CHARSET Released 2016-02-20 v1.0.0-RC1 modifications to the way the bootstrap utility works fix a race condition crash bug in bootstrapping fix a parser bug Released 2016-02-11 v1.0.0-PRE2 1.0.0-PRE2 brings in a lot of changes that got merged while we were testing out PRE1. so, hey. - Configurable names for the maxwell schema database (Kristian Kaufman) - Configurable key (primary key, id, database) into the kafka partition hash function (Kristian Kaufman) - Configurable Kafka partition hash function (java hashCode, murmur3) (Kristian Kaufman) - support GEOMETRY columns, output as well-known-text - add --blacklist_tables option to fully ignore excessive schema changes (Nicolas Maquet) - bootstrap rows now have 'bootstrap-insert' type Released 2016-01-30 v1.0.0-PRE1 Here we have the preview release of @nmaquet's excellent work around bootstrapping initial versions of mysql tables. Released 2016-01-09 v0.17.0 v0.17 is a large bugfix release with one new feature. - FEATURE: allow specifying an alternative mysql schema-storage server and replication server - BUGFIX: properly handle case-sensitivity by aping the behavior of the master server. Fixes #230. - BUGFIX: parse some forms of CHECK( ... ) statements. Fixes #203. - BUGFIX: many more SQL-parser fixes. We are mostly through some thousands of lines of SQL produced by mysql-test. Released 2016-01-07 v0.16.2 This is a large-ish bugfix release. - Support, with reservations, binlog_row_image=MINIMAL - parser bug: handle table names that look like floating points - parser bug: fix for entity names that have '.', '\\', etc in them - handle UPPERCASE encoding names - support UCS2 (start trying to operate ok on the mysql-test suite) - use ObjectOutputStream.reset to fix memory leaks when buffering to disk Released 2015-12-16 v0.16.1 This is a bug-fix-roundup release: - support ALTER DATABASE - fix a bunch of parse errors: we've started running mysql-test at maxwell and are fixing up failures. - some modifications to the overflow-to-disk logic; we buffer the input and output, and we fix a memory leak Released 2015-12-11 v0.16.0 Version 0.16.0 introduces a feature where UPDATE statements will now show both the new row image and the old values of the fields that changed. Thanks @kristiankaufmann Released 2015-12-10 v0.15.0 fix a parse problem with indices ordered by ASC/DESC Released 2015-12-07 v0.15.0-RC1 large transactions now buffer to disk instead of crushing maxwell. support ALGORITHM=[algo], LOCK=[lock] for 5.6 alters Released 2015-12-04 v0.14.6 fix TIME column support fix parsing on millisecond precision column defintions fix CREATE SCHEMA parsing Released 2015-11-27 v0.14.5 handle BOOLEAN columns with true/false defaults Released 2015-11-25 v0.14.4 fixes parsing of \"mysql comments\" ( /*! .. */ ) More performance improvements, another 10% in a tight loop. Released 2015-11-24 v0.14.3 fixes a regression in 0.14.2 that creates duplicate copies of the \"mysql\" database in the schema. Released 2015-11-23 v0.14.2 capture the mysql database along with the rest of the schema. Eliding it was a bad premature optimization that led to crashes when tables in the mysql database changed. Released 2015-11-20 v0.14.1 fixes a parser bug around named PRIMARY KEYs. Released 2015-11-17 v0.14.0 This release introduces row filters, allowing you to include or exclude tables from maxwell's output based on names or regular expressions. Released 2015-11-03 v0.13.1 v0.13.1 is a bug fix of v0.13.0 -- fixes a bug where long rows were truncated. v0.13.0 contains: - Big performance boost for maxwell: 75% faster in some benchmarks - @davidsheldon contributed some nice bug fixes around CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema. - we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas. Released 2015-10-29 v0.13.0 Lucky release number 13 brings some reasonably big changes: - Big performance boost for maxwell: 75% faster in some benchmarks - @davidsheldon contributed some nice bug fixes around CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema. - we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas. This release has a pretty bad bug. do not use. Released 2015-10-29 v0.12.0 add support for BIT columns. Released 2015-10-16 v0.11.4 this is another bugfix release that fixes a problem where the replication thread can die in the middle of processing a transaction event. I really need to fix this at a lower level, ie the open-replicator level. Released 2015-09-30 v0.11.3 this is a bugfix release: - fix problems with table creation options inside alter statements ( ALTER TABLE foo auto_increment=10 ) - fix a host of shutdown-procedure bugs the test suite should also be way more reliable, not like you care. Released 2015-09-29 v0.11.2 This is a bugfix release. It includes: - soft deletions of maxwell.schemas to fix A->B->A master swapping without creating intense replication delay - detect and fail early if we see binlog_row_image=minimal - kill off maxwell if the position thread dies - fix a bug where maxwell could pick up a copy of schema from a different server_id (curse you operator precedence!) Released 2015-09-18 v0.11.1 maxwell gets a very minimal pass at detecting when a master has changed, in which it will kill off schemas and positions from a server_id that no longer is valid. this should prevent the worst of cases. Released 2015-09-16 v0.11.0 This release of Maxwell preserves transaction information in the kafka stream by adding a xid key in the JSON object, as well as a commit key for the final row inside the transaction. It also contains a bugfix around server_id handling. Released 2015-09-15 v0.10.1 proper support for BLOB, BINARY, VARBINARY columns (base 64 encoded) fix a problem with the SQL parser where specifying encoding or collation in a string column in the wrong order would crash make table option parsing more lenient Released 2015-09-11 v0.11.0-RC1 merge master fixes Released 2015-09-09 v0.11.0-PRE4 bugfix on v0.11.0-PRE3 Released 2015-09-09 v0.10.0 Mysql 5.6 checksum support! some more bugfixes with the SQL parser Released 2015-09-09 v0.11.0-PRE3 handle SAVEPOINT within transactions downgrade unhandled SQL to a warning Released 2015-09-08 v0.11.0-PRE2 fixes for myISAM \"transactions\" Released 2015-09-03 v0.11.0-PRE1 fix a server_id bug (was always 1 in maxwell.schemas) JSON output now includes transaction IDs Released 2015-09-02 v0.10.0-RC4 deal with BINARY flag in string column creation. Released 2015-08-31 v0.9.5 handle the BINARY flag in column creation Released 2015-08-31 v0.10.0-RC3 handle \"TRUNCATE [TABLE_NAME]\" statements Released 2015-08-27 v0.10.0-RC2 fixes a bug with checksum processing. Released 2015-08-26 v0.10.0-RC1 upgrade to open-replicator 1.3.0-RC1, which brings binlog checksum (and thus easy 5.6.1) support to maxwell. Released 2015-08-04 v0.9.4 allow a configurable number (including unlimited) of schemas to be stored Released 2015-07-27 v0.9.3 bump open-replicator to 1.2.3, which allows processing of single rows greater than 2^24 bytes Released 2015-07-14 v0.9.2 bump open-replicator buffer to 50mb by default log to STDERR, not STDOUT --output_file option for file producer Released 2015-07-10 v0.9.1 Maxwell is now aware that column names are case-insenstive fix a nasty bug in which maxwell would store the wrong position after it lost its connection to the master. Released 2015-06-22 v0.9.0 Also, vanchi is so paranoid he's worried immediately about this. mysql 5.6 support (without checksum support, yet) fix a bunch of miscellaneous bugs @akshayi1 found (REAL, BOOL, BOOLEAN types, TRUNCATE TABLE) Released 2015-06-18 v0.8.1 minor bugfix release around mysql connections going away. Released 2015-06-16 v0.8.0 add \"ts\" field to row output add --config option for passing a different config file support int1, int2, int4, int8 columns Released 2015-06-09 v0.7.2 handle inline sql comments ignore more user management SQL Released 2015-05-29 v0.7.1 only keep 5 most recent schemas Released 2015-05-15 v0.7.0 handle CURRENT_TIMESTAMP parsing properly better binlog position sync behavior Released 2015-04-28 v0.6.3 better blacklist for CREATE TRIGGER Released 2015-04-13 v0.6.2 maxwell now ignores SAVEPOINT statements. Released 2015-04-13 v0.6.1 fixes a bug with parsing length-limited indexes. Released 2015-04-13 v0.6.0 Version 0.6.0 has Maxwell outputting a JSON kafka key, so that one can use Kafka's neat \"store the last copy of a key\" retention policy. It also fixes a couple of bugs in the query parsing path. Released 2015-04-09 v0.5.0 maxwell now captures primary keys on tables. We'll use this to form kafka key names later. maxwell now outputs to a single topic, hashing the data by database name to keep a database's updates in order. Released 2015-04-06 v0.4.0 v0.4.0 fixes some bugs with long-lived mysql connections by adding connection pooling support. Released 2015-03-25 v0.3.0 This version fixes a fairly nasty bug in which the binlog-position flush thread was sharing a connection with the rest of the system, leading to crashes. It also enables kafka gzip compression by default. Released 2015-03-24 v0.2.2 Version 0.2.2 sets up the LANG environment variable, which fixes a bug in utf-8 handling. Released 2015-03-22 v0.2.1 version 0.2.1 makes Maxwell ignore CREATE INDEX ddl statements and others. Released 2015-03-21 v0.2.0 This release gets Maxwell storing the last-written binlog position inside the mysql master itself. Released 2015-03-18 v0.1.4 support --position_file param Released 2015-03-09 v0.1.3 Adds kafka command line options. Released 2015-03-09 v0.1.1 v0.1.1, a small bugfix release. Released 2015-03-06 v0.1 This is the first possible release of Maxwell that might work. It includes some exceedingly basic kafka support, and JSON output of binlog deltas. Released 2015-03-04","title":"Changelog"},{"location":"changelog/#maxwell-changelog","text":"","title":"Maxwell changelog"},{"location":"changelog/#v1380","text":"Maxwell gets the ability to talk to bigtable! I have no idea how well it'll work. I hope it works for you! upgrade protobuf to fix a rabbitmq issue with booleans, I think. rabbitMQ timeouts on connection other fixes. I can't imagine the security department cares about my naming what with what's going on inside 1019. I guess we'll see. - Released 2022-07-29","title":"v1.38.0"},{"location":"changelog/#v1377","text":"Bump viafoura/metrics-datadog 2.0.0-RC3 Released 2022-06-21","title":"v1.37.7"},{"location":"changelog/#v1376","text":"In non-GTID mode, Verify that the master's server hasn't changed out from underneath us. thanks Tamin Khan Released 2022-05-12","title":"v1.37.6"},{"location":"changelog/#v1375","text":"Upgrade binlog-replicator. pulls in some minor fixes. Released 2022-04-16","title":"v1.37.5"},{"location":"changelog/#v1374","text":"configure custom producer via environment sns and sqs producers take output config properly Released 2022-04-08","title":"v1.37.4"},{"location":"changelog/#v1373","text":"fixes for mariadb Released 2022-03-25","title":"v1.37.3"},{"location":"changelog/#v1372","text":"configurable binlog event queue size Released 2022-03-14","title":"v1.37.2"},{"location":"changelog/#v1371","text":"list changes Released 2022-03-07","title":"v1.37.1"},{"location":"changelog/#v1370","text":"Change max size of RowMap buffer to unblock high-efficiency producers Released 2022-01-26","title":"v1.37.0"},{"location":"changelog/#v1360","text":"fix bug where the millionth binlog would kinda sort \"overflow\" and the binlog positions would stop moving. My benefactor here asked that I stopped creating cute release names. The security department, mysteriously. Released 2022-01-23","title":"v1.36.0"},{"location":"changelog/#v1355","text":"log4j, again and agian. Released 2021-12-29","title":"v1.35.5"},{"location":"changelog/#v1354","text":"log4j turns 2.17.0, happy birthday Released 2021-12-18","title":"v1.35.4"},{"location":"changelog/#v1353","text":"log4j vulnerability #2 Released 2021-12-15","title":"v1.35.3"},{"location":"changelog/#v1352","text":"better logging when we can't connect on startup Released 2021-12-12","title":"v1.35.2"},{"location":"changelog/#v1351","text":"log4j upgrade to upgrade past the giant security hole Released 2021-12-10","title":"v1.35.1"},{"location":"changelog/#v1350","text":"couple of parser fixes docker builds are now multi-platform replication_reconnection_retries configuration option quote table names in bootstrapper properly Released 2021-11-30","title":"v1.35.0"},{"location":"changelog/#v1341","text":"support for mysql 8's visible/invisible columns support mariadb's if-exists/if-not-exists for partition management add an index for the http endpoint Released 2021-09-21","title":"v1.34.1"},{"location":"changelog/#v1340","text":"intern a bunch of objects in our in-memory representation of schema. Saves gobs of memory in cases where one has N copies of the same database. Note that this changes the API of Columns, should any embedded Maxwell application be using that. go up to BIGINT for maxwell's auto-increment ids Released 2021-07-29","title":"v1.34.0"},{"location":"changelog/#v1331","text":"properties may now be fetched from a javascript blob in the env RowMap provides access to primary keys fix an odd NPE in mariaDB init Released 2021-06-02","title":"v1.33.1"},{"location":"changelog/#v1330","text":"Add HTTP endpoint for runtime reconfiguration Released 2021-03-29","title":"v1.33.0"},{"location":"changelog/#v1320","text":"Amazon SNS producer added, thanks Rober Wittman kafka 2.7.0 supported stackdriver metrics logging available Released 2021-03-17","title":"v1.32.0"},{"location":"changelog/#v1310","text":"Add producer for NATS streaming server Released 2021-02-11","title":"v1.31.0"},{"location":"changelog/#v1300","text":"support server-sent heartbeating on the binlog connection via --binlog-heartbeat can connect to rabbitmq by URL, supports SSL connections fix parser bug with multiline SQL target JDK 11 -- we have dropped support for JDK 8 ability to send a microsecond timestamp via --output_push_timestamp fixes for odd azure mysql connection failures Released 2021-02-05","title":"v1.30.0"},{"location":"changelog/#v1292","text":"fix for terrible performance regression in bootstrapping Released 2021-01-27","title":"v1.29.2"},{"location":"changelog/#v1291","text":"small bugfix release, fixes binlog event type processing in mysql 8 Released 2020-12-23","title":"v1.29.1"},{"location":"changelog/#v1290","text":"High Availability support via jgroups-raft rework --help text Released 2020-12-15","title":"v1.29.0"},{"location":"changelog/#v1282","text":"fix for encryption parsing error on table creation some logging around memory usage in RowMapBuffer Released 2020-12-02","title":"v1.28.2"},{"location":"changelog/#v1281","text":"fix http server issue in 1.28.0 Released 2020-11-25","title":"v1.28.1"},{"location":"changelog/#v1280","text":"schema compaction! with the new --max_schemas option, maxwell will periodically roll up the maxwell . schemas table, preventing it from growing infinitely long. fix metricsAgeSloMS calculation support SRID columns fix parsing of complex INDEX(CAST()) statements various dependency bumps Released 2020-11-19","title":"v1.28.0"},{"location":"changelog/#v1271","text":"redis producer gets sentinal support fix a double-reconnect race condition file producer honors javascript row-suppression better error messaging when we lack REPLICATION SLAVE privs miscellaneous dependency bumps Released 2020-08-07","title":"v1.27.1"},{"location":"changelog/#v1270","text":"better support for empty/null passwords allow bootstrap utility to query replication_host a few library upgrades, notably pubsub and kinesis library bootstrap connection uses jdbc_options properly add logging for when we hit out of sync schema exceptions allow for partitioning by thread_id, thx @gogov fresh and clean documentation Released 2020-06-30","title":"v1.27.0"},{"location":"changelog/#v1264","text":"support now() function with precision Released 2020-06-08","title":"v1.26.4"},{"location":"changelog/#v1263","text":"use pooled redis connections, fixes corruption when redis was accessed from multiple threads (bootstrap/producer), thanks @lucastex fix date handling of '0000-01-01' fix race condition in binlog reconnect logic Released 2020-05-26","title":"v1.26.3"},{"location":"changelog/#v1262","text":"bootstraps can be scheduled in the future by setting the started_at column, thanks @lucastex two mysql 8 fixes; one for a DEFAULT(function()) parse error, one for supporting DEFAULT ENCRYPTION Released 2020-05-18","title":"v1.26.2"},{"location":"changelog/#v1261","text":"fixes for redis re-connection login, thanks much @lucastex Released 2020-05-07","title":"v1.26.1"},{"location":"changelog/#v1260","text":"We now support mysql 8's caching_sha2_password authentication scheme support for converting JSON field names to camelCase Released 2020-05-06","title":"v1.26.0"},{"location":"changelog/#v1253","text":"fixes memory leak in mysql-binlog-connector fixes exceptions that occur when a connection passes wait_timeout Released 2020-05-02","title":"v1.25.3"},{"location":"changelog/#v1252","text":"Fixes for a long standing JSON bug in 8.0.19+ Released 2020-05-01","title":"v1.25.2"},{"location":"changelog/#v1251","text":"issue #1457, ALTER DATABASE with implicit database name maxwell now runs on JDK 11 in docker exit with status 2 when we can't find binlog files Released 2020-04-22","title":"v1.25.1"},{"location":"changelog/#v1250","text":"swap un-maintained snaq.db with C3P0. support eu datadog metrics protect against lost connections during key queries (bootstrapping, heartbeats, postition setting) Released 2020-03-29","title":"v1.25.0"},{"location":"changelog/#v1242","text":"bugfix parsing errors: compressed columns, exchange partitions, parenthesis-enclosed default values, drop column foo.t . add partition-by-random feature. update jackson-databind to get security patch fix redis channel interpolation on RPUSH Released 2020-03-25","title":"v1.24.2"},{"location":"changelog/#v1241","text":"allow jdbc_options on secondary connections fix a crash in bootstrapping / javascript filters fix a regression in message.publish.age metric Released 2020-01-21","title":"v1.24.1"},{"location":"changelog/#v1240","text":"add comments field to bootstrapping, thanks Tom Collins fix sql bug with #comments style comments Released 2019-12-14","title":"v1.24.0"},{"location":"changelog/#v1235","text":"Update bootstrap documentation Bump drop wizard metrics to support Java versions 10+ Released 2019-12-12","title":"v1.23.5"},{"location":"changelog/#v1234","text":"Bump and override dependencies to fix security vulnerabilities. Update redis-key config options list changes Released 2019-12-03","title":"v1.23.4"},{"location":"changelog/#v1233","text":"pubsubDelayMultiplier may now be 1.0 allow %{database} and %{topic} interpolation into redis producer docs updates setup default client_id in maxwell-bootstrap util Released 2019-11-21","title":"v1.23.3"},{"location":"changelog/#v1232","text":"upgrade jackson stop passing maxwell rows through the JS filter. too dangerous. Released 2019-10-18","title":"v1.23.2"},{"location":"changelog/#v1231","text":"Add option for XADD (redis streams) operation Add configuration flag for tuning transaction buffer memory sectionalize help text Released 2019-10-12","title":"v1.23.1"},{"location":"changelog/#v1230","text":"Added AWS FIFO support Add retry and batch settings to pubs producer Add support for age SLO metrics Released 2019-10-08","title":"v1.23.0"},{"location":"changelog/#v1226","text":"upgrade mysql-connector-java to 8.0.17 use a newer docker image as base list changes Released 2019-09-20","title":"v1.22.6"},{"location":"changelog/#v1225","text":"bugfix for bootstrapping off a split replica that doesn't contain a \"maxwell\" database Fix a parser issue with db.table.column style column names Released 2019-09-06","title":"v1.22.5"},{"location":"changelog/#v1224","text":"Add row type to fallback message Upgrade jackson-databind Released 2019-08-23","title":"v1.22.4"},{"location":"changelog/#v1223","text":"fix issue with google pubsub in 1.22.2 Released 2019-06-20","title":"v1.22.3"},{"location":"changelog/#v1222","text":"fix an issue with bootstrapping-on-replicas add --output_primary_keys and --output_primary_key_columns fix a very minor memory leak with blacklists Released 2019-06-18","title":"v1.22.2"},{"location":"changelog/#v1221","text":"fix crash in rabbit-mq producer better support for maxwell + azure-mysql remove bogus different-host bootstrap check some security upgrades Released 2019-05-28","title":"v1.22.1"},{"location":"changelog/#v1220","text":"Bootstrapping has been reworked and is now available in all setups, including those in which the maxwell store is split from the replicator. cleanup and fix a deadlock in the kafka fallback queue logic add .partition_string = to javascript filters Released 2019-04-16","title":"v1.22.0"},{"location":"changelog/#v1211","text":"Upgrade binlog connector. Should fix issues around deserialization errors. Released 2019-03-29","title":"v1.21.1"},{"location":"changelog/#v1210","text":"Bootstrapping output no longer contain binlog positions. Please update any code that relies on this. Fix 3 parser issues. Released 2019-03-23","title":"v1.21.0"},{"location":"changelog/#v1200","text":"add support for partitioning by transaction ID thx @hexene add support for a kafka \"fallback\" topic to write to when a message fails to write add UJIS charset support parser bug: multiple strings concatenate to make one default string parser bug: deal with bizarre column renames which are then referenced in AFTER column statements Released 2019-02-28","title":"v1.20.0"},{"location":"changelog/#v1197","text":"fix a parser error with empty sql comments interpret latin-1 as windows-1252, not iso-whatever, thx @borleaandrei Released 2019-01-25","title":"v1.19.7"},{"location":"changelog/#v1196","text":"Further fixes for GTID-reconnection issues. Crash sanely when GTID-enabled maxwell is connected to clearly the wrong master, thanks @acampoh Released 2019-01-20","title":"v1.19.6"},{"location":"changelog/#v1195","text":"Fixes for unreliable connections wrt to GTID events; previously we restart in any old position, now we throw away the current transaction and restart the replicator again at the head of the GTID event. Released 2019-01-15","title":"v1.19.5"},{"location":"changelog/#v1194","text":"Fixes for a maxwell database not making it through the blacklist Add output_null_zerodates parameter to control how we treat '0000-00-00' Released 2019-01-12","title":"v1.19.4"},{"location":"changelog/#v1193","text":"Add a universal backpressure mechanism. This should help people who were running into out-of-memory situations while bootstrapping. Released 2018-12-19","title":"v1.19.3"},{"location":"changelog/#v1192","text":"Include schema_id in bootstrap events add more logging around binlog connector losing connection add retry logic to redis some aws fixes allow pushing JS hashes/arrays into data from js filters list changes Released 2018-12-02","title":"v1.19.2"},{"location":"changelog/#v1191","text":"Handle mysql bit literals in DEFAULT statements blacklist out CREATE ROLE etc upgrade dependencies to pick up security issues Released 2018-11-12","title":"v1.19.1"},{"location":"changelog/#v1190","text":"mysql 8 support! utf8 enum values are supported now fix #1125, bootstrapping issue for TINYINT(1) fix #1145, nasty bug around SQL blacklist and columns starting with \"begin\" only resume bootstraps that are targeted at this client_id fixes for blacklists and heartbeats. Did I ever mention blacklists are a terrible idea? Released 2018-10-27","title":"v1.19.0"},{"location":"changelog/#v1180","text":"memory optimizations for large schemas (especially shareded schemas with lots of duplicates) add support for an http endpoint to support Prometheus metrics allow javascript filters to access the row query object javascript filters now run in the bootstrap process support for non-latin1 column names add --output_schema_id option better handling of packet-too-big errors from Kinesis add message.publish.age metric Released 2018-09-15","title":"v1.18.0"},{"location":"changelog/#v1171","text":"fix a regression around filters + bootstrapping fix a regression around filters + database-only-ddl Released 2018-07-03","title":"v1.17.1"},{"location":"changelog/#v1170","text":"v1.17.0 brings a new level of configurability by allowing you to inject a bit of javascript into maxwell's processing. Should be useful! Also: fix regression for Alibaba RDS tables Released 2018-06-28","title":"v1.17.0"},{"location":"changelog/#v1161","text":"Fix Bootstrapping for JSON columns add --recapture_schema flag for when ya wanna start over add kafka 1.0 libraries, make them default Released 2018-06-21","title":"v1.16.1"},{"location":"changelog/#v1160","text":"v1.16.0 brings a rewrite of Maxwell's filtering system, giving it a concise list of rules that are executed in sequence. It's now possible to exclude tables from a particular database, exclude columns matching a value, and probably some other use cases. See http://maxwells-daemon.io/config/#filtering for details. Released 2018-06-15","title":"v1.16.0"},{"location":"changelog/#v1150","text":"This is a bug-fix release, but it's big enough I'm giving it a minor version. Fix a very old bug in which DDL rows were writing the start of the row into maxwell.positions , leading to chaos in some scenarios where maxwell managed to stop on the row and double-process it, as well as to a few well-meaning patches. Fix the fact that maxwell was outputting \"next-position\" instead of \"position\" of a row into JSON. Fix the master-recovery code to store schema that corresponds to the start of a row, and points the replicator at the next-position. Much thanks to Tim, Likun and others in sorting this mess out. Released 2018-06-13","title":"v1.15.0"},{"location":"changelog/#v1147","text":"add RowMap#getRowQuery, thx @saimon7 revert alpine-linux docker image fiasco fix RawJSONString not serializable, thx @niuhaifeng Released 2018-06-03","title":"v1.14.7"},{"location":"changelog/#v1146","text":"Fix docker image Released 2018-05-15","title":"v1.14.6"},{"location":"changelog/#v1145","text":"reduce docker image footprint add benchmarking framework performance improvements for date/datetime columns fix parser error on UPGRADE PARTITIONING Released 2018-05-15","title":"v1.14.5"},{"location":"changelog/#v1144","text":"Fix race condition in SchemaCapturer Released 2018-05-07","title":"v1.14.4"},{"location":"changelog/#v1143","text":"Enable jvm metrics Released 2018-05-04","title":"v1.14.3"},{"location":"changelog/#v1142","text":"fix regression in 1.14.1 around bootstrapping host detection fix heartbeating code around table includes Released 2018-05-02","title":"v1.14.2"},{"location":"changelog/#v1141","text":"bootstraps can now take a client_id improved config validation for embedded mode Released 2018-05-01","title":"v1.14.1"},{"location":"changelog/#v1140","text":"new feature --output_xoffset to uniquely identify rows within transactions, thx Jens Gyti Bug fixes around \"0000-00-00\" times. Bug fixes around dates pre 1000 AD Released 2018-04-24","title":"v1.14.0"},{"location":"changelog/#v1135","text":"Support environment variable based configuration Released 2018-04-11","title":"v1.13.5"},{"location":"changelog/#v1134","text":"Added possibility to do not declare the rabbitmq exchange. Released 2018-04-03","title":"v1.13.4"},{"location":"changelog/#v1133","text":"Add logging for binlog errors Maven warning fix Do not include current position DDL schema to avoid processing DDL twice Always write null fields in primary key fields Bugfix: fix http_path_prefix command line option issue Released 2018-04-03","title":"v1.13.3"},{"location":"changelog/#v1132","text":"fix a bug with CHARACTER SET = DEFAULT maxwell now eclipse-friendly. configurable bind-address for maxwell's http server Released 2018-03-06","title":"v1.13.2"},{"location":"changelog/#v1131","text":"redis producer now supports LPUSH, thx @m-denton RowMap can now contain artbitrary attributes for embedded maxwell, thx @jkgeyti bugfix: fix jdbc option parsing when value contains = bugfix: apparently the SQS producer was disabled bugfix: fix a situation where adding a second client could cause schemas to become out of sync support for --daemon Released 2018-02-20","title":"v1.13.1"},{"location":"changelog/#v1130","text":"proper SSL connection support, thanks @cadams5 support for including original SQL in insert/update/deletes, thanks @saimon7 fixes for float4, float8 and other non-mysql datatypes bump kinesis lib to 0.12.8 fix for bug when two databases share a single table Released 2018-02-01","title":"v1.13.0"},{"location":"changelog/#v1120","text":"Support for injecting a custom producer, thanks @tomcollinsproject New producer for Amazon SQS, thanks @vikrant2mahajan Maxwell can now filter rows based on column values, thanks @finnplay Fixes for the Google Pubsub producer (it was really broken), thanks @finnplay DDL output can now optionally include the source SQL, thanks @sungjuly Support for double-quoted table/database/etc names rabbitmq option for persistent messages, thanks @d-babiak SQL parser bugfix for values like +1.234, thanks @hexene Released 2018-01-09","title":"v1.12.0"},{"location":"changelog/#v1110","text":"- default kafka client upgrades to 0.11.0.1 - fix the encryption issue (https://github.com/zendesk/maxwell/issues/803) Released 2017-11-22","title":"v1.11.0"},{"location":"changelog/#v1109","text":"We recommend all v1.10.7 and v1.10.8 users upgrade to v1.10.9. Add missing Kafka clients Listen and report on binlog connector lifecycle events for better visibility Reduce docker image size Released 2017-10-30","title":"v1.10.9"},{"location":"changelog/#v1108","text":"Fix docker builds Add Google Cloud Pub/Sub producer RabbitMQ producer enhancements Released 2017-10-12","title":"v1.10.8"},{"location":"changelog/#v1107","text":"Java 8 upgrade Diagnostic health check endpoint Encryption Documentation update: encryption, kinesis producer, schema storage fundamentals, etc. Released 2017-10-11","title":"v1.10.7"},{"location":"changelog/#v1106","text":"Binlog-connector upgrade Bug-fix: when using literal string for an option that accepts Regex, Regex characters are no longer special If master recovery is enabled, Maxwell cleans up old positions for the same server and client id Released 2017-08-14","title":"v1.10.6"},{"location":"changelog/#v1105","text":"Shyko's binlog-connector is now the default and only replication backend available for maxwell. Released 2017-07-25","title":"v1.10.5"},{"location":"changelog/#v1104","text":"Notable changes: Shutdown hardening. If maxwell can't shut down (because the kafka producer is in a bad state and close() never terminates, for example), it would previously stall and process no messages. Now, shutdown is run in a separate thread and there is an additional watchdog thread which forcibly kills the maxwell process if it can't shut down within 10 seconds. Initial support for running maxwell from java, rather then as its own process. This mode of operation is still experimental, but we'll accept PRs to improve it (thanks Geoff Lywood). Fix incorrect handling of negative (pre-epoch dates) when using binlog_connector mode (thanks Geoff Lywood). Released 2017-07-10","title":"v1.10.4"},{"location":"changelog/#v1103","text":"tiny release to fix a units error in the replication.lag metric (subtracting seconds from milliseconds) Released 2017-06-06","title":"v1.10.3"},{"location":"changelog/#v1102","text":"added metrics: \"replication.queue.time\" and \"inflightmessages.count\" renamed \"time.overall\" metric to \"message.publish.time\" documentation updates (thanks Chintan Tank) Released 2017-06-04","title":"v1.10.2"},{"location":"changelog/#v1101","text":"The observable changes in this minor release are a new configuration for Kafka/Kinesis producer to abort processing on publish errors, and support of Kafka 0.10.2. Also a bunch of good refactoring has been done for heartbeat processing. List of changes: Support Kafka 0.10.2 Stop procesing RDS hearbeats Keep maxwell heartbeat going every 10 seconds when database is quiet Allow for empty double-quoted string literals for database schema changes Ignore Kafka/Kinesis producer errors based on new configuration ignore_producer_error Released 2017-05-26","title":"v1.10.1"},{"location":"changelog/#v1100","text":"This is a small release, primarily around a change to how schemas are stored. Maxwell now stores the last_heartbeat_read with each entry in the schemas table, making schema management more resilient to cases where binlog numbers are reused, but means that you must take care if you need to roll back to an earlier version. If you deploy v1.10.0, then roll back to an earlier version, you should manually update all schemas . last_heartbeat_read values to 0 before redeploying v1.10.0 or higher. Other minor changes: allow negative default numbers in columns only store final binlog position if it has changed blacklist internal aurora table `rds_heartbeat*' log4j version bump (allows for one entry per line JSON logging) Released 2017-05-09","title":"v1.10.0"},{"location":"changelog/#v190","text":"Maxwell 1.9 adds one main feature: monitoring support, contributed by Scott Ferguson. Multiple backends can be configured, read the updated docs for full details. There's also some bugfixes: filter DDL messages based on config determine newest schema from binlog order, not creation order add task manager to shutdown cleanly on error minor logging improvements Released 2017-04-26","title":"v1.9.0"},{"location":"changelog/#v182","text":"Bugfix release. maxwell would crash on a quoted partition name fixes for alters on non-string tables containing VARCHAR use seconds instead of milliseconds for DDL messages Released 2017-04-11","title":"v1.8.2"},{"location":"changelog/#v181","text":"performance improves in capturing and restoring schema, thx Joren Minnaert Allow for capturing from a separate mysql host (adds support for using Maxscale as a replication proxy), thx Adam Szkoda Released 2017-02-20","title":"v1.8.1"},{"location":"changelog/#v180","text":"In version 1.8.0 Maxwell gains alpha support for GTID-based positions! All praise due to Henry Cai. Released 2017-02-14","title":"v1.8.0"},{"location":"changelog/#v172","text":"Fix a bug found where maxwell could cache the wrong TABLE_MAP_ID for a binlog event, leading to crashes or in some cases data mismatches. Released 2017-01-30","title":"v1.7.2"},{"location":"changelog/#v171","text":"bootstrapping now can take a --where clause performance improvements in the kafka producer Released 2017-01-24","title":"v1.7.1"},{"location":"changelog/#v170","text":"Maxwell 1.7 brings 2 major new, alpha features. The first is Mysql 5.7 support, including JSON column type support and handling of 5.7 SQL, but not including GTID support yet. This is based on porting Maxwell to Stanley Shyko's binlog-connector library. Thanks to Stanley for his amazing support doing this port. The second major new feature is a producer for Amazon's Kinesis streams, This was contributed in full by the dogged and persistent Thomas Dziedzic. Check it out with --producer=kinesis . There's also some bugfixes: - Amazon RDS heartbeat events now tick maxwell's position, thx Scott Ferguson - allow CHECK() statements inside column definitions Released 2017-01-07","title":"v1.7.0"},{"location":"changelog/#v160","text":"This is mostly a bugfix release, but it gets a minor version bump due to a single change of behavior: dates and timestamps which mysql may accept, but are considered invalid (0000-00-00 is a notable example) previously had inconsistent behavior. Now we convert these to NULL. Other bugfixes: - heartbeats have moved into their own table - more fixes around alibaba rds - ignore DELETE statements that are output for MEMORY tables upon server restart - allow pointing maxwell to a pre-existing database Released 2016-12-29","title":"v1.6.0"},{"location":"changelog/#v152","text":"add support for kafka 0.10.1 @ smferguson master recovery: cleanup positions from previous master; prevent errors on flip-back. fix a bug that would trigger in certain cases when dropping a column that was part of the primary-key Released 2016-12-07","title":"v1.5.2"},{"location":"changelog/#v151","text":"This is a bugfix release. - fixes for bootstrapping with an alternative maxwell-schema name and an include_database filter, thanks Lucian Jones - fixes for kafka 0.10 with lz4 compression, thanks Scott Ferguson - ignore the RDS table mysql.ha_health_check table - Get the bootstrapping process to output NULL values. - fix a quoting issue in the bootstrap code, thanks @mylesjao. Released 2016-11-24","title":"v1.5.1"},{"location":"changelog/#v150","text":"CHANGE: Kafka producer no longer ships with hard-coded defaults. Please ensure you have \"compression.type\", \"metadata.fetch.timeout.ms\", and \"retries\" configured to your liking. bugfix: fix a regression in handling ALTER TABLE change c int after b statements warn on servers with missing server_id Released 2016-11-07","title":"v1.5.0"},{"location":"changelog/#v142","text":"kafka 0.10.0 support, as well as a re-working of the --kafka_version command line option. Released 2016-11-01","title":"v1.4.2"},{"location":"changelog/#v141","text":"support per-table topics, Thanks @smferguson and @sschatts. fix a parser issue with DROP COLUMN CASCADE, thanks @smferguson Released 2016-10-27","title":"v1.4.1"},{"location":"changelog/#v140","text":"1.4.0 brings us two nice new features: - partition-by-column: see --kafka_partition_columns. Thanks @smferguson - output schema changes as JSON: see --output_ddl. Thanks @xmlking - As well as a fix around race conditions on shutdown. Released 2016-10-21","title":"v1.4.0"},{"location":"changelog/#v130","text":"support for fractional DATETIME, TIME, TIMESTAMP columns, thanks @Dagnan support for outputting server_id & thread_id, thanks @sagiba fix a race condition in bootstrap support Released 2016-10-03","title":"v1.3.0"},{"location":"changelog/#v122","text":"Maxwell will now include by default fields with NULL values (as null fields). To disable this and restore the old functionality where fields were omitted, pass --output_nulls=false Fix an issue with multi-client support where two replicators would ping-pong heartbeats at each other Fix an issue where a client would attempt to recover a position from a mismatched client_id Fix a bug when using CHANGE COLUMN on a primary key Released 2016-09-23","title":"v1.2.2"},{"location":"changelog/#v121","text":"This is a bugfix release. - fix a parser bug around ALTER TABLE CHARACTER SET - fix bin/maxwell to pull in the proper version of the kafka-clients library Released 2016-09-15","title":"v1.2.1"},{"location":"changelog/#v120","text":"1.2.0 is a major release of Maxwell that introduces master recovery features; when a slave is promoted to master, Maxwell is now capable of recovering the position. See the --master_recovery flag for more details. It also upgrades the kafka producer library to 0.9. If you're using maxwell with a kafka 0.8 server, you must now pass the --kafka0.8 flag to maxwell. Released 2016-09-12","title":"v1.2.0"},{"location":"changelog/#v116","text":"minor bugfix in which maxwell with --replay mode was trying to write heartbeats Released 2016-09-07","title":"v1.1.6"},{"location":"changelog/#v115","text":"@dadah89 adds --output_binlog_position to optionally output the position with the row @dadah89 adds --output_commit_info to turn off xid/commit fields maxwell now supports tables with partitions maxwell now supports N maxwells per-server. see the client_id / replica_server_id options. two parser fixes, for engine= innodb and CHARSET ASCII lay the ground work for doing master recovery; we add a heartbeat into the positions table that we can co-ordinate around. Released 2016-09-04","title":"v1.1.5"},{"location":"changelog/#v114","text":"add support for a bunch more charsets (gbk, big5, notably) fix Maxwell's handling of kafka errors - previously we were trying to crash Maxwell by throwing a RuntimeException out of the Kafka Producer, but this was a failure. Now we log and skip all errors. Released 2016-08-05","title":"v1.1.4"},{"location":"changelog/#v113","text":"This is a bugfix release, which fixes: - https://github.com/zendesk/maxwell/issues/376, a problem parsing RENAME INDEX - https://github.com/zendesk/maxwell/issues/371, a problem with the SERIAL datatype - https://github.com/zendesk/maxwell/issues/362, we now preserve the original casing of columns - https://github.com/zendesk/maxwell/issues/373, we were incorrectly expecting heartbeats to work under 5.1 Released 2016-07-14","title":"v1.1.3"},{"location":"changelog/#v112","text":"pick up latest mysql-connector-j, fixes #369 fix an issue where maxwell could skip ahead positions if a leader failed. rework buffering code to be much kinder to the GC and JVM heap in case of very large transactions / rows inside transactions kinder, gentler help text when you specify an option incorrectly Released 2016-06-27","title":"v1.1.2"},{"location":"changelog/#v111","text":"fixes a race condition setting the binlog position that would get maxwell stuck Released 2016-05-23","title":"v1.1.1"},{"location":"changelog/#v110","text":"much more efficient processing of schema updates storage, especially when dealing with large schemas. @lileeyao added --exclude-columns and the --jdbc_options features @lileeyao added --jdbc_options can now blacklist entire databases new kafka key format available, using a JSON array instead of an object bugfix: unsigned integer columns were captured incorrectly. 1.1 will recapture the schema and attempt to correct the error. Released 2016-05-20","title":"v1.1.0"},{"location":"changelog/#v110-pre4","text":"Eddie McLean gives some helpful patches around bootstrapping Bugfixes for the patch-up-the-schema code around unsigned ints Released 2016-05-06","title":"v1.1.0-pre4"},{"location":"changelog/#v110-pre3","text":"forgot to include some updates that back-patch unsigned column problems Released 2016-05-05","title":"v1.1.0-pre3"},{"location":"changelog/#v110-pre2","text":"fix performance issues when capturing schema in AWS Aurora fix a bug in capturing unsigned integer columns Released 2016-05-04","title":"v1.1.0-pre2"},{"location":"changelog/#v101","text":"fixes a parsing bug with CURRENT_TIMESTAMP() Released 2016-04-12","title":"v1.0.1"},{"location":"changelog/#v100","text":"Since v0.17.0, Maxwell has gotten: - bootstrapping support - blacklisting for tables - flexible kafka partitioning - replication heartbeats - GEOMETRY columns - a whole lotta lotta bugfixes and I, Osheroff, think the damn thing is stable enough for a 1.0. So there. Released 2016-03-11","title":"v1.0.0"},{"location":"changelog/#v100-rc3","text":"pull in support for replication heartbeats. helps in the flakier network environs. Released 2016-03-08","title":"v1.0.0-RC3"},{"location":"changelog/#v100-rc2","text":"fixes the way ALTER DATABASE charset= was handled adds proper handling of ALTER TABLE CONVERT TO CHARSET Released 2016-02-20","title":"v1.0.0-RC2"},{"location":"changelog/#v100-rc1","text":"modifications to the way the bootstrap utility works fix a race condition crash bug in bootstrapping fix a parser bug Released 2016-02-11","title":"v1.0.0-RC1"},{"location":"changelog/#v100-pre2","text":"1.0.0-PRE2 brings in a lot of changes that got merged while we were testing out PRE1. so, hey. - Configurable names for the maxwell schema database (Kristian Kaufman) - Configurable key (primary key, id, database) into the kafka partition hash function (Kristian Kaufman) - Configurable Kafka partition hash function (java hashCode, murmur3) (Kristian Kaufman) - support GEOMETRY columns, output as well-known-text - add --blacklist_tables option to fully ignore excessive schema changes (Nicolas Maquet) - bootstrap rows now have 'bootstrap-insert' type Released 2016-01-30","title":"v1.0.0-PRE2"},{"location":"changelog/#v100-pre1","text":"Here we have the preview release of @nmaquet's excellent work around bootstrapping initial versions of mysql tables. Released 2016-01-09","title":"v1.0.0-PRE1"},{"location":"changelog/#v0170","text":"v0.17 is a large bugfix release with one new feature. - FEATURE: allow specifying an alternative mysql schema-storage server and replication server - BUGFIX: properly handle case-sensitivity by aping the behavior of the master server. Fixes #230. - BUGFIX: parse some forms of CHECK( ... ) statements. Fixes #203. - BUGFIX: many more SQL-parser fixes. We are mostly through some thousands of lines of SQL produced by mysql-test. Released 2016-01-07","title":"v0.17.0"},{"location":"changelog/#v0162","text":"This is a large-ish bugfix release. - Support, with reservations, binlog_row_image=MINIMAL - parser bug: handle table names that look like floating points - parser bug: fix for entity names that have '.', '\\', etc in them - handle UPPERCASE encoding names - support UCS2 (start trying to operate ok on the mysql-test suite) - use ObjectOutputStream.reset to fix memory leaks when buffering to disk Released 2015-12-16","title":"v0.16.2"},{"location":"changelog/#v0161","text":"This is a bug-fix-roundup release: - support ALTER DATABASE - fix a bunch of parse errors: we've started running mysql-test at maxwell and are fixing up failures. - some modifications to the overflow-to-disk logic; we buffer the input and output, and we fix a memory leak Released 2015-12-11","title":"v0.16.1"},{"location":"changelog/#v0160","text":"Version 0.16.0 introduces a feature where UPDATE statements will now show both the new row image and the old values of the fields that changed. Thanks @kristiankaufmann Released 2015-12-10","title":"v0.16.0"},{"location":"changelog/#v0150","text":"fix a parse problem with indices ordered by ASC/DESC Released 2015-12-07","title":"v0.15.0"},{"location":"changelog/#v0150-rc1","text":"large transactions now buffer to disk instead of crushing maxwell. support ALGORITHM=[algo], LOCK=[lock] for 5.6 alters Released 2015-12-04","title":"v0.15.0-RC1"},{"location":"changelog/#v0146","text":"fix TIME column support fix parsing on millisecond precision column defintions fix CREATE SCHEMA parsing Released 2015-11-27","title":"v0.14.6"},{"location":"changelog/#v0145","text":"handle BOOLEAN columns with true/false defaults Released 2015-11-25","title":"v0.14.5"},{"location":"changelog/#v0144","text":"fixes parsing of \"mysql comments\" ( /*! .. */ ) More performance improvements, another 10% in a tight loop. Released 2015-11-24","title":"v0.14.4"},{"location":"changelog/#v0143","text":"fixes a regression in 0.14.2 that creates duplicate copies of the \"mysql\" database in the schema. Released 2015-11-23","title":"v0.14.3"},{"location":"changelog/#v0142","text":"capture the mysql database along with the rest of the schema. Eliding it was a bad premature optimization that led to crashes when tables in the mysql database changed. Released 2015-11-20","title":"v0.14.2"},{"location":"changelog/#v0141","text":"fixes a parser bug around named PRIMARY KEYs. Released 2015-11-17","title":"v0.14.1"},{"location":"changelog/#v0140","text":"This release introduces row filters, allowing you to include or exclude tables from maxwell's output based on names or regular expressions. Released 2015-11-03","title":"v0.14.0"},{"location":"changelog/#v0131","text":"v0.13.1 is a bug fix of v0.13.0 -- fixes a bug where long rows were truncated. v0.13.0 contains: - Big performance boost for maxwell: 75% faster in some benchmarks - @davidsheldon contributed some nice bug fixes around CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema. - we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas. Released 2015-10-29","title":"v0.13.1"},{"location":"changelog/#v0130","text":"Lucky release number 13 brings some reasonably big changes: - Big performance boost for maxwell: 75% faster in some benchmarks - @davidsheldon contributed some nice bug fixes around CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema. - we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas. This release has a pretty bad bug. do not use. Released 2015-10-29","title":"v0.13.0"},{"location":"changelog/#v0120","text":"add support for BIT columns. Released 2015-10-16","title":"v0.12.0"},{"location":"changelog/#v0114","text":"this is another bugfix release that fixes a problem where the replication thread can die in the middle of processing a transaction event. I really need to fix this at a lower level, ie the open-replicator level. Released 2015-09-30","title":"v0.11.4"},{"location":"changelog/#v0113","text":"this is a bugfix release: - fix problems with table creation options inside alter statements ( ALTER TABLE foo auto_increment=10 ) - fix a host of shutdown-procedure bugs the test suite should also be way more reliable, not like you care. Released 2015-09-29","title":"v0.11.3"},{"location":"changelog/#v0112","text":"This is a bugfix release. It includes: - soft deletions of maxwell.schemas to fix A->B->A master swapping without creating intense replication delay - detect and fail early if we see binlog_row_image=minimal - kill off maxwell if the position thread dies - fix a bug where maxwell could pick up a copy of schema from a different server_id (curse you operator precedence!) Released 2015-09-18","title":"v0.11.2"},{"location":"changelog/#v0111","text":"maxwell gets a very minimal pass at detecting when a master has changed, in which it will kill off schemas and positions from a server_id that no longer is valid. this should prevent the worst of cases. Released 2015-09-16","title":"v0.11.1"},{"location":"changelog/#v0110","text":"This release of Maxwell preserves transaction information in the kafka stream by adding a xid key in the JSON object, as well as a commit key for the final row inside the transaction. It also contains a bugfix around server_id handling. Released 2015-09-15","title":"v0.11.0"},{"location":"changelog/#v0101","text":"proper support for BLOB, BINARY, VARBINARY columns (base 64 encoded) fix a problem with the SQL parser where specifying encoding or collation in a string column in the wrong order would crash make table option parsing more lenient Released 2015-09-11","title":"v0.10.1"},{"location":"changelog/#v0110-rc1","text":"merge master fixes Released 2015-09-09","title":"v0.11.0-RC1"},{"location":"changelog/#v0110-pre4","text":"bugfix on v0.11.0-PRE3 Released 2015-09-09","title":"v0.11.0-PRE4"},{"location":"changelog/#v0100","text":"Mysql 5.6 checksum support! some more bugfixes with the SQL parser Released 2015-09-09","title":"v0.10.0"},{"location":"changelog/#v0110-pre3","text":"handle SAVEPOINT within transactions downgrade unhandled SQL to a warning Released 2015-09-08","title":"v0.11.0-PRE3"},{"location":"changelog/#v0110-pre2","text":"fixes for myISAM \"transactions\" Released 2015-09-03","title":"v0.11.0-PRE2"},{"location":"changelog/#v0110-pre1","text":"fix a server_id bug (was always 1 in maxwell.schemas) JSON output now includes transaction IDs Released 2015-09-02","title":"v0.11.0-PRE1"},{"location":"changelog/#v0100-rc4","text":"deal with BINARY flag in string column creation. Released 2015-08-31","title":"v0.10.0-RC4"},{"location":"changelog/#v095","text":"handle the BINARY flag in column creation Released 2015-08-31","title":"v0.9.5"},{"location":"changelog/#v0100-rc3","text":"handle \"TRUNCATE [TABLE_NAME]\" statements Released 2015-08-27","title":"v0.10.0-RC3"},{"location":"changelog/#v0100-rc2","text":"fixes a bug with checksum processing. Released 2015-08-26","title":"v0.10.0-RC2"},{"location":"changelog/#v0100-rc1","text":"upgrade to open-replicator 1.3.0-RC1, which brings binlog checksum (and thus easy 5.6.1) support to maxwell. Released 2015-08-04","title":"v0.10.0-RC1"},{"location":"changelog/#v094","text":"allow a configurable number (including unlimited) of schemas to be stored Released 2015-07-27","title":"v0.9.4"},{"location":"changelog/#v093","text":"bump open-replicator to 1.2.3, which allows processing of single rows greater than 2^24 bytes Released 2015-07-14","title":"v0.9.3"},{"location":"changelog/#v092","text":"bump open-replicator buffer to 50mb by default log to STDERR, not STDOUT --output_file option for file producer Released 2015-07-10","title":"v0.9.2"},{"location":"changelog/#v091","text":"Maxwell is now aware that column names are case-insenstive fix a nasty bug in which maxwell would store the wrong position after it lost its connection to the master. Released 2015-06-22","title":"v0.9.1"},{"location":"changelog/#v090","text":"Also, vanchi is so paranoid he's worried immediately about this. mysql 5.6 support (without checksum support, yet) fix a bunch of miscellaneous bugs @akshayi1 found (REAL, BOOL, BOOLEAN types, TRUNCATE TABLE) Released 2015-06-18","title":"v0.9.0"},{"location":"changelog/#v081","text":"minor bugfix release around mysql connections going away. Released 2015-06-16","title":"v0.8.1"},{"location":"changelog/#v080","text":"add \"ts\" field to row output add --config option for passing a different config file support int1, int2, int4, int8 columns Released 2015-06-09","title":"v0.8.0"},{"location":"changelog/#v072","text":"handle inline sql comments ignore more user management SQL Released 2015-05-29","title":"v0.7.2"},{"location":"changelog/#v071","text":"only keep 5 most recent schemas Released 2015-05-15","title":"v0.7.1"},{"location":"changelog/#v070","text":"handle CURRENT_TIMESTAMP parsing properly better binlog position sync behavior Released 2015-04-28","title":"v0.7.0"},{"location":"changelog/#v063","text":"better blacklist for CREATE TRIGGER Released 2015-04-13","title":"v0.6.3"},{"location":"changelog/#v062","text":"maxwell now ignores SAVEPOINT statements. Released 2015-04-13","title":"v0.6.2"},{"location":"changelog/#v061","text":"fixes a bug with parsing length-limited indexes. Released 2015-04-13","title":"v0.6.1"},{"location":"changelog/#v060","text":"Version 0.6.0 has Maxwell outputting a JSON kafka key, so that one can use Kafka's neat \"store the last copy of a key\" retention policy. It also fixes a couple of bugs in the query parsing path. Released 2015-04-09","title":"v0.6.0"},{"location":"changelog/#v050","text":"maxwell now captures primary keys on tables. We'll use this to form kafka key names later. maxwell now outputs to a single topic, hashing the data by database name to keep a database's updates in order. Released 2015-04-06","title":"v0.5.0"},{"location":"changelog/#v040","text":"v0.4.0 fixes some bugs with long-lived mysql connections by adding connection pooling support. Released 2015-03-25","title":"v0.4.0"},{"location":"changelog/#v030","text":"This version fixes a fairly nasty bug in which the binlog-position flush thread was sharing a connection with the rest of the system, leading to crashes. It also enables kafka gzip compression by default. Released 2015-03-24","title":"v0.3.0"},{"location":"changelog/#v022","text":"Version 0.2.2 sets up the LANG environment variable, which fixes a bug in utf-8 handling. Released 2015-03-22","title":"v0.2.2"},{"location":"changelog/#v021","text":"version 0.2.1 makes Maxwell ignore CREATE INDEX ddl statements and others. Released 2015-03-21","title":"v0.2.1"},{"location":"changelog/#v020","text":"This release gets Maxwell storing the last-written binlog position inside the mysql master itself. Released 2015-03-18","title":"v0.2.0"},{"location":"changelog/#v014","text":"support --position_file param Released 2015-03-09","title":"v0.1.4"},{"location":"changelog/#v013","text":"Adds kafka command line options. Released 2015-03-09","title":"v0.1.3"},{"location":"changelog/#v011","text":"v0.1.1, a small bugfix release. Released 2015-03-06","title":"v0.1.1"},{"location":"changelog/#v01","text":"This is the first possible release of Maxwell that might work. It includes some exceedingly basic kafka support, and JSON output of binlog deltas. Released 2015-03-04","title":"v0.1"},{"location":"compat/","text":"Compability Requirements JRE 11 or above mysql 5.1, 5.5, 5.6, 5.7, 8 kafka 0.8.2 or greater (if using kafka) Caveats / Notes binlog_row_image=MINIMAL Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want. It will differ from normal Maxwell operation in that: INSERT statements will no longer output a column's default value UPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs, but data will only include what is needed to perform the update (generally, id columns and changed columns). The old section may or may not be included, depending on the nature of the update. DELETE statements will be incomplete; generally they will only include the primary key. Master recovery Maxwell includes support for master position recovery (non-GTID). It works like this: maxwell writes heartbeats into the binlogs (via the positions table) maxwell reads its own heartbeats, using them as a secondary position guide if maxwell boots and can't find its position matching the server_id it's connecting to, it will look for a row in maxwell.positions from a different server_id. if it finds that row, it will scan backwards in the binary logs of the new master until it finds that heartbeat. Notes: master recovery is not compatible with separate schema-store hosts and replication-hosts, due to the heartbeat mechanism. this code should be considered alpha-quality. on highly active servers, as much as 1 second of data may be duplicated. master recovery is not available in GTID-mode. GTID is generally a preferred method of master failover.","title":"Compatibility"},{"location":"compat/#compability","text":"","title":"Compability"},{"location":"compat/#requirements","text":"JRE 11 or above mysql 5.1, 5.5, 5.6, 5.7, 8 kafka 0.8.2 or greater (if using kafka)","title":"Requirements"},{"location":"compat/#caveats-notes","text":"","title":"Caveats / Notes"},{"location":"compat/#binlog_row_imageminimal","text":"Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want. It will differ from normal Maxwell operation in that: INSERT statements will no longer output a column's default value UPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs, but data will only include what is needed to perform the update (generally, id columns and changed columns). The old section may or may not be included, depending on the nature of the update. DELETE statements will be incomplete; generally they will only include the primary key.","title":"binlog_row_image=MINIMAL"},{"location":"compat/#master-recovery","text":"Maxwell includes support for master position recovery (non-GTID). It works like this: maxwell writes heartbeats into the binlogs (via the positions table) maxwell reads its own heartbeats, using them as a secondary position guide if maxwell boots and can't find its position matching the server_id it's connecting to, it will look for a row in maxwell.positions from a different server_id. if it finds that row, it will scan backwards in the binary logs of the new master until it finds that heartbeat. Notes: master recovery is not compatible with separate schema-store hosts and replication-hosts, due to the heartbeat mechanism. this code should be considered alpha-quality. on highly active servers, as much as 1 second of data may be duplicated. master recovery is not available in GTID-mode. GTID is generally a preferred method of master failover.","title":"Master recovery"},{"location":"config/","text":"Reference Configuration options are set either via command line or the \"config.properties\" file. general option argument description default config STRING location of config.properties file $PWD/config.properties log_level LOG_LEVEL log level info daemon running maxwell as a daemon env_config_prefix STRING env vars matching prefix are treated as config values mysql option argument description default host STRING mysql host localhost user STRING mysql username password STRING mysql password (no password) port INT mysql port 3306 jdbc_options STRING mysql jdbc connection options DEFAULT_JDBC_OPTS ssl SSL_OPT SSL behavior for mysql cx DISABLED schema_database STRING database to store schema and position in maxwell client_id STRING unique text identifier for maxwell instance maxwell replica_server_id LONG unique numeric identifier for this maxwell instance 6379 (see notes ) master_recovery BOOLEAN enable experimental master recovery code false gtid_mode BOOLEAN enable GTID-based replication false recapture_schema BOOLEAN recapture the latest schema. Not available in config.properties. false max_schemas LONG how many schema deltas to keep before triggering compaction operation unlimited binlog_heartbeat BOOLEAN enable binlog heartbeats to detect stale connections DISABLED replication_host STRING server to replicate from. See split server roles schema-store host replication_password STRING password on replication server (none) replication_port INT port on replication server 3306 replication_user STRING user on replication server replication_ssl SSL_OPT SSL behavior for replication cx cx DISABLED replication_jdbc_options STRING mysql jdbc connection options for replication server DEFAULT_JDBC_OPTS schema_host STRING server to capture schema from. See split server roles schema-store host schema_password STRING password on schema-capture server (none) schema_port INT port on schema-capture server 3306 schema_user STRING user on schema-capture server schema_ssl SSL_OPT SSL behavior for schema-capture server DISABLED schema_jdbc_options STRING mysql jdbc connection options for schema server DEFAULT_JDBC_OPTS producer options option argument description default producer PRODUCER_TYPE type of producer to use stdout custom_producer.factory CLASS_NAME fully qualified custom producer factory class, see example producer_ack_timeout PRODUCER_ACK_TIMEOUT time in milliseconds before async producers consider a message lost producer_partition_by PARTITION_BY input to kafka/kinesis partition function database producer_partition_columns STRING if partitioning by 'column', a comma separated list of columns producer_partition_by_fallback PARTITION_BY_FALLBACK required when producer_partition_by=column. Used when the column is missing ignore_producer_error BOOLEAN When false, Maxwell will terminate on kafka/kinesis/pubsub publish errors (aside from RecordTooLargeException). When true, errors are only logged. See also dead_letter_topic true file producer option argument description default output_file STRING output file for file producer javascript STRING file containing javascript filters kafka producer option argument description default kafka.bootstrap.servers STRING kafka brokers, given as HOST:PORT[,HOST:PORT] kafka_topic STRING kafka topic to write to. maxwell dead_letter_topic STRING the topic to write a \"skeleton row\" (a row where data includes only primary key columns) when there's an error publishing a row. When ignore_producer_error is false , only RecordTooLargeException causes a fallback record to be published, since other errors cause termination. Currently only supported in Kafka publisher kafka_version KAFKA_VERSION run maxwell with specified kafka producer version. Not available in config.properties. 0.11.0.1 kafka_partition_hash [ default | murmur3 ] hash function to use when choosing kafka partition default kafka_key_format [ array | hash ] how maxwell outputs kafka keys, either a hash or an array of hashes hash ddl_kafka_topic STRING if output_ddl is true, kafka topic to write DDL changes to kafka_topic See also: Kafka Producer Documentation kinesis producer option argument description default kinesis_stream STRING kinesis stream name See also: Kinesis Producer Documentation sqs producer option argument description default sqs_queue_uri STRING SQS Queue URI See also: SQS Producer Documentation sns producer option argument description default sns_topic STRING The SNS topic to publish to. FIFO topics should end with .fifo sns_attrs STRING Properties to set as attributes on the SNS message See also: SNS Producer Documentation nats producer option argument description default nats_url STRING Comma separated list of nats urls. may include user:password style auth nats://localhost:4222 nats_subject STRING Nats subject hierarchy. Topic substitution available. %{database}.%{table} See also: Nats Producer Documentation pubsub producer option argument description default pubsub_topic STRING Google Cloud pub-sub topic pubsub_platform_id STRING Google Cloud platform id associated with topic ddl_pubsub_topic STRING Google Cloud pub-sub topic to send DDL events to pubsub_request_bytes_threshold LONG Set number of bytes until batch is send 1 pubsub_message_count_batch_size LONG Set number of messages until batch is send 1 pubsub_publish_delay_threshold LONG Set time passed in millis until batch is send 1 pubsub_retry_delay LONG Controls the delay in millis before sending the first retry message 100 pubsub_retry_delay_multiplier FLOAT Controls the increase in retry delay per retry 1.3 pubsub_max_retry_delay LONG Puts a limit on the value in seconds of the retry delay 60 pubsub_initial_rpc_timeout LONG Controls the timeout in seconds for the initial RPC 5 pubsub_rpc_timeout_multiplier FLOAT Controls the change in RPC timeout 1.0 pubsub_max_rpc_timeout LONG Puts a limit on the value in seconds of the RPC timeout 600 pubsub_total_timeout LONG Puts a limit on the value in seconds of the retry delay, so that the RetryDelayMultiplier can't increase the retry delay higher than this amount 600 See also: PubSub Producer Documentation bigquery producer option argument description default bigquery_project_id STRING Google Cloud bigquery project id bigquery_dataset STRING Google Cloud bigquery dataset id bigquery_table STRING Google Cloud bigquery table id See also: PubSub Producer Documentation rabbitmq producer option argument description default rabbitmq_user STRING Username of Rabbitmq connection guest rabbitmq_pass STRING Password of Rabbitmq connection guest rabbitmq_host STRING Host of Rabbitmq machine rabbitmq_port INT Port of Rabbitmq machine rabbitmq_virtual_host STRING Virtual Host of Rabbitmq rabbitmq_exchange STRING Name of exchange for rabbitmq publisher rabbitmq_exchange_type STRING Exchange type for rabbitmq rabbitmq_exchange_durable BOOLEAN Exchange durability. false rabbitmq_exchange_autodelete BOOLEAN If set, the exchange is deleted when all queues have finished using it. false rabbitmq_routing_key_template STRING A string template for the routing key, %db% and %table% will be substituted. %db%.%table% . rabbitmq_message_persistent BOOLEAN Eanble message persistence. false rabbitmq_declare_exchange BOOLEAN Should declare the exchange for rabbitmq publisher true See also: RabbitMQ Producer Documentation redis producer option argument description default redis_host STRING Host of Redis server localhost redis_port INT Port of Redis server 6379 redis_auth STRING Authentication key for a password-protected Redis server redis_database INT Database of Redis server 0 redis_type [ pubsub | xadd | lpush | rpush ] Selects either Redis Pub/Sub, Stream, or List. pubsub redis_key STRING Redis channel/key for Pub/Sub, XADD or LPUSH/RPUSH maxwell redis_stream_json_key STRING Redis XADD Stream Message Field Name message redis_sentinels STRING Redis sentinels list in format host1:port1,host2:port2,host3:port3... Must be only used with redis_sentinel_master_name redis_sentinel_master_name STRING Redis sentinel master name. Must be only used with redis_sentinels See also: Redis Producer Documentation formatting option argument description default output_binlog_position BOOLEAN records include binlog position false output_gtid_position BOOLEAN records include gtid position, if available false output_commit_info BOOLEAN records include commit and xid true output_xoffset BOOLEAN records include virtual tx-row offset false output_push_timestamp BOOLEAN records are timestamped with a high-precision value before being sent to the producer false output_nulls BOOLEAN records include fields with NULL values true output_server_id BOOLEAN records include server_id false output_thread_id BOOLEAN records include thread_id false output_schema_id BOOLEAN records include schema_id, schema_id is the id of the latest schema tracked by maxwell and doesn't relate to any mysql tracked value false output_row_query BOOLEAN records include INSERT/UPDATE/DELETE statement. Mysql option \"binlog_rows_query_log_events\" must be enabled false output_primary_keys BOOLEAN DML records include list of values that make up a row's primary key false output_primary_key_columns BOOLEAN DML records include list of columns that make up a row's primary key false output_ddl BOOLEAN output DDL (table-alter, table-create, etc) events false output_null_zerodates BOOLEAN should we transform '0000-00-00' to null? false output_naming_strategy STRING naming strategy of field name of JSON. can be underscore_to_camelcase none filtering option argument description default filter STRING filter rules, eg exclude: db.*, include: *.tbl, include: *./bar(bar)?/, exclude: foo.bar.col=val See also: filtering encryption option argument description default encrypt [ none | data | all ] encrypt mode: none = no encryption. \"data\": encrypt the data field only. all : encrypt entire maxwell message none secret_key string specify the encryption key to be used null high availability option argument description default ha enable maxwell client HA jgroups_config string location of xml configuration file for jGroups $PWD/raft.xml raft_member_id string uniquely identify this node within jgroups-raft cluster See also: High Availability monitoring / metrics option argument description default metrics_prefix STRING the prefix maxwell will apply to all metrics MaxwellMetrics metrics_type [slf4j | jmx | http | datadog] how maxwell metrics will be reported metrics_jvm BOOLEAN enable jvm metrics: memory usage, GC stats, etc. false metrics_slf4j_interval SECONDS the frequency metrics are emitted to the log, in seconds, when slf4j reporting is configured 60 http_port INT the port the server will bind to when http reporting is configured 8080 http_path_prefix STRING http path prefix for the server / http_bind_address STRING the address the server will bind to when http reporting is configured all addresses http_diagnostic BOOLEAN enable http diagnostic endpoint false http_diagnostic_timeout MILLISECONDS the http diagnostic response timeout 10000 metrics_datadog_type [udp | http] when metrics_type includes datadog this is the way metrics will be reported, can only be one of [udp | http] udp metrics_datadog_tags STRING datadog tags that should be supplied, e.g. tag1:value1,tag2:value2 metrics_age_slo INT Latency service level objective threshold in seconds (Optional). When set, a message.publish.age.slo_violation metric is emitted to Datadog if the latency exceeds the threshold metrics_datadog_interval INT the frequency metrics are pushed to datadog, in seconds 60 metrics_datadog_apikey STRING the datadog api key to use when metrics_datadog_type = http metrics_datadog_site STRING the site to publish metrics to when metrics_datadog_type = http us metrics_datadog_host STRING the host to publish metrics to when metrics_datadog_type = udp localhost metrics_datadog_port INT the port to publish metrics to when metrics_datadog_type = udp 8125 See also: Monitoring misc option argument description default bootstrapper [async | sync | none] bootstrapper type. See bootstrapping docs . async init_position FILE:POSITION[:HEARTBEAT] ignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties. replay BOOLEAN enable maxwell's read-only \"replay\" mode: don't store a binlog position or schema changes. Not available in config.properties. buffer_memory_usage FLOAT Determines how much memory the Maxwell event buffer will use from the jvm max memory. Size of the buffer is: buffer_memory_usage * -Xmx\" 0.25 http_config BOOLEAN enable http config endpoint for config updates without restart false binlog_event_queue_size INT Size of queue to buffer events parsed from binlog 5000 LOG_LEVEL: [ debug | info | warn | error ] SSL_OPTION: [ DISABLED | PREFERRED | REQUIRED | VERIFY_CA | VERIFY_IDENTITY ] PRODUCER_TYPE: [ stdout | file | kafka | kinesis | pubsub | sqs | rabbitmq | redis ] DEFAULT_JDBC_OPTS: zeroDateTimeBehavior=convertToNull&connectTimeout=5000 PARTITION_BY: [ database | table | primary_key | transaction_id | column | random ] PARTITION_BY_FALLBACK: [ database | table | primary_key | transaction_id ] KAFKA_VERSION: [ 0.8.2.2 | 0.9.0.1 | 0.10.0.1 | 0.10.2.1 | 0.11.0.1 ] PRODUCER_ACK_TIMEOUT: In certain failure modes, async producers (kafka, kinesis, pubsub, sqs) may simply disappear a message, never notifying maxwell of success or failure. This timeout can be set as a heuristic; after this many milliseconds, maxwell will consider an outstanding message lost and fail it. Configuration methods Maxwell is configurable via the command-line, a properties file, or the environment. The configuration priority is: command line options > scoped env vars > properties file > default values config.properties Maxwell can be configured via a java properties file, specified via --config or named \"config.properties\" in the current working directory. Any command line options (except init_position , replay , kafka_version and daemon ) may be specified as \"key=value\" pairs. via environment If env_config_prefix given via command line or in config.properties , Maxwell will configure itself with all environment variables that match the prefix. The environment variable names are case insensitive. For example, if maxwell is started with --env_config_prefix=FOO_ and the environment contains FOO_USER=auser , this would be equivalent to passing --user=auser . via PATCH: /config If http_config is set to true in config.properties or in the environment, the endpoint /config will be exposed. Currently only filter updates are supported, and a filter can be updated with a request in the following format PATCH: /config { \"filter\": \"exclude: noisy_db.*\" } A get request will return the live config state GET: /config { \"filter\": \"exclude: noisy_db.*\" }","title":"Reference"},{"location":"config/#reference","text":"Configuration options are set either via command line or the \"config.properties\" file.","title":"Reference"},{"location":"config/#general","text":"option argument description default config STRING location of config.properties file $PWD/config.properties log_level LOG_LEVEL log level info daemon running maxwell as a daemon env_config_prefix STRING env vars matching prefix are treated as config values","title":"general"},{"location":"config/#mysql","text":"option argument description default host STRING mysql host localhost user STRING mysql username password STRING mysql password (no password) port INT mysql port 3306 jdbc_options STRING mysql jdbc connection options DEFAULT_JDBC_OPTS ssl SSL_OPT SSL behavior for mysql cx DISABLED schema_database STRING database to store schema and position in maxwell client_id STRING unique text identifier for maxwell instance maxwell replica_server_id LONG unique numeric identifier for this maxwell instance 6379 (see notes ) master_recovery BOOLEAN enable experimental master recovery code false gtid_mode BOOLEAN enable GTID-based replication false recapture_schema BOOLEAN recapture the latest schema. Not available in config.properties. false max_schemas LONG how many schema deltas to keep before triggering compaction operation unlimited binlog_heartbeat BOOLEAN enable binlog heartbeats to detect stale connections DISABLED replication_host STRING server to replicate from. See split server roles schema-store host replication_password STRING password on replication server (none) replication_port INT port on replication server 3306 replication_user STRING user on replication server replication_ssl SSL_OPT SSL behavior for replication cx cx DISABLED replication_jdbc_options STRING mysql jdbc connection options for replication server DEFAULT_JDBC_OPTS schema_host STRING server to capture schema from. See split server roles schema-store host schema_password STRING password on schema-capture server (none) schema_port INT port on schema-capture server 3306 schema_user STRING user on schema-capture server schema_ssl SSL_OPT SSL behavior for schema-capture server DISABLED schema_jdbc_options STRING mysql jdbc connection options for schema server DEFAULT_JDBC_OPTS","title":"mysql"},{"location":"config/#producer-options","text":"option argument description default producer PRODUCER_TYPE type of producer to use stdout custom_producer.factory CLASS_NAME fully qualified custom producer factory class, see example producer_ack_timeout PRODUCER_ACK_TIMEOUT time in milliseconds before async producers consider a message lost producer_partition_by PARTITION_BY input to kafka/kinesis partition function database producer_partition_columns STRING if partitioning by 'column', a comma separated list of columns producer_partition_by_fallback PARTITION_BY_FALLBACK required when producer_partition_by=column. Used when the column is missing ignore_producer_error BOOLEAN When false, Maxwell will terminate on kafka/kinesis/pubsub publish errors (aside from RecordTooLargeException). When true, errors are only logged. See also dead_letter_topic true","title":"producer options"},{"location":"config/#file-producer","text":"option argument description default output_file STRING output file for file producer javascript STRING file containing javascript filters","title":"file producer"},{"location":"config/#kafka-producer","text":"option argument description default kafka.bootstrap.servers STRING kafka brokers, given as HOST:PORT[,HOST:PORT] kafka_topic STRING kafka topic to write to. maxwell dead_letter_topic STRING the topic to write a \"skeleton row\" (a row where data includes only primary key columns) when there's an error publishing a row. When ignore_producer_error is false , only RecordTooLargeException causes a fallback record to be published, since other errors cause termination. Currently only supported in Kafka publisher kafka_version KAFKA_VERSION run maxwell with specified kafka producer version. Not available in config.properties. 0.11.0.1 kafka_partition_hash [ default | murmur3 ] hash function to use when choosing kafka partition default kafka_key_format [ array | hash ] how maxwell outputs kafka keys, either a hash or an array of hashes hash ddl_kafka_topic STRING if output_ddl is true, kafka topic to write DDL changes to kafka_topic See also: Kafka Producer Documentation","title":"kafka producer"},{"location":"config/#kinesis-producer","text":"option argument description default kinesis_stream STRING kinesis stream name See also: Kinesis Producer Documentation","title":"kinesis producer"},{"location":"config/#sqs-producer","text":"option argument description default sqs_queue_uri STRING SQS Queue URI See also: SQS Producer Documentation","title":"sqs producer"},{"location":"config/#sns-producer","text":"option argument description default sns_topic STRING The SNS topic to publish to. FIFO topics should end with .fifo sns_attrs STRING Properties to set as attributes on the SNS message See also: SNS Producer Documentation","title":"sns producer"},{"location":"config/#nats-producer","text":"option argument description default nats_url STRING Comma separated list of nats urls. may include user:password style auth nats://localhost:4222 nats_subject STRING Nats subject hierarchy. Topic substitution available. %{database}.%{table} See also: Nats Producer Documentation","title":"nats producer"},{"location":"config/#pubsub-producer","text":"option argument description default pubsub_topic STRING Google Cloud pub-sub topic pubsub_platform_id STRING Google Cloud platform id associated with topic ddl_pubsub_topic STRING Google Cloud pub-sub topic to send DDL events to pubsub_request_bytes_threshold LONG Set number of bytes until batch is send 1 pubsub_message_count_batch_size LONG Set number of messages until batch is send 1 pubsub_publish_delay_threshold LONG Set time passed in millis until batch is send 1 pubsub_retry_delay LONG Controls the delay in millis before sending the first retry message 100 pubsub_retry_delay_multiplier FLOAT Controls the increase in retry delay per retry 1.3 pubsub_max_retry_delay LONG Puts a limit on the value in seconds of the retry delay 60 pubsub_initial_rpc_timeout LONG Controls the timeout in seconds for the initial RPC 5 pubsub_rpc_timeout_multiplier FLOAT Controls the change in RPC timeout 1.0 pubsub_max_rpc_timeout LONG Puts a limit on the value in seconds of the RPC timeout 600 pubsub_total_timeout LONG Puts a limit on the value in seconds of the retry delay, so that the RetryDelayMultiplier can't increase the retry delay higher than this amount 600 See also: PubSub Producer Documentation","title":"pubsub producer"},{"location":"config/#bigquery-producer","text":"option argument description default bigquery_project_id STRING Google Cloud bigquery project id bigquery_dataset STRING Google Cloud bigquery dataset id bigquery_table STRING Google Cloud bigquery table id See also: PubSub Producer Documentation","title":"bigquery producer"},{"location":"config/#rabbitmq-producer","text":"option argument description default rabbitmq_user STRING Username of Rabbitmq connection guest rabbitmq_pass STRING Password of Rabbitmq connection guest rabbitmq_host STRING Host of Rabbitmq machine rabbitmq_port INT Port of Rabbitmq machine rabbitmq_virtual_host STRING Virtual Host of Rabbitmq rabbitmq_exchange STRING Name of exchange for rabbitmq publisher rabbitmq_exchange_type STRING Exchange type for rabbitmq rabbitmq_exchange_durable BOOLEAN Exchange durability. false rabbitmq_exchange_autodelete BOOLEAN If set, the exchange is deleted when all queues have finished using it. false rabbitmq_routing_key_template STRING A string template for the routing key, %db% and %table% will be substituted. %db%.%table% . rabbitmq_message_persistent BOOLEAN Eanble message persistence. false rabbitmq_declare_exchange BOOLEAN Should declare the exchange for rabbitmq publisher true See also: RabbitMQ Producer Documentation","title":"rabbitmq producer"},{"location":"config/#redis-producer","text":"option argument description default redis_host STRING Host of Redis server localhost redis_port INT Port of Redis server 6379 redis_auth STRING Authentication key for a password-protected Redis server redis_database INT Database of Redis server 0 redis_type [ pubsub | xadd | lpush | rpush ] Selects either Redis Pub/Sub, Stream, or List. pubsub redis_key STRING Redis channel/key for Pub/Sub, XADD or LPUSH/RPUSH maxwell redis_stream_json_key STRING Redis XADD Stream Message Field Name message redis_sentinels STRING Redis sentinels list in format host1:port1,host2:port2,host3:port3... Must be only used with redis_sentinel_master_name redis_sentinel_master_name STRING Redis sentinel master name. Must be only used with redis_sentinels See also: Redis Producer Documentation","title":"redis producer"},{"location":"config/#formatting","text":"option argument description default output_binlog_position BOOLEAN records include binlog position false output_gtid_position BOOLEAN records include gtid position, if available false output_commit_info BOOLEAN records include commit and xid true output_xoffset BOOLEAN records include virtual tx-row offset false output_push_timestamp BOOLEAN records are timestamped with a high-precision value before being sent to the producer false output_nulls BOOLEAN records include fields with NULL values true output_server_id BOOLEAN records include server_id false output_thread_id BOOLEAN records include thread_id false output_schema_id BOOLEAN records include schema_id, schema_id is the id of the latest schema tracked by maxwell and doesn't relate to any mysql tracked value false output_row_query BOOLEAN records include INSERT/UPDATE/DELETE statement. Mysql option \"binlog_rows_query_log_events\" must be enabled false output_primary_keys BOOLEAN DML records include list of values that make up a row's primary key false output_primary_key_columns BOOLEAN DML records include list of columns that make up a row's primary key false output_ddl BOOLEAN output DDL (table-alter, table-create, etc) events false output_null_zerodates BOOLEAN should we transform '0000-00-00' to null? false output_naming_strategy STRING naming strategy of field name of JSON. can be underscore_to_camelcase none","title":"formatting"},{"location":"config/#filtering","text":"option argument description default filter STRING filter rules, eg exclude: db.*, include: *.tbl, include: *./bar(bar)?/, exclude: foo.bar.col=val See also: filtering","title":"filtering"},{"location":"config/#encryption","text":"option argument description default encrypt [ none | data | all ] encrypt mode: none = no encryption. \"data\": encrypt the data field only. all : encrypt entire maxwell message none secret_key string specify the encryption key to be used null","title":"encryption"},{"location":"config/#high-availability","text":"option argument description default ha enable maxwell client HA jgroups_config string location of xml configuration file for jGroups $PWD/raft.xml raft_member_id string uniquely identify this node within jgroups-raft cluster See also: High Availability","title":"high availability"},{"location":"config/#monitoring-metrics","text":"option argument description default metrics_prefix STRING the prefix maxwell will apply to all metrics MaxwellMetrics metrics_type [slf4j | jmx | http | datadog] how maxwell metrics will be reported metrics_jvm BOOLEAN enable jvm metrics: memory usage, GC stats, etc. false metrics_slf4j_interval SECONDS the frequency metrics are emitted to the log, in seconds, when slf4j reporting is configured 60 http_port INT the port the server will bind to when http reporting is configured 8080 http_path_prefix STRING http path prefix for the server / http_bind_address STRING the address the server will bind to when http reporting is configured all addresses http_diagnostic BOOLEAN enable http diagnostic endpoint false http_diagnostic_timeout MILLISECONDS the http diagnostic response timeout 10000 metrics_datadog_type [udp | http] when metrics_type includes datadog this is the way metrics will be reported, can only be one of [udp | http] udp metrics_datadog_tags STRING datadog tags that should be supplied, e.g. tag1:value1,tag2:value2 metrics_age_slo INT Latency service level objective threshold in seconds (Optional). When set, a message.publish.age.slo_violation metric is emitted to Datadog if the latency exceeds the threshold metrics_datadog_interval INT the frequency metrics are pushed to datadog, in seconds 60 metrics_datadog_apikey STRING the datadog api key to use when metrics_datadog_type = http metrics_datadog_site STRING the site to publish metrics to when metrics_datadog_type = http us metrics_datadog_host STRING the host to publish metrics to when metrics_datadog_type = udp localhost metrics_datadog_port INT the port to publish metrics to when metrics_datadog_type = udp 8125 See also: Monitoring","title":"monitoring / metrics"},{"location":"config/#misc","text":"option argument description default bootstrapper [async | sync | none] bootstrapper type. See bootstrapping docs . async init_position FILE:POSITION[:HEARTBEAT] ignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties. replay BOOLEAN enable maxwell's read-only \"replay\" mode: don't store a binlog position or schema changes. Not available in config.properties. buffer_memory_usage FLOAT Determines how much memory the Maxwell event buffer will use from the jvm max memory. Size of the buffer is: buffer_memory_usage * -Xmx\" 0.25 http_config BOOLEAN enable http config endpoint for config updates without restart false binlog_event_queue_size INT Size of queue to buffer events parsed from binlog 5000 LOG_LEVEL: [ debug | info | warn | error ] SSL_OPTION: [ DISABLED | PREFERRED | REQUIRED | VERIFY_CA | VERIFY_IDENTITY ] PRODUCER_TYPE: [ stdout | file | kafka | kinesis | pubsub | sqs | rabbitmq | redis ] DEFAULT_JDBC_OPTS: zeroDateTimeBehavior=convertToNull&connectTimeout=5000 PARTITION_BY: [ database | table | primary_key | transaction_id | column | random ] PARTITION_BY_FALLBACK: [ database | table | primary_key | transaction_id ] KAFKA_VERSION: [ 0.8.2.2 | 0.9.0.1 | 0.10.0.1 | 0.10.2.1 | 0.11.0.1 ] PRODUCER_ACK_TIMEOUT: In certain failure modes, async producers (kafka, kinesis, pubsub, sqs) may simply disappear a message, never notifying maxwell of success or failure. This timeout can be set as a heuristic; after this many milliseconds, maxwell will consider an outstanding message lost and fail it.","title":"misc"},{"location":"config/#configuration-methods","text":"Maxwell is configurable via the command-line, a properties file, or the environment. The configuration priority is: command line options > scoped env vars > properties file > default values","title":"Configuration methods"},{"location":"config/#configproperties","text":"Maxwell can be configured via a java properties file, specified via --config or named \"config.properties\" in the current working directory. Any command line options (except init_position , replay , kafka_version and daemon ) may be specified as \"key=value\" pairs.","title":"config.properties"},{"location":"config/#via-environment","text":"If env_config_prefix given via command line or in config.properties , Maxwell will configure itself with all environment variables that match the prefix. The environment variable names are case insensitive. For example, if maxwell is started with --env_config_prefix=FOO_ and the environment contains FOO_USER=auser , this would be equivalent to passing --user=auser .","title":"via environment"},{"location":"config/#via-patch-config","text":"If http_config is set to true in config.properties or in the environment, the endpoint /config will be exposed. Currently only filter updates are supported, and a filter can be updated with a request in the following format PATCH: /config { \"filter\": \"exclude: noisy_db.*\" } A get request will return the live config state GET: /config { \"filter\": \"exclude: noisy_db.*\" }","title":"via PATCH: /config"},{"location":"dataformat/","text":"So you ran some sql? create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' ); insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.'; update test.e set m = 5.444, c = now(3) where id = 1; delete from test.e where id = 1; alter table test.e add column torvalds bigint unsigned after m; drop table test.e; Maxwell will produce some output for that. Let's look at it. INSERT mysql> insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.'; { \"database\":\"test\", \"table\":\"e\", \"type\":\"insert\", \"ts\":1477053217, \"xid\":23396, \"commit\":true, \"position\":\"master.000006:800911\", \"server_id\":23042, \"thread_id\":108, \"primary_key\": [1, \"2016-10-21 05:33:37.523000\"], \"primary_key_columns\": [\"id\", \"c\"], \"data\":{ \"id\":1, \"m\":4.2341, \"c\":\"2016-10-21 05:33:37.523000\", \"comment\":\"I am a creature of light.\" } } Most of the fields are self-explanatory, but a couple of them deserve mention: \u21b3 \"type\":\"insert\", Most commonly you will see insert/update/delete here. If you're bootstrapping a table, you will see \"bootstrap-insert\", and DDL statements (explained later) have their own types. \u21b3 \"xid\":23396, This is InnoDB's \"transaction ID\" for the transaction this row is associated with. It's unique within the lifetime of a server as near as I can tell. \u21b3 \"server_id\":23042, The mysql server_id of the server that accepted this transaction. \u21b3 \"thread_id\":108, A thread_id is more or less a unique identifier of the client connection that generated the data. \u21b3 \"commit\":true, If you need to re-assemble transactions in your stream processors, you can use this field and xid to do so. The data will look like: row with no commit , xid=142 row with no commit , xid=142 row with commit=true , xid=142 row with no commit , xid=155 ... \u21b3 \"primary_key\": [1,\"2016-10-21 05:33:37.523000\"], You only get this with --output_primary_key. List of values that make up the primary key for this row. \u21b3 \"primary_key_columns\": [\"id\",\"c\"], You only get this with --output_primary_key_columns. List of columns that make make up the primary key for this row. UPDATE mysql> update test.e set m = 5.444, c = now(3) where id = 1; { \"database\":\"test\", \"table\":\"e\", \"type\":\"update\", \"ts\":1477053234, ... \"data\":{ \"id\":1, \"m\":5.444, \"c\":\"2016-10-21 05:33:54.631000\", \"comment\":\"I am a creature of light.\" }, \"old\":{ \"m\":4.2341, \"c\":\"2016-10-21 05:33:37.523000\" } } What's important to note here is the old field, which stores old values for rows that changed. So data still has a complete copy of the row (just as with the insert), but now you can reconstruct what the row was by doing data.merge(old) . DELETE mysql> delete from test.e where id = 1; { \"database\":\"test\", \"table\":\"e\", \"type\":\"delete\", ... \"data\":{ \"id\":1, \"m\":5.444, \"c\":\"2016-10-21 05:33:54.631000\", \"comment\":\"I am a creature of light.\" } } after a DELETE, data contains a copy of the row, just before it shuffled off this mortal coil. CREATE TABLE create table test.e ( ... ) { \"type\":\"table-create\", \"database\":\"test\", \"table\":\"e\", \"def\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"ts\":1477053126000, \"sql\":\"create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' )\", \"position\":\"master.000006:800050\" } You only get this with --output_ddl . \u21b3 \"type\": \"table-create\" here you have database-create , database-alter , database-drop , table-create , table-alter , table-drop . \u21b3 \"type\":\"int\", Mostly here we preserve the inbound type of the column. There's a couple of exceptions where we will change the column type, you could read about them in the unalias_type function if you so desired. ALTER TABLE mysql> alter table test.e add column torvalds bigint unsigned after m; { \"type\":\"table-alter\", \"database\":\"test\", \"table\":\"e\", \"old\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"def\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"bigint\", \"name\":\"torvalds\", \"signed\":false }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"ts\":1477053308000, \"sql\":\"alter table test.e add column torvalds bigint unsigned after m\", \"position\":\"master.000006:804398\" } As with the CREATE TABLE, we have a complete image of the table before-and-after the alter blob (+ binary encoded strings) Maxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding). datetime Datetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings. Note that mysql has no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and Maxwell chooses to reproduce these invalid datetimes faithfully, for lack of something better to do. mysql> create table test_datetime ( id int(11), dtcol datetime ); mysql> insert into test_datetime set dtcol='0000-00-00 00:00:00'; <maxwell { \"table\" : \"test_datetime\", \"type\": \"insert\", \"data\": { \"dtcol\": \"0000-00-00 00:00:00\" } } As of 1.3.0, Maxwell supports microsecond precision datetime/timestamp/time columns. sets output as JSON arrays. mysql> create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') ); mysql> insert into test_sets set setcol = 'b_val,c_val'; <maxwell { \"table\":\"test_sets\", \"type\":\"insert\", \"data\": { \"setcol\": [\"b_val\", \"c_val\"] } } strings (varchar, text) Maxwell will accept a variety of character encodings, but will always output UTF-8 strings. The following table describes support for mysql's character sets: charset status utf8 supported utf8mb4 supported latin1 supported latin2 supported ascii supported ucs2 supported binary supported (as base64) utf16 supported, not tested in production utf32 supported, not tested in production big5 supported, not tested in production cp850 supported, not tested in production sjis supported, not tested in production hebrew supported, not tested in production tis620 supported, not tested in production euckr supported, not tested in production gb2312 supported, not tested in production greek supported, not tested in production cp1250 supported, not tested in production gbk supported, not tested in production latin5 supported, not tested in production macroman supported, not tested in production cp852 supported, not tested in production cp1251 supported, not tested in production cp866 supported, not tested in production cp1256 supported, not tested in production cp1257 supported, not tested in production dec8 unsupported hp8 unsupported koi8r unsupported swe7 unsupported ujis unsupported koi8u unsupported armscii8 unsupported keybcs2 unsupported macce unsupported latin7 unsupported geostd8 unsupported cp932 unsupported eucjpms unsupported","title":"Data Format"},{"location":"dataformat/#so-you-ran-some-sql","text":"create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' ); insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.'; update test.e set m = 5.444, c = now(3) where id = 1; delete from test.e where id = 1; alter table test.e add column torvalds bigint unsigned after m; drop table test.e; Maxwell will produce some output for that. Let's look at it.","title":"So you ran some sql?"},{"location":"dataformat/#insert","text":"mysql> insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.'; { \"database\":\"test\", \"table\":\"e\", \"type\":\"insert\", \"ts\":1477053217, \"xid\":23396, \"commit\":true, \"position\":\"master.000006:800911\", \"server_id\":23042, \"thread_id\":108, \"primary_key\": [1, \"2016-10-21 05:33:37.523000\"], \"primary_key_columns\": [\"id\", \"c\"], \"data\":{ \"id\":1, \"m\":4.2341, \"c\":\"2016-10-21 05:33:37.523000\", \"comment\":\"I am a creature of light.\" } } Most of the fields are self-explanatory, but a couple of them deserve mention: \u21b3 \"type\":\"insert\", Most commonly you will see insert/update/delete here. If you're bootstrapping a table, you will see \"bootstrap-insert\", and DDL statements (explained later) have their own types. \u21b3 \"xid\":23396, This is InnoDB's \"transaction ID\" for the transaction this row is associated with. It's unique within the lifetime of a server as near as I can tell. \u21b3 \"server_id\":23042, The mysql server_id of the server that accepted this transaction. \u21b3 \"thread_id\":108, A thread_id is more or less a unique identifier of the client connection that generated the data. \u21b3 \"commit\":true, If you need to re-assemble transactions in your stream processors, you can use this field and xid to do so. The data will look like: row with no commit , xid=142 row with no commit , xid=142 row with commit=true , xid=142 row with no commit , xid=155 ... \u21b3 \"primary_key\": [1,\"2016-10-21 05:33:37.523000\"], You only get this with --output_primary_key. List of values that make up the primary key for this row. \u21b3 \"primary_key_columns\": [\"id\",\"c\"], You only get this with --output_primary_key_columns. List of columns that make make up the primary key for this row.","title":"INSERT"},{"location":"dataformat/#update","text":"mysql> update test.e set m = 5.444, c = now(3) where id = 1; { \"database\":\"test\", \"table\":\"e\", \"type\":\"update\", \"ts\":1477053234, ... \"data\":{ \"id\":1, \"m\":5.444, \"c\":\"2016-10-21 05:33:54.631000\", \"comment\":\"I am a creature of light.\" }, \"old\":{ \"m\":4.2341, \"c\":\"2016-10-21 05:33:37.523000\" } } What's important to note here is the old field, which stores old values for rows that changed. So data still has a complete copy of the row (just as with the insert), but now you can reconstruct what the row was by doing data.merge(old) .","title":"UPDATE"},{"location":"dataformat/#delete","text":"mysql> delete from test.e where id = 1; { \"database\":\"test\", \"table\":\"e\", \"type\":\"delete\", ... \"data\":{ \"id\":1, \"m\":5.444, \"c\":\"2016-10-21 05:33:54.631000\", \"comment\":\"I am a creature of light.\" } } after a DELETE, data contains a copy of the row, just before it shuffled off this mortal coil.","title":"DELETE"},{"location":"dataformat/#create-table","text":"create table test.e ( ... ) { \"type\":\"table-create\", \"database\":\"test\", \"table\":\"e\", \"def\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"ts\":1477053126000, \"sql\":\"create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' )\", \"position\":\"master.000006:800050\" } You only get this with --output_ddl . \u21b3 \"type\": \"table-create\" here you have database-create , database-alter , database-drop , table-create , table-alter , table-drop . \u21b3 \"type\":\"int\", Mostly here we preserve the inbound type of the column. There's a couple of exceptions where we will change the column type, you could read about them in the unalias_type function if you so desired.","title":"CREATE TABLE"},{"location":"dataformat/#alter-table","text":"mysql> alter table test.e add column torvalds bigint unsigned after m; { \"type\":\"table-alter\", \"database\":\"test\", \"table\":\"e\", \"old\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"def\":{ \"database\":\"test\", \"charset\":\"utf8mb4\", \"table\":\"e\", \"columns\":[ { \"type\":\"int\", \"name\":\"id\", \"signed\":true }, { \"type\":\"double\", \"name\":\"m\" }, { \"type\":\"bigint\", \"name\":\"torvalds\", \"signed\":false }, { \"type\":\"timestamp\", \"name\":\"c\", \"column-length\":6 }, { \"type\":\"varchar\", \"name\":\"comment\", \"charset\":\"latin1\" } ], \"primary-key\":[ \"id\" ] }, \"ts\":1477053308000, \"sql\":\"alter table test.e add column torvalds bigint unsigned after m\", \"position\":\"master.000006:804398\" } As with the CREATE TABLE, we have a complete image of the table before-and-after the alter","title":"ALTER TABLE"},{"location":"dataformat/#blob-binary-encoded-strings","text":"Maxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding).","title":"blob (+ binary encoded strings)"},{"location":"dataformat/#datetime","text":"Datetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings. Note that mysql has no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and Maxwell chooses to reproduce these invalid datetimes faithfully, for lack of something better to do. mysql> create table test_datetime ( id int(11), dtcol datetime ); mysql> insert into test_datetime set dtcol='0000-00-00 00:00:00'; <maxwell { \"table\" : \"test_datetime\", \"type\": \"insert\", \"data\": { \"dtcol\": \"0000-00-00 00:00:00\" } } As of 1.3.0, Maxwell supports microsecond precision datetime/timestamp/time columns.","title":"datetime"},{"location":"dataformat/#sets","text":"output as JSON arrays. mysql> create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') ); mysql> insert into test_sets set setcol = 'b_val,c_val'; <maxwell { \"table\":\"test_sets\", \"type\":\"insert\", \"data\": { \"setcol\": [\"b_val\", \"c_val\"] } }","title":"sets"},{"location":"dataformat/#strings-varchar-text","text":"Maxwell will accept a variety of character encodings, but will always output UTF-8 strings. The following table describes support for mysql's character sets: charset status utf8 supported utf8mb4 supported latin1 supported latin2 supported ascii supported ucs2 supported binary supported (as base64) utf16 supported, not tested in production utf32 supported, not tested in production big5 supported, not tested in production cp850 supported, not tested in production sjis supported, not tested in production hebrew supported, not tested in production tis620 supported, not tested in production euckr supported, not tested in production gb2312 supported, not tested in production greek supported, not tested in production cp1250 supported, not tested in production gbk supported, not tested in production latin5 supported, not tested in production macroman supported, not tested in production cp852 supported, not tested in production cp1251 supported, not tested in production cp866 supported, not tested in production cp1256 supported, not tested in production cp1257 supported, not tested in production dec8 unsupported hp8 unsupported koi8r unsupported swe7 unsupported ujis unsupported koi8u unsupported armscii8 unsupported keybcs2 unsupported macce unsupported latin7 unsupported geostd8 unsupported cp932 unsupported eucjpms unsupported","title":"strings (varchar, text)"},{"location":"deployment/","text":"Reconfiguring a running mysql instance: mysql> set global binlog_format=ROW; mysql> set global binlog_row_image=FULL; note: binlog_format is a session-based property. You will need to shutdown all active connections to fully convert to row-based replication. GTID support Maxwell contains support for GTID-based replication . Enable it with the --gtid_mode configuration param. Here's how you might configure your mysql server for GTID mode: $ vi my.cnf [mysqld] server_id=1 log-bin=master binlog_format=row gtid-mode=ON log-slave-updates=ON enforce-gtid-consistency=true When in GTID-mode, Maxwell will transparently pick up a new replication position after a master change. Note that you will still have to re-point maxwell to the new master (or use a floating VIP) RDS To run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following: set binlog_format to \"ROW\". Do this in the \"parameter groups\" section. For a Mysql-RDS instance this parameter will be in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\". setup RDS binlog retention as described here . The tl;dr is to execute call mysql.rds_set_configuration('binlog retention hours', 24) on the server. Replicating and storing schema from different servers Maxwell uses MySQL for 3 different functions: A host to store the captured schema in ( --host ). A host to replicate binlogs from ( --replication_host ). A host to capture the schema from ( --schema_host ). Often, all three hosts are the same. host and replication_host should be different if maxwell is chained off a replica. schema_host should only be used when using the maxscale replication proxy. Multiple Maxwells Maxwell can operate with multiple instances running against a single master, in different configurations. This can be useful if you wish to have producers running in different configurations, for example producing different groups of tables to different topics. Each instance of Maxwell must be configured with a unique client_id , in order to store unique binlog positions. With MySQL 5.5 and below, each replicator (be it mysql, maxwell, whatever) must also be configured with a unique replica_server_id . This is a 32-bit integer that corresponds to mysql's server_id parameter. The value you configure should be unique across all mysql and maxwell instances.","title":"Deployment"},{"location":"deployment/#reconfiguring-a-running-mysql-instance","text":"mysql> set global binlog_format=ROW; mysql> set global binlog_row_image=FULL; note: binlog_format is a session-based property. You will need to shutdown all active connections to fully convert to row-based replication.","title":"Reconfiguring a running mysql instance:"},{"location":"deployment/#gtid-support","text":"Maxwell contains support for GTID-based replication . Enable it with the --gtid_mode configuration param. Here's how you might configure your mysql server for GTID mode: $ vi my.cnf [mysqld] server_id=1 log-bin=master binlog_format=row gtid-mode=ON log-slave-updates=ON enforce-gtid-consistency=true When in GTID-mode, Maxwell will transparently pick up a new replication position after a master change. Note that you will still have to re-point maxwell to the new master (or use a floating VIP)","title":"GTID support"},{"location":"deployment/#rds","text":"To run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following: set binlog_format to \"ROW\". Do this in the \"parameter groups\" section. For a Mysql-RDS instance this parameter will be in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\". setup RDS binlog retention as described here . The tl;dr is to execute call mysql.rds_set_configuration('binlog retention hours', 24) on the server.","title":"RDS"},{"location":"deployment/#replicating-and-storing-schema-from-different-servers","text":"Maxwell uses MySQL for 3 different functions: A host to store the captured schema in ( --host ). A host to replicate binlogs from ( --replication_host ). A host to capture the schema from ( --schema_host ). Often, all three hosts are the same. host and replication_host should be different if maxwell is chained off a replica. schema_host should only be used when using the maxscale replication proxy.","title":"Replicating and storing schema from different servers"},{"location":"deployment/#multiple-maxwells","text":"Maxwell can operate with multiple instances running against a single master, in different configurations. This can be useful if you wish to have producers running in different configurations, for example producing different groups of tables to different topics. Each instance of Maxwell must be configured with a unique client_id , in order to store unique binlog positions. With MySQL 5.5 and below, each replicator (be it mysql, maxwell, whatever) must also be configured with a unique replica_server_id . This is a 32-bit integer that corresponds to mysql's server_id parameter. The value you configure should be unique across all mysql and maxwell instances.","title":"Multiple Maxwells"},{"location":"embedding/","text":"Embedding Maxwell Maxwell typically runs as a command-line program. However, for advanced uses it is possible to run maxwell from any JVM-based language. Some fairly incomplete API documentation is available here: https://maxwells-daemon.io/apidocs Compatibility caveat Maxwell makes every attempt to remain backwards compatible. However this only applies to the command-line usage - Maxwell's Java API may change without notice. However (and unless otherwise indicated) breaking API changes will result in a type error - i.e. if your code still compiles, then the API has not changed.","title":"Embedding"},{"location":"embedding/#embedding-maxwell","text":"Maxwell typically runs as a command-line program. However, for advanced uses it is possible to run maxwell from any JVM-based language. Some fairly incomplete API documentation is available here: https://maxwells-daemon.io/apidocs","title":"Embedding Maxwell"},{"location":"embedding/#compatibility-caveat","text":"Maxwell makes every attempt to remain backwards compatible. However this only applies to the command-line usage - Maxwell's Java API may change without notice. However (and unless otherwise indicated) breaking API changes will result in a type error - i.e. if your code still compiles, then the API has not changed.","title":"Compatibility caveat"},{"location":"encryption/","text":"Using encryption When encryption is enabled, maxwell will encrypt messages using a AES/CBC/PKCS5PADDING cipher with your own encryption key. Values are first encrypted and then base64 encoded, an initialization vector is randomly generated and put into the final message Decryption To decrypt your data you must first decode the string from base64 and then apply the cipher to decrypt. A sample implementation is provided in RowEncrypt.decrypt(). Examples insert into minimal set account_id =1, text_field='hello' encrypt=none (unencrypted): {\"database\":\"shard_1\",\"table\":\"minimal\",\"type\":\"insert\",\"ts\":1490115785,\"xid\":153,\"commit\":true,\"data\":{\"id\":1,\"account_id\":1,\"text_field\":\"hello\"}} encrypt=data : {\"database\":\"shard_1\",\"table\":\"minimal\",\"type\":\"insert\",\"ts\":1504585129,\"xid\":161,\"commit\":true,\"encrypted\":{\"iv\":\"lqiXoTdz6jed3XgJPpa7EQ==\",\"bytes\":\"1soc4leskiIm6yuT2D49VA3AYVKCvN+0wh+8d1iwSZETK7N2pG4HDbqnVpJUUCOaKjpcPlP7Sc7Z3SPhGD5JeA==\"}} encrypt=all : {\"encrypted\":{\"bytes\":\"iZssjWfzS0NlqIj82ddpvoQeKSx4D3GIPSCgjdkpgQlCWzN2p3VVZOn3Oj1x4w+a6dVhoFmllWxBK6aAkdVK9t6Vt1+um6lWwSeXNQIL/RbknW5Q8I9emm5bC1Dd1LftBuX/1Uw0wjbsq8Qt3HErvmmiIMe2S27EIWshvBnmw9MibryjLD0brvIbFFxwDuSQuVA4OFyV9TN32N/ZXiBwIA==\",\"iv\":\"XXs6AePsXJWAAIrKyLlR0g==\"}}","title":"Encryption"},{"location":"encryption/#using-encryption","text":"When encryption is enabled, maxwell will encrypt messages using a AES/CBC/PKCS5PADDING cipher with your own encryption key. Values are first encrypted and then base64 encoded, an initialization vector is randomly generated and put into the final message","title":"Using encryption"},{"location":"encryption/#decryption","text":"To decrypt your data you must first decode the string from base64 and then apply the cipher to decrypt. A sample implementation is provided in RowEncrypt.decrypt().","title":"Decryption"},{"location":"encryption/#examples","text":"insert into minimal set account_id =1, text_field='hello' encrypt=none (unencrypted): {\"database\":\"shard_1\",\"table\":\"minimal\",\"type\":\"insert\",\"ts\":1490115785,\"xid\":153,\"commit\":true,\"data\":{\"id\":1,\"account_id\":1,\"text_field\":\"hello\"}} encrypt=data : {\"database\":\"shard_1\",\"table\":\"minimal\",\"type\":\"insert\",\"ts\":1504585129,\"xid\":161,\"commit\":true,\"encrypted\":{\"iv\":\"lqiXoTdz6jed3XgJPpa7EQ==\",\"bytes\":\"1soc4leskiIm6yuT2D49VA3AYVKCvN+0wh+8d1iwSZETK7N2pG4HDbqnVpJUUCOaKjpcPlP7Sc7Z3SPhGD5JeA==\"}} encrypt=all : {\"encrypted\":{\"bytes\":\"iZssjWfzS0NlqIj82ddpvoQeKSx4D3GIPSCgjdkpgQlCWzN2p3VVZOn3Oj1x4w+a6dVhoFmllWxBK6aAkdVK9t6Vt1+um6lWwSeXNQIL/RbknW5Q8I9emm5bC1Dd1LftBuX/1Uw0wjbsq8Qt3HErvmmiIMe2S27EIWshvBnmw9MibryjLD0brvIbFFxwDuSQuVA4OFyV9TN32N/ZXiBwIA==\",\"iv\":\"XXs6AePsXJWAAIrKyLlR0g==\"}}","title":"Examples"},{"location":"filtering/","text":"Basic Filters Maxwell can be configured to filter out updates from specific tables. This is controlled by the --filter command line flag. Example 1 --filter = 'exclude: foodb.*, include: foodb.tbl, include: foodb./table_\\d+/' This example tells Maxwell to suppress all updates that happen on foodb , except for updates to tbl and any table in foodb matching the regexp /table_\\d+/ . Example 2 Filter options are evaluated in the order specified, so in this example we suppress everything except updates in the db1 database. --filter = 'exclude: *.*, include: db1.*' Column Filters Maxwell can also include/exclude based on column values: --filter = 'exclude: db.tbl.col = reject' will reject any row in db.tbl that contains col and where the stringified value of \"col\" is \"reject\". Column filters are ignored if the specified column is not present, so --filter = 'exclude: *.*.col_a = *' will exclude updates to any table that contains col_a , but include every other table. Blacklisting In rare cases, you may wish to tell Maxwell to completely ignore a database or table, including schema changes. In general, don't use this. If you must use this: --filter = 'blacklist: bad_db.*' Note that once Maxwell has been running with a table or database marked as blacklisted, you must continue to run Maxwell with that table or database blacklisted or else Maxwell will halt. If you want to stop blacklisting a table or database, you will have to drop the maxwell schema first. Also note that this is the feature I most regret writing. Javascript Filters If you need more flexibility than the native filters provide, you can write a small chunk of javascript for Maxwell to pass each row through with --javascript FILE . This file should contain at least a javascript function named process_row . This function will be passed a WrappedRowMap object and is free to make filtering and data munging decisions: function process_row(row) { if ( row.database == \"test\" && row.table == \"bar\" ) { var username = row.data.get(\"username\"); if ( username == \"osheroff\" ) row.suppress(); row.data.put(\"username\", username.toUpperCase()); } } There's a longer example here: https://github.com/zendesk/maxwell/blob/master/src/example/filter.js .","title":"Filtering"},{"location":"filtering/#basic-filters","text":"Maxwell can be configured to filter out updates from specific tables. This is controlled by the --filter command line flag.","title":"Basic Filters"},{"location":"filtering/#example-1","text":"--filter = 'exclude: foodb.*, include: foodb.tbl, include: foodb./table_\\d+/' This example tells Maxwell to suppress all updates that happen on foodb , except for updates to tbl and any table in foodb matching the regexp /table_\\d+/ .","title":"Example 1"},{"location":"filtering/#example-2","text":"Filter options are evaluated in the order specified, so in this example we suppress everything except updates in the db1 database. --filter = 'exclude: *.*, include: db1.*'","title":"Example 2"},{"location":"filtering/#column-filters","text":"Maxwell can also include/exclude based on column values: --filter = 'exclude: db.tbl.col = reject' will reject any row in db.tbl that contains col and where the stringified value of \"col\" is \"reject\". Column filters are ignored if the specified column is not present, so --filter = 'exclude: *.*.col_a = *' will exclude updates to any table that contains col_a , but include every other table.","title":"Column Filters"},{"location":"filtering/#blacklisting","text":"In rare cases, you may wish to tell Maxwell to completely ignore a database or table, including schema changes. In general, don't use this. If you must use this: --filter = 'blacklist: bad_db.*' Note that once Maxwell has been running with a table or database marked as blacklisted, you must continue to run Maxwell with that table or database blacklisted or else Maxwell will halt. If you want to stop blacklisting a table or database, you will have to drop the maxwell schema first. Also note that this is the feature I most regret writing.","title":"Blacklisting"},{"location":"filtering/#javascript-filters","text":"If you need more flexibility than the native filters provide, you can write a small chunk of javascript for Maxwell to pass each row through with --javascript FILE . This file should contain at least a javascript function named process_row . This function will be passed a WrappedRowMap object and is free to make filtering and data munging decisions: function process_row(row) { if ( row.database == \"test\" && row.table == \"bar\" ) { var username = row.data.get(\"username\"); if ( username == \"osheroff\" ) row.suppress(); row.data.put(\"username\", username.toUpperCase()); } } There's a longer example here: https://github.com/zendesk/maxwell/blob/master/src/example/filter.js .","title":"Javascript Filters"},{"location":"high_availability/","text":"High Availabilty As v1.29.1, Maxwell contains experiemental (alpha quality) client-side HA. Support for performing leader elections is done via jgroups-raft . Getting started First, copy raft.xml.example to raft.xml . Edit to your liking, paying attention to: <raft.RAFT members=\"A,B,C\" raft_id=\"${raft_id:undefined}\"/> Note that because we are using a RAFT-based leader election, we will have to spin up at least 3 maxwell client nodes. Now start each of your HA maxwell nodes like this: host1: $ bin/maxwell --ha --raft_member_id=A host2: $ bin/maxwell --ha --raft_member_id=B host3: $ bin/maxwell --ha --raft_member_id=C if all goes well, the 3 nodes will communicate via multicast/UDP, elect one to be the cluster leader, and away you will go. If one node is terminated or partitioned, a new election will be held to replace it. Getting deeper More advanced (especially inter-DC) configurations may be implemented by editing raft.xml ; you'll probably need to get the nodes to communicate with each other via TCP instead of UDP, and maybe tunnel through a firewall or two, good stuff like that. It's of course out of scope for this document, so check out the jgroups documentation , but if you come up with something good drop me a line. Common problems Something I encountered right out of the gate was this: 12:37:53,135 WARN UDP - failed to join /224.0.75.75:7500 on utun0: java.net.SocketException: Can't assign requested address which can be worked around by forcing the JVM onto an ipv4 stack: JAVA_OPTS=\"-Djava.net.preferIPv4Stack=true\" bin/maxwell --ha --raft_member_id=B","title":"High Availability"},{"location":"high_availability/#high-availabilty","text":"As v1.29.1, Maxwell contains experiemental (alpha quality) client-side HA. Support for performing leader elections is done via jgroups-raft .","title":"High Availabilty"},{"location":"high_availability/#getting-started","text":"First, copy raft.xml.example to raft.xml . Edit to your liking, paying attention to: <raft.RAFT members=\"A,B,C\" raft_id=\"${raft_id:undefined}\"/> Note that because we are using a RAFT-based leader election, we will have to spin up at least 3 maxwell client nodes. Now start each of your HA maxwell nodes like this: host1: $ bin/maxwell --ha --raft_member_id=A host2: $ bin/maxwell --ha --raft_member_id=B host3: $ bin/maxwell --ha --raft_member_id=C if all goes well, the 3 nodes will communicate via multicast/UDP, elect one to be the cluster leader, and away you will go. If one node is terminated or partitioned, a new election will be held to replace it.","title":"Getting started"},{"location":"high_availability/#getting-deeper","text":"More advanced (especially inter-DC) configurations may be implemented by editing raft.xml ; you'll probably need to get the nodes to communicate with each other via TCP instead of UDP, and maybe tunnel through a firewall or two, good stuff like that. It's of course out of scope for this document, so check out the jgroups documentation , but if you come up with something good drop me a line.","title":"Getting deeper"},{"location":"high_availability/#common-problems","text":"Something I encountered right out of the gate was this: 12:37:53,135 WARN UDP - failed to join /224.0.75.75:7500 on utun0: java.net.SocketException: Can't assign requested address which can be worked around by forcing the JVM onto an ipv4 stack: JAVA_OPTS=\"-Djava.net.preferIPv4Stack=true\" bin/maxwell --ha --raft_member_id=B","title":"Common problems"},{"location":"monitoring/","text":"Monitoring Maxwell exposes certain metrics through either its base logging mechanism, JMX, HTTP or by push to Datadog. This is configurable through commandline options or the config.properties file. These can provide insight into system health. Metrics All metrics are prepended with the configured metrics_prefix. metric description Counters messages.succeeded count of messages that were successfully sent to Kafka messages.failed count of messages that failed to send to Kafka row.count a count of rows that have been processed from the binlog. note that not every row results in a message being sent to Kafka. Meters messages.succeeded.meter a measure of the rate at which messages were successfully sent to Kafka messages.failed.meter a measure of the rate at which messages failed to send Kafka row.meter a measure of the rate at which rows arrive to Maxwell from the binlog connector Gauges replication.lag the time elapsed between the database transaction commit and the time it was processed by Maxwell, in milliseconds inflightmessages.count the number of messages that are currently in-flight (awaiting acknowledgement from the destination, or ahead of messages which are) Timers message.publish.time the time it took to send a given record to Kafka, in milliseconds message.publish.age the time between an event occurring on the DB and being published to kafka, in milliseconds. Note: since MySQL timestamps are accurate to the second, this is only accurate to +/- 500ms. replication.queue.time the time it took to enqueue a given binlog event for processing, in milliseconds HTTP Endpoints When the HTTP server is enabled the following endpoints are exposed: endpoint description /metrics return all metrics as JSON /prometheus return all metrics as Prometheus format /healthcheck run Maxwell's healthchecks. Considered unhealthy if >0 messages have failed in the last 15 minutes. /ping a simple ping test, responds with pong JMX Configuration Standard configuration is either via commandline args or the config.properties file. However, when exposing JMX metrics additional configuration is required to allow remote access. In this case Maxwell makes use of the JAVA_OPTS environment variable. To make use of this set JAVA_OPTS before starting Maxwell. The following is an example which allows remote access with no authentication and insecure connections. export JAVA_OPTS=\"-Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.port=9010 \\ -Dcom.sun.management.jmxremote.local.only=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Djava.rmi.server.hostname=SERVER_IP_ADDRESS\"","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"Maxwell exposes certain metrics through either its base logging mechanism, JMX, HTTP or by push to Datadog. This is configurable through commandline options or the config.properties file. These can provide insight into system health.","title":"Monitoring"},{"location":"monitoring/#metrics","text":"All metrics are prepended with the configured metrics_prefix. metric description Counters messages.succeeded count of messages that were successfully sent to Kafka messages.failed count of messages that failed to send to Kafka row.count a count of rows that have been processed from the binlog. note that not every row results in a message being sent to Kafka. Meters messages.succeeded.meter a measure of the rate at which messages were successfully sent to Kafka messages.failed.meter a measure of the rate at which messages failed to send Kafka row.meter a measure of the rate at which rows arrive to Maxwell from the binlog connector Gauges replication.lag the time elapsed between the database transaction commit and the time it was processed by Maxwell, in milliseconds inflightmessages.count the number of messages that are currently in-flight (awaiting acknowledgement from the destination, or ahead of messages which are) Timers message.publish.time the time it took to send a given record to Kafka, in milliseconds message.publish.age the time between an event occurring on the DB and being published to kafka, in milliseconds. Note: since MySQL timestamps are accurate to the second, this is only accurate to +/- 500ms. replication.queue.time the time it took to enqueue a given binlog event for processing, in milliseconds","title":"Metrics"},{"location":"monitoring/#http-endpoints","text":"When the HTTP server is enabled the following endpoints are exposed: endpoint description /metrics return all metrics as JSON /prometheus return all metrics as Prometheus format /healthcheck run Maxwell's healthchecks. Considered unhealthy if >0 messages have failed in the last 15 minutes. /ping a simple ping test, responds with pong","title":"HTTP Endpoints"},{"location":"monitoring/#jmx-configuration","text":"Standard configuration is either via commandline args or the config.properties file. However, when exposing JMX metrics additional configuration is required to allow remote access. In this case Maxwell makes use of the JAVA_OPTS environment variable. To make use of this set JAVA_OPTS before starting Maxwell. The following is an example which allows remote access with no authentication and insecure connections. export JAVA_OPTS=\"-Dcom.sun.management.jmxremote \\ -Dcom.sun.management.jmxremote.port=9010 \\ -Dcom.sun.management.jmxremote.local.only=false \\ -Dcom.sun.management.jmxremote.authenticate=false \\ -Dcom.sun.management.jmxremote.ssl=false \\ -Djava.rmi.server.hostname=SERVER_IP_ADDRESS\"","title":"JMX Configuration"},{"location":"producers/","text":"Kafka The Kafka producer is perhaps the most production hardened of all the producers, having run on high traffic instances at WEB scale. Topic Maxwell writes to a kafka topic named \"maxwell\" by default. It is configurable via --kafka_topic . The given topic can be a plain string or a dynamic string, e.g. namespace_%{database}_%{table} , where the topic will be generated from data in the row. Client version By default, maxwell uses the kafka 1.0.0 library. The --kafka_version flag lets you choose an alternate library version: 0.8.2.2, 0.9.0.1, 0.10.0.1, 0.10.2.1 or 0.11.0.1, 1.0.0. This flag is only available on the command line. The 0.8.2.2 client is only compatible with brokers running kafka 0.8. The 0.10.0.x client is only compatible with brokers 0.10.0.x or later. Mixing the 0.10 client with other versions can lead to serious performance impacts. For More details, read about it here . The 0.11.0 client can talk to version 0.10.0 or newer brokers. The 0.9.0.1 client is not compatible with brokers running kafka 0.8. The exception below will show in logs when that is the case: ERROR Sender - Uncaught error in kafka producer I/O thread: SchemaException: Error reading field 'throttle_time_ms': java.nio.BufferUnderflowException Passing options to kafka Any options present in config.properties that are prefixed with kafka. will be passed into the Kafka producer library (with kafka. stripped off, see below for examples). We use the \"new producer\" configuration, as described here: http://kafka.apache.org/documentation.html#newproducerconfigs Example kafka configs Highest throughput These properties would give high throughput performance. kafka.acks = 1 kafka.compression.type = snappy kafka.retries=0 Most reliable For at-least-once delivery, you will want something more like: kafka.acks = all kafka.retries = 5 # or some larger number And you will also want to set min.insync.replicas on Maxwell's output topic. Key format Maxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format: { \"database\":\"test_tb\",\"table\":\"test_tbl\",\"pk.id\":4,\"pk.part2\":\"hello\"} This key is designed to co-operate with Kafka's log compaction, which will save the last-known value for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act as a source of truth. Partitioning Both Kafka and AWS Kinesis support the notion of partitioned streams. Because they like to make our lives hard, Kafka calls its two units \"topics\" and \"partitions\", and Kinesis calls them \"streams\" and \"shards. They're the same thing, though. Maxwell is generally configured to write to N partitions/shards on one topic/stream, and how it distributes to those N partitions/shards can be controlled by producer_partition_by . producer_partition_by gives you a choice of splitting your stream by database, table, primary key, transaction id, column data, or \"random\". How you choose to partition your stream greatly influences the load and serialization properties of your downstream consumers, so choose carefully. A good rule of thumb is to use the finest-grained partition scheme possible given serialization constraints. A brief partitioning digression: If I were building, say, a simple search index of a single table, I might choose to partition by primary key; this would give you the best distribution of workload amongst your stream processors while maintaining a strict ordering of updates that happen to a certain row. If I were building something that needed better serialization properties -- let's say I needed to maintain strict ordering between updates that occured on different tables -- I would drop back to partitioning by table or database. This will reduce my throughput by a lot as a single stream consumer node will end up will all the load for particular table/database, but I'm guaranteed that the updates stay in order. If you choose to partition by column data (that is, values inside columns in your updates), you must set both: producer_partition_columns - a comma-separated list of column names, and producer_partiton_by_fallback - [ database , table , primary_key ] - this will be used as the partition key when the column does not exist. When partitioning by column Maxwell will treat the values for the specified columns as strings, concatenate them and use that value to partition the data. Kafka partitioning A binlog event's partition is determined by the selected hash function and hash string as follows HASH_FUNCTION(producer_partion_value) % TOPIC.NUMBER_OF_PARTITIONS The HASH_FUNCTION is either java's hashCode or murmurhash3 . The default HASH_FUNCTION is hashCode . Murmurhash3 may be set with the kafka_partition_hash option. The seed value for the murmurhash function is hardcoded to 25342 in the MaxwellKafkaPartitioner class. We tell you this in case you need to reverse engineer where a row will land. Maxwell will discover the number of partitions in its kafka topic upon boot. This means that you should pre-create your kafka topics: bin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\ --topic maxwell --partitions 20 --replication-factor 2 http://kafka.apache.org/documentation.html#quickstart Kinesis AWS Credentials You will need to obtain an IAM user that has the following permissions for the stream you are planning on producing to: \"kinesis:PutRecord\" \"kinesis:PutRecords\" \"kinesis:DescribeStream\" Additionally, the producer will need to be able to produce CloudWatch metrics which requires the following permission applied to the resource `*``: - \"cloudwatch:PutMetricData\" The resulting IAM policy document may look like this: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"kinesis:PutRecord\", \"kinesis:PutRecords\", \"kinesis:DescribeStream\" ], \"Resource\": \"arn:aws:kinesis:us-west-2:123456789012:stream/my-stream\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:PutMetricData\" ], \"Resource\": \"*\" } ] } See the AWS docs for the latest examples on which permissions are needed. The producer uses the DefaultAWSCredentialsProviderChain class to gain aws credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain. Options Set the output stream in config.properties by setting the kinesis_stream property. The producer uses the KPL (Kinesis Producer Library) and uses the KPL built in configurations. Copy kinesis-producer-library.properties.example to kinesis-producer-library.properties and configure the properties file to your needs. You are required to configure the region. For example: # set explicitly Region=us-west-2 # or set with an environment variable Region=$AWS_DEFAULT_REGION By default, the KPL implements record aggregation , which usually increases producer throughput by allowing you to increase the number of records sent per API call. However, aggregated records are encoded differently (using Google Protocol Buffers) than records that are not aggregated. Therefore, if you are not using the KCL (Kinesis Client Library) to consume records (for example, you are using AWS Lambda) you will need to either disaggregate the records in your consumer (for example, by using the AWS Kinesis Aggregation library ), or disable record aggregation in your kinesis-producer-library.properties configuration. To disable aggregation, add the following to your configuration: AggregationEnabled=false Remember: if you disable record aggregation, you will lose the benefit of potentially greater producer throughput. SQS AWS Credentials You will need to obtain an IAM user that has the permission to access the SQS service. The SQS producer also uses DefaultAWSCredentialsProviderChain to get AWS credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain. In case you need to set up a different region also along with credentials then default one, see the AWS docs . Options Set the output queue in the config.properties by setting the sqs_queue_uri property to full SQS queue uri from AWS console. The producer uses the AWS SQS SDK . SNS AWS Credentials You will need to obtain an IAM user that has the permission to access the SNS topic. The SNS producer also uses DefaultAWSCredentialsProviderChain to get AWS credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain. In case you need to set up a different region also along with credentials then default one, see the AWS docs . Options Set the topic arn in the config.properties by setting the sns_topic property to the topic name. FIFO topics should have a .fifo suffix. Optionally, you can enable sns_attrs to have maxwell attach various attributes to the message for subscription filtering. (Only database and table are currently supported) The producer uses the AWS SNS SDK . Nats The configurable properties for nats are: nats_url - defaults to nats://localhost:4222 nats_subject - defaults to %{database}.%{table} nats_subject defines the Nats subject hierarchy to write to. Topic substitution is available. All non-alphanumeric characters in the substitued values will be replaced by underscores. Google Cloud Pub/Sub In order to publish to Google Cloud Pub/Sub, you will need to obtain an IAM service account that has been granted the roles/pubsub.publisher role. See the Google Cloud Platform docs for the latest examples of which permissions are needed , as well as how to properly configure service accounts . Set the output stream in config.properties by setting the pubsub_project_id and pubsub_topic properties. Optionally configure a dedicated output topic for DDL updates by setting the ddl_pubsub_topic property. The producer uses the Google Cloud Java Library for Pub/Sub and uses its built-in configurations. Google Cloud BigQuery To stream data into Google Cloud Bigquery, first there must be a table created on bigquery in order to stream the data into defined as bigquery_project_id.bigquery_dataset.bigquery_table . The schema of the table must match the outputConfig. The column types should be defined as below database: string table: string type: string ts: integer xid: integer xoffset: integer commit: boolean position: string gtid: string server_id: integer primary_key: string data: string old: string See the Google Cloud Platform docs for the latest examples of which permissions are needed , as well as how to properly configure service accounts . Set the output stream in config.properties by setting the bigquery_project_id , bigquery_dataset and bigquery_table properties. The producer uses the Google Cloud Java Bigquery Storage Library for Bigquery Bigquery Storage Write API documenatation . To use the Storage Write API, you must have bigquery.tables.updateData permissions. This producer is using the Default Stream with at-least once semantics for greater data resiliency and fewer scaling restrictions RabbitMQ To produce messages to RabbitMQ, you will need to specify a host in config.properties with rabbitmq_host . This is the only required property, everything else falls back to a sane default. The remaining configurable properties are: rabbitmq_user - defaults to guest rabbitmq_pass - defaults to guest rabbitmq_virtual_host - defaults to / rabbitmq_exchange - defaults to maxwell rabbitmq_exchange_type - defaults to fanout rabbitmq_exchange_durable - defaults to false rabbitmq_exchange_autodelete - defaults to false rabbitmq_routing_key_template - defaults to %db%.%table% This config controls the routing key, where %db% and %table% are placeholders that will be substituted at runtime rabbitmq_message_persistent - defaults to false rabbitmq_declare_exchange - defaults to true For more details on these options, you are encouraged to the read official RabbitMQ documentation here: https://www.rabbitmq.com/documentation.html Redis Choose type of redis data structure to create to by setting redis_type to one of: pubsub , xadd , lpush or rpush . The default is pubsub . redis_key defaults to \"maxwell\" and supports topic substitution Other configurable properties are: redis_host - defaults to localhost redis_port - defaults to 6379 redis_auth - defaults to null redis_database - defaults to 0 redis_type - defaults to pubsub redis_key - defaults to maxwell redis_stream_json_key - defaults to message redis_sentinels - doesn't have a default value redis_sentinel_master_name - doesn't have a default value Custom Producer If none of the producers packaged with Maxwell meet your requirements, a custom producer can be added at runtime. In order to register your custom producer, you must implement the ProducerFactory interface, which is responsible for creating your custom AbstractProducer . Next, set the custom_producer.factory configuration property to your ProducerFactory 's fully qualified class name. Then add the custom ProducerFactory JAR and all its dependencies to the $MAXWELL_HOME/lib directory. Your custom producer will likely require configuration properties as well. For that, use the custom_producer.* (or CUSTOM_PRODUCER_* if using env-variable configuration) property namespace. Those properties will be available to your producer via MaxwellConfig.customProducerProperties . Custom producer factory and producer examples can be found here: https://github.com/zendesk/maxwell/tree/master/src/example/com/zendesk/maxwell/example/producerfactory Topic substitution Some producers may be given a template string from which they dynamically generate a topic (or whatever their equivalent of a kafka topic is). Subsitutions are enclosed in by %{} . The following substitutions are available: %{database} %{table} %{type} (insert/update/delete) Topic substituion is available in the following producers: Kakfa, for topics Redis, for channels Nats, for subject heirarchies","title":"Producers"},{"location":"producers/#kafka","text":"The Kafka producer is perhaps the most production hardened of all the producers, having run on high traffic instances at WEB scale.","title":"Kafka"},{"location":"producers/#topic","text":"Maxwell writes to a kafka topic named \"maxwell\" by default. It is configurable via --kafka_topic . The given topic can be a plain string or a dynamic string, e.g. namespace_%{database}_%{table} , where the topic will be generated from data in the row.","title":"Topic"},{"location":"producers/#client-version","text":"By default, maxwell uses the kafka 1.0.0 library. The --kafka_version flag lets you choose an alternate library version: 0.8.2.2, 0.9.0.1, 0.10.0.1, 0.10.2.1 or 0.11.0.1, 1.0.0. This flag is only available on the command line. The 0.8.2.2 client is only compatible with brokers running kafka 0.8. The 0.10.0.x client is only compatible with brokers 0.10.0.x or later. Mixing the 0.10 client with other versions can lead to serious performance impacts. For More details, read about it here . The 0.11.0 client can talk to version 0.10.0 or newer brokers. The 0.9.0.1 client is not compatible with brokers running kafka 0.8. The exception below will show in logs when that is the case: ERROR Sender - Uncaught error in kafka producer I/O thread: SchemaException: Error reading field 'throttle_time_ms': java.nio.BufferUnderflowException","title":"Client version"},{"location":"producers/#passing-options-to-kafka","text":"Any options present in config.properties that are prefixed with kafka. will be passed into the Kafka producer library (with kafka. stripped off, see below for examples). We use the \"new producer\" configuration, as described here: http://kafka.apache.org/documentation.html#newproducerconfigs","title":"Passing options to kafka"},{"location":"producers/#example-kafka-configs","text":"","title":"Example kafka configs"},{"location":"producers/#highest-throughput","text":"These properties would give high throughput performance. kafka.acks = 1 kafka.compression.type = snappy kafka.retries=0","title":"Highest throughput"},{"location":"producers/#most-reliable","text":"For at-least-once delivery, you will want something more like: kafka.acks = all kafka.retries = 5 # or some larger number And you will also want to set min.insync.replicas on Maxwell's output topic.","title":"Most reliable"},{"location":"producers/#key-format","text":"Maxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format: { \"database\":\"test_tb\",\"table\":\"test_tbl\",\"pk.id\":4,\"pk.part2\":\"hello\"} This key is designed to co-operate with Kafka's log compaction, which will save the last-known value for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act as a source of truth.","title":"Key format"},{"location":"producers/#partitioning","text":"Both Kafka and AWS Kinesis support the notion of partitioned streams. Because they like to make our lives hard, Kafka calls its two units \"topics\" and \"partitions\", and Kinesis calls them \"streams\" and \"shards. They're the same thing, though. Maxwell is generally configured to write to N partitions/shards on one topic/stream, and how it distributes to those N partitions/shards can be controlled by producer_partition_by . producer_partition_by gives you a choice of splitting your stream by database, table, primary key, transaction id, column data, or \"random\". How you choose to partition your stream greatly influences the load and serialization properties of your downstream consumers, so choose carefully. A good rule of thumb is to use the finest-grained partition scheme possible given serialization constraints. A brief partitioning digression: If I were building, say, a simple search index of a single table, I might choose to partition by primary key; this would give you the best distribution of workload amongst your stream processors while maintaining a strict ordering of updates that happen to a certain row. If I were building something that needed better serialization properties -- let's say I needed to maintain strict ordering between updates that occured on different tables -- I would drop back to partitioning by table or database. This will reduce my throughput by a lot as a single stream consumer node will end up will all the load for particular table/database, but I'm guaranteed that the updates stay in order. If you choose to partition by column data (that is, values inside columns in your updates), you must set both: producer_partition_columns - a comma-separated list of column names, and producer_partiton_by_fallback - [ database , table , primary_key ] - this will be used as the partition key when the column does not exist. When partitioning by column Maxwell will treat the values for the specified columns as strings, concatenate them and use that value to partition the data.","title":"Partitioning"},{"location":"producers/#kafka-partitioning","text":"A binlog event's partition is determined by the selected hash function and hash string as follows HASH_FUNCTION(producer_partion_value) % TOPIC.NUMBER_OF_PARTITIONS The HASH_FUNCTION is either java's hashCode or murmurhash3 . The default HASH_FUNCTION is hashCode . Murmurhash3 may be set with the kafka_partition_hash option. The seed value for the murmurhash function is hardcoded to 25342 in the MaxwellKafkaPartitioner class. We tell you this in case you need to reverse engineer where a row will land. Maxwell will discover the number of partitions in its kafka topic upon boot. This means that you should pre-create your kafka topics: bin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\ --topic maxwell --partitions 20 --replication-factor 2 http://kafka.apache.org/documentation.html#quickstart","title":"Kafka partitioning"},{"location":"producers/#kinesis","text":"","title":"Kinesis"},{"location":"producers/#aws-credentials","text":"You will need to obtain an IAM user that has the following permissions for the stream you are planning on producing to: \"kinesis:PutRecord\" \"kinesis:PutRecords\" \"kinesis:DescribeStream\" Additionally, the producer will need to be able to produce CloudWatch metrics which requires the following permission applied to the resource `*``: - \"cloudwatch:PutMetricData\" The resulting IAM policy document may look like this: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"kinesis:PutRecord\", \"kinesis:PutRecords\", \"kinesis:DescribeStream\" ], \"Resource\": \"arn:aws:kinesis:us-west-2:123456789012:stream/my-stream\" }, { \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:PutMetricData\" ], \"Resource\": \"*\" } ] } See the AWS docs for the latest examples on which permissions are needed. The producer uses the DefaultAWSCredentialsProviderChain class to gain aws credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain.","title":"AWS Credentials"},{"location":"producers/#options","text":"Set the output stream in config.properties by setting the kinesis_stream property. The producer uses the KPL (Kinesis Producer Library) and uses the KPL built in configurations. Copy kinesis-producer-library.properties.example to kinesis-producer-library.properties and configure the properties file to your needs. You are required to configure the region. For example: # set explicitly Region=us-west-2 # or set with an environment variable Region=$AWS_DEFAULT_REGION By default, the KPL implements record aggregation , which usually increases producer throughput by allowing you to increase the number of records sent per API call. However, aggregated records are encoded differently (using Google Protocol Buffers) than records that are not aggregated. Therefore, if you are not using the KCL (Kinesis Client Library) to consume records (for example, you are using AWS Lambda) you will need to either disaggregate the records in your consumer (for example, by using the AWS Kinesis Aggregation library ), or disable record aggregation in your kinesis-producer-library.properties configuration. To disable aggregation, add the following to your configuration: AggregationEnabled=false Remember: if you disable record aggregation, you will lose the benefit of potentially greater producer throughput.","title":"Options"},{"location":"producers/#sqs","text":"","title":"SQS"},{"location":"producers/#aws-credentials_1","text":"You will need to obtain an IAM user that has the permission to access the SQS service. The SQS producer also uses DefaultAWSCredentialsProviderChain to get AWS credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain. In case you need to set up a different region also along with credentials then default one, see the AWS docs .","title":"AWS Credentials"},{"location":"producers/#options_1","text":"Set the output queue in the config.properties by setting the sqs_queue_uri property to full SQS queue uri from AWS console. The producer uses the AWS SQS SDK .","title":"Options"},{"location":"producers/#sns","text":"","title":"SNS"},{"location":"producers/#aws-credentials_2","text":"You will need to obtain an IAM user that has the permission to access the SNS topic. The SNS producer also uses DefaultAWSCredentialsProviderChain to get AWS credentials. See the AWS docs on how to setup the IAM user with the Default Credential Provider Chain. In case you need to set up a different region also along with credentials then default one, see the AWS docs .","title":"AWS Credentials"},{"location":"producers/#options_2","text":"Set the topic arn in the config.properties by setting the sns_topic property to the topic name. FIFO topics should have a .fifo suffix. Optionally, you can enable sns_attrs to have maxwell attach various attributes to the message for subscription filtering. (Only database and table are currently supported) The producer uses the AWS SNS SDK .","title":"Options"},{"location":"producers/#nats","text":"The configurable properties for nats are: nats_url - defaults to nats://localhost:4222 nats_subject - defaults to %{database}.%{table} nats_subject defines the Nats subject hierarchy to write to. Topic substitution is available. All non-alphanumeric characters in the substitued values will be replaced by underscores.","title":"Nats"},{"location":"producers/#google-cloud-pubsub","text":"In order to publish to Google Cloud Pub/Sub, you will need to obtain an IAM service account that has been granted the roles/pubsub.publisher role. See the Google Cloud Platform docs for the latest examples of which permissions are needed , as well as how to properly configure service accounts . Set the output stream in config.properties by setting the pubsub_project_id and pubsub_topic properties. Optionally configure a dedicated output topic for DDL updates by setting the ddl_pubsub_topic property. The producer uses the Google Cloud Java Library for Pub/Sub and uses its built-in configurations.","title":"Google Cloud Pub/Sub"},{"location":"producers/#google-cloud-bigquery","text":"To stream data into Google Cloud Bigquery, first there must be a table created on bigquery in order to stream the data into defined as bigquery_project_id.bigquery_dataset.bigquery_table . The schema of the table must match the outputConfig. The column types should be defined as below database: string table: string type: string ts: integer xid: integer xoffset: integer commit: boolean position: string gtid: string server_id: integer primary_key: string data: string old: string See the Google Cloud Platform docs for the latest examples of which permissions are needed , as well as how to properly configure service accounts . Set the output stream in config.properties by setting the bigquery_project_id , bigquery_dataset and bigquery_table properties. The producer uses the Google Cloud Java Bigquery Storage Library for Bigquery Bigquery Storage Write API documenatation . To use the Storage Write API, you must have bigquery.tables.updateData permissions. This producer is using the Default Stream with at-least once semantics for greater data resiliency and fewer scaling restrictions","title":"Google Cloud BigQuery"},{"location":"producers/#rabbitmq","text":"To produce messages to RabbitMQ, you will need to specify a host in config.properties with rabbitmq_host . This is the only required property, everything else falls back to a sane default. The remaining configurable properties are: rabbitmq_user - defaults to guest rabbitmq_pass - defaults to guest rabbitmq_virtual_host - defaults to / rabbitmq_exchange - defaults to maxwell rabbitmq_exchange_type - defaults to fanout rabbitmq_exchange_durable - defaults to false rabbitmq_exchange_autodelete - defaults to false rabbitmq_routing_key_template - defaults to %db%.%table% This config controls the routing key, where %db% and %table% are placeholders that will be substituted at runtime rabbitmq_message_persistent - defaults to false rabbitmq_declare_exchange - defaults to true For more details on these options, you are encouraged to the read official RabbitMQ documentation here: https://www.rabbitmq.com/documentation.html","title":"RabbitMQ"},{"location":"producers/#redis","text":"Choose type of redis data structure to create to by setting redis_type to one of: pubsub , xadd , lpush or rpush . The default is pubsub . redis_key defaults to \"maxwell\" and supports topic substitution Other configurable properties are: redis_host - defaults to localhost redis_port - defaults to 6379 redis_auth - defaults to null redis_database - defaults to 0 redis_type - defaults to pubsub redis_key - defaults to maxwell redis_stream_json_key - defaults to message redis_sentinels - doesn't have a default value redis_sentinel_master_name - doesn't have a default value","title":"Redis"},{"location":"producers/#custom-producer","text":"If none of the producers packaged with Maxwell meet your requirements, a custom producer can be added at runtime. In order to register your custom producer, you must implement the ProducerFactory interface, which is responsible for creating your custom AbstractProducer . Next, set the custom_producer.factory configuration property to your ProducerFactory 's fully qualified class name. Then add the custom ProducerFactory JAR and all its dependencies to the $MAXWELL_HOME/lib directory. Your custom producer will likely require configuration properties as well. For that, use the custom_producer.* (or CUSTOM_PRODUCER_* if using env-variable configuration) property namespace. Those properties will be available to your producer via MaxwellConfig.customProducerProperties . Custom producer factory and producer examples can be found here: https://github.com/zendesk/maxwell/tree/master/src/example/com/zendesk/maxwell/example/producerfactory","title":"Custom Producer"},{"location":"producers/#topic-substitution","text":"Some producers may be given a template string from which they dynamically generate a topic (or whatever their equivalent of a kafka topic is). Subsitutions are enclosed in by %{} . The following substitutions are available: %{database} %{table} %{type} (insert/update/delete) Topic substituion is available in the following producers: Kakfa, for topics Redis, for channels Nats, for subject heirarchies","title":"Topic substitution"},{"location":"quickstart/","text":"Download Download binary distro: https://github.com/zendesk/maxwell/releases/download/v1.38.0/maxwell-1.38.0.tar.gz Sources and bug tracking is available on github: https://github.com/zendesk/maxwell curl : curl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.38.0/maxwell-1.38.0.tar.gz \\ | tar zxvf - cd maxwell-1.38.0 docker : docker pull zendesk/maxwell homebrew : brew install maxwell Configure Mysql # /etc/my.cnf [mysqld] # maxwell needs binlog_format=row binlog_format=row server_id=1 log-bin=master mysql> CREATE USER 'maxwell'@'%' IDENTIFIED BY 'XXXXXX'; mysql> CREATE USER 'maxwell'@'localhost' IDENTIFIED BY 'XXXXXX'; mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'%'; mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'localhost'; mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'%'; mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'localhost'; Run Maxwell Command line bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout Docker docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\ --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=stdout Kafka Boot kafka as described here: http://kafka.apache.org/documentation.html#quickstart , then: bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=kafka --kafka.bootstrap.servers=localhost:9092 --kafka_topic=maxwell (or docker): docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\ --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kafka \\ --kafka.bootstrap.servers=$KAFKA_HOST:$KAFKA_PORT --kafka_topic=maxwell Kinesis docker run -it --rm --name maxwell -v `cd && pwd`/.aws:/root/.aws zendesk/maxwell sh -c 'cp /app/kinesis-producer-library.properties.example /app/kinesis-producer-library.properties && echo \"Region=$AWS_DEFAULT_REGION\" >> /app/kinesis-producer-library.properties && bin/maxwell --user=$MYSQL_USERNAME --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kinesis --kinesis_stream=$KINESIS_STREAM' Nats bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=nats --nats_url=='0.0.0.0:4222' Google Cloud Pub/Sub bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=pubsub --pubsub_project_id='$PUBSUB_PROJECT_ID' \\ --pubsub_topic='maxwell' Google Cloud Bigquery bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=bigquery --bigquery_project_id='$BIGQUERY_PROJECT_ID' \\ --bigquery_dataset='$BIGQUERY_DATASET' \\ --bigquery_table='$BIGQUERY_TABLE' RabbitMQ bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=rabbitmq --rabbitmq_host='rabbitmq.hostname' Redis bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=redis --redis_host=redis.hostname SNS bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=sns --sns_topic=sns.topic --sns_attrs=database,table","title":"Quick Start"},{"location":"quickstart/#download","text":"Download binary distro: https://github.com/zendesk/maxwell/releases/download/v1.38.0/maxwell-1.38.0.tar.gz Sources and bug tracking is available on github: https://github.com/zendesk/maxwell curl : curl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.38.0/maxwell-1.38.0.tar.gz \\ | tar zxvf - cd maxwell-1.38.0 docker : docker pull zendesk/maxwell homebrew : brew install maxwell","title":"Download"},{"location":"quickstart/#configure-mysql","text":"# /etc/my.cnf [mysqld] # maxwell needs binlog_format=row binlog_format=row server_id=1 log-bin=master mysql> CREATE USER 'maxwell'@'%' IDENTIFIED BY 'XXXXXX'; mysql> CREATE USER 'maxwell'@'localhost' IDENTIFIED BY 'XXXXXX'; mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'%'; mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'localhost'; mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'%'; mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'localhost';","title":"Configure Mysql"},{"location":"quickstart/#run-maxwell","text":"","title":"Run Maxwell"},{"location":"quickstart/#command-line","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout","title":"Command line"},{"location":"quickstart/#docker","text":"docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\ --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=stdout","title":"Docker"},{"location":"quickstart/#kafka","text":"Boot kafka as described here: http://kafka.apache.org/documentation.html#quickstart , then: bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=kafka --kafka.bootstrap.servers=localhost:9092 --kafka_topic=maxwell (or docker): docker run -it --rm zendesk/maxwell bin/maxwell --user=$MYSQL_USERNAME \\ --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kafka \\ --kafka.bootstrap.servers=$KAFKA_HOST:$KAFKA_PORT --kafka_topic=maxwell","title":"Kafka"},{"location":"quickstart/#kinesis","text":"docker run -it --rm --name maxwell -v `cd && pwd`/.aws:/root/.aws zendesk/maxwell sh -c 'cp /app/kinesis-producer-library.properties.example /app/kinesis-producer-library.properties && echo \"Region=$AWS_DEFAULT_REGION\" >> /app/kinesis-producer-library.properties && bin/maxwell --user=$MYSQL_USERNAME --password=$MYSQL_PASSWORD --host=$MYSQL_HOST --producer=kinesis --kinesis_stream=$KINESIS_STREAM'","title":"Kinesis"},{"location":"quickstart/#nats","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=nats --nats_url=='0.0.0.0:4222'","title":"Nats"},{"location":"quickstart/#google-cloud-pubsub","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=pubsub --pubsub_project_id='$PUBSUB_PROJECT_ID' \\ --pubsub_topic='maxwell'","title":"Google Cloud Pub/Sub"},{"location":"quickstart/#google-cloud-bigquery","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=bigquery --bigquery_project_id='$BIGQUERY_PROJECT_ID' \\ --bigquery_dataset='$BIGQUERY_DATASET' \\ --bigquery_table='$BIGQUERY_TABLE'","title":"Google Cloud Bigquery"},{"location":"quickstart/#rabbitmq","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=rabbitmq --rabbitmq_host='rabbitmq.hostname'","title":"RabbitMQ"},{"location":"quickstart/#redis","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=redis --redis_host=redis.hostname","title":"Redis"},{"location":"quickstart/#sns","text":"bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\ --producer=sns --sns_topic=sns.topic --sns_attrs=database,table","title":"SNS"},{"location":"schemas/","text":"Internals Schema storage MySQL binlogs give maxwell access to the raw bytes for each update that happens. In order to generate useful output, Maxwell needs to know the data type of every column, so that it can interpret the bytes as a number, string, boolean, etc. There is a set of \"base schema\" tables in the maxwell database - tables , columns , databases . This is where we capture the initial schema, the first time Maxwell is run. As time progresses, maxwell will see any modifications you make to the schema (these are part of the binlog). As each change occurs, Maxwell will generate an internal representation of what changed. Maxwell stores these diffs in the schemas table - each diff contains the following information to place it in the timeline of your database: binlog_file , binlog_position (or gtid_set ): the exact point in the binlog stream where the schema change occurred deltas : the internal representation of the schema change base_schema_id : the previous schema that this delta applies to last_heartbeat_read : the most recent maxwell heartbeat seen in the binlog prior to this change server_id : the server which this schema applies to This information creates a concrete timeline of the history of your schema for a given server. Given any binlog file+position (or gtid) and a last_heartbeat value, the current schema can be found by finding the \"most recent\" schema for this server_id, then following the chain of base_schema_id until it terminates (in a null , which means we've reached the initial captured schema). \"Most recent\" should be fairly intuitive - firstly we sort by last_heartbeat_read , then by binlog_file , then binlog_position . We limit the search to values \"before\" the binlog position we're searching for, because we don't want to use a schema corresponding to a change that is \"in the future\" - i.e. further ahead in the binlog than our current position. Master failover Schemas are important for master failover. When Maxwell detects that it is talking to a new server_id (one that differs from its stored position ), it attempts a master failover (if enabled). It searches backwards in the new servers binlog files, looking for an update to maxwell.heartbeats corresponding to the timestamp stored in its position table. Once it finds this (unique) update, it knows the binlog location for both the old and new master which correspond to the exact same event. Using this information, maxwell creates a merge point - it finds the active schema for the old master's stored position, and then creates a new schema entry with an empty delta, the new server_id , and a base_schema_id of the previous schema. In this way, Maxwell is able to create a chain of schema updates even across different servers with different sets of binlogs.","title":"Internals"},{"location":"schemas/#internals","text":"","title":"Internals"},{"location":"schemas/#schema-storage","text":"MySQL binlogs give maxwell access to the raw bytes for each update that happens. In order to generate useful output, Maxwell needs to know the data type of every column, so that it can interpret the bytes as a number, string, boolean, etc. There is a set of \"base schema\" tables in the maxwell database - tables , columns , databases . This is where we capture the initial schema, the first time Maxwell is run. As time progresses, maxwell will see any modifications you make to the schema (these are part of the binlog). As each change occurs, Maxwell will generate an internal representation of what changed. Maxwell stores these diffs in the schemas table - each diff contains the following information to place it in the timeline of your database: binlog_file , binlog_position (or gtid_set ): the exact point in the binlog stream where the schema change occurred deltas : the internal representation of the schema change base_schema_id : the previous schema that this delta applies to last_heartbeat_read : the most recent maxwell heartbeat seen in the binlog prior to this change server_id : the server which this schema applies to This information creates a concrete timeline of the history of your schema for a given server. Given any binlog file+position (or gtid) and a last_heartbeat value, the current schema can be found by finding the \"most recent\" schema for this server_id, then following the chain of base_schema_id until it terminates (in a null , which means we've reached the initial captured schema). \"Most recent\" should be fairly intuitive - firstly we sort by last_heartbeat_read , then by binlog_file , then binlog_position . We limit the search to values \"before\" the binlog position we're searching for, because we don't want to use a schema corresponding to a change that is \"in the future\" - i.e. further ahead in the binlog than our current position.","title":"Schema storage"},{"location":"schemas/#master-failover","text":"Schemas are important for master failover. When Maxwell detects that it is talking to a new server_id (one that differs from its stored position ), it attempts a master failover (if enabled). It searches backwards in the new servers binlog files, looking for an update to maxwell.heartbeats corresponding to the timestamp stored in its position table. Once it finds this (unique) update, it knows the binlog location for both the old and new master which correspond to the exact same event. Using this information, maxwell creates a merge point - it finds the active schema for the old master's stored position, and then creates a new schema entry with an empty delta, the new server_id , and a base_schema_id of the previous schema. In this way, Maxwell is able to create a chain of schema updates even across different servers with different sets of binlogs.","title":"Master failover"}]}