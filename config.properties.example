# tl;dr config
log_level=info

producer=kafka
kafka.bootstrap.servers=localhost:9092

# mysql login info
host=localhost
user=maxwell
password=maxwell


######### general stuff #############

# choose where to produce data to. currently this is one of: stdout|file|kafka|kinesis|pubsub|rabbitmq|redis|graylog
#producer=kafka

# set delivery timeout (including acknowledgement) in milliseconds for a COMMIT row message that has been sent by the producer,
# where producer is either kafka or kinesis
# maxwell may be terminated if the timeout has expired, when majority of the COMMIT row messages have been acknowledged
# set it to 0 to turn off this check
#producer_ack_timeout=120000 # default 0

# set the log level.  note that you can configure things further in log4j2.xml
#log_level=DEBUG # [DEBUG, INFO, WARN, ERROR]


######### mysql stuff ###############

# mysql host to connect to
#host=hostname

# mysql port to connect to
#port=3306

# mysql user to connect as.  This user must have REPLICATION SLAVE permissions,
# as well as full access to the `maxwell` (or schema_database) database
#user=maxwell

# mysql password
#password=maxwell

# options to pass into the jdbc connection, given as opt=val&opt2=val2
#jdbc_options=opt1=100&opt2=hello

# name of the mysql database where maxwell keeps its own state
#schema_database=maxwell


# maxwell can optionally replicate from a different server than where it stores
# schema and binlog position info.  Specify that different server here:
#
#
#replication_host=other
#replication_user=username
#replication_password=password
#replication_port=3306

######### output format stuff ###############

# records include binlog position (default false)
#output_binlog_position=true

# records output null values (default true)
#output_nulls=true

# records include server_id (default false)
#output_server_id=true

# records include thread_id (default false)
#output_thread_id=true

# records include commit and xid (default true)
#output_commit_info=true

# produce DDL records to ddl_kafka_topic (default: false)
#output_ddl=true

######### kafka stuff ###############

# list of kafka brokers
#kafka.bootstrap.servers=hosta:9092,hostb:9092

# kafka topic to write to
# this can be static, e.g. 'maxwell', or dynamic, e.g. namespace_%{database}_%{table}
# in the latter case 'database' and 'table' will be replaced with the values for the row being processed
#kafka_topic=maxwell

# kafka topic to write DDL to
#ddl_kafka_topic=maxwell_ddl

# hash function to use.  "default" is just the JVM's 'hashCode'
# function.
#kafka_partition_hash=default # [default, murmur3]

# how maxwell writes its kafka key.
#
# 'hash' looks like:
# {"database":"test","table":"tickets","pk.id":10001}
#
# 'array' looks like:
# ["test","tickets",[{"id":10001}]]
#
# currently the default is "hash"
#kafka_key_format=hash # [hash, array]

# other kafka options.  Anything prefixed "kafka." will get
# passed directly into the kafka-producer's config.
#kafka.batch.size=16384

# a few defaults.
# These are 0.11-specific. They may or may not work with other versions.
kafka.compression.type=snappy
kafka.retries=0
kafka.acks=1

####### producer partitioning #######
# used by kafka and kinesis

# controls the input input into the hash function.  Note that this defines
# which transactions keep ordering with respect to each other.  Generally
# here you're making a trade-off between inter-row consistency and balanced
# partitions.
#producer_partition_by=database # [database, table, primary_key, column]

# required when using producer_partition_by=column
# otherwise the partitioner will revert to table
# can be a single or multiple columns, e.g. aggregate_id or aggregate_id,event_type
#producer_partition_columns=

# required when using producer_partition_by=column
# the fallback partitioning behavior when the specified column(s) do not exist
# can be any one of [database, table, primary_key]
#producer_partition_by_fallback=database

######### kinesis ####################
kinesis_stream=maxwell

# AWS places a 256 unicode character limit on the max key length of a record
# http://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecord.html
#
# Setting this option to true enables hashing the key with the md5 algorithm
# before we send it to kinesis so all the keys work within the key size limit.
# Values: true, false
# Default: false
#kinesis_md5_keys=true

######### pub/sub #####################
#pubsub_project_id=maxwell
#pubsub_topic=maxwell
#ddl_pubsub_topic=maxwell_ddl

######### rabbitmq ###################
#rabbitmq_host=rabbitmq_hostname
#rabbitmq_user=guest
#rabbitmq_pass=guest
#rabbitmq_virtual_host=/
#rabbitmq_exchange=maxwell
#rabbitmq_exchange_type=fanout
#rabbitmq_exchange_durable=false
#rabbitmq_exchange_autodelete=false
#rabbitmq_routing_key_template=%db%.%table%
#rabbitmq_message_persistent=false

######### redis ######################
#redis_host=redis_host
#redis_port=6379
#redis_auth=redis_auth
#redis_database=0
#redis_pub_channel=maxwell

######### graylog ######################
#graylog_host=graylog_host
#graylog_port=12201
#graylog_transport=udp
#graylog_additional_field.field_foo=foo
#graylog_additional_field.field_bar=bar

######### filter stuff ###############

# filter rows out of Maxwell's output.
# all filters may be given either as literal names, or as java-style regular expressions.
# This is a literal name: "exclude_tables=tblname".
# This is a regexp:       "exclude_tables=/tblname_\\d+/"

# include *only* these databases
#include_dbs=db1,/db\\d+/

# exclude these databases (will override an include)
#exclude_dbs=db3,/db\\d+/

# include *only* these tables
#include_tables=tbl1,/tbl\\d+/

# exclude these tables
#exclude_tables=tbl1,/tbl\\d+/

# exclude these columns
#exclude_columns=col1,/col\\d+/

# "blacklist" these dbs -- this means maxwell will ignore schema
# changes happening to these databases.  Can be useful if you have a
# high-churn schema that you want to completely ignore, but it's
# a bit dangerous: once you set this option, you must leave it set, or
# else maxwell will likely blow up.
#
# All of this is to say: don't set this unless you know what you're doing.
#blacklist_dbs=db1,/db\\d+/
#blacklist_tables=table1,table_no
#

######### encryption ###############

# Encryption mode. Possible values are none, data, and all. (default none)
#encrypt=none

# Specify the secret key to be used
#secret_key=RandomInitVector

######## monitoring stuff ###########

# Maxwell collects metrics via dropwizard. These can be exposed through the
# base logging mechanism (slf4j), JMX, HTTP or pushed to Datadog.
# Options: [jmx, slf4j, http, datadog]
# Supplying multiple is allowed.
#metrics_type=jmx,slf4j

# The prefix maxwell will apply to all metrics
#metrics_prefix=MaxwellMetrics # default MaxwellMetrics

# When metrics_type includes slf4j this is the frequency metrics are emitted to the log, in seconds
#metrics_slf4j_interval=60

# When metrics_type includes http or diagnostic is enabled, this is the port the server will bind to.
#http_port=8080

# When metrics_type includes http or diagnostic is enabled, this is the http path prefix, default /.
#http_path_prefix=/some/path/

# ** The following are Datadog specific. **
# When metrics_type includes datadog this is the way metrics will be reported.
# Options: [udp, http]
# Supplying multiple is not allowed.
#metrics_datadog_type=udp

# datadog tags that should be supplied
#metrics_datadog_tags=tag1:value1,tag2:value2

# The frequency metrics are pushed to datadog, in seconds
#metrics_datadog_interval=60

# required if metrics_datadog_type = http
#metrics_datadog_apikey=API_KEY

# required if metrics_datadog_type = udp
#metrics_datadog_host=localhost # default localhost
#metrics_datadog_port=8125 # default 8125

# Maxwell exposes http diagnostic endpoint to check below in parallel:
# 1. binlog replication lag
# 2. producer (currently kafka) lag

# To enable Maxwell diagnostic
#http_diagnostic=true # default false

# Diagnostic check timeout in milliseconds, required if diagnostic = true
#http_diagnostic_timeout=10000 # default 10000

######### misc stuff ###############

# maxwell's bootstrapping functionality has a couple of modes.
#
# In "async" mode, maxwell will output the replication stream while it
# simultaneously outputs the database to the topic.  Note that it won't
# output replication data for any tables it is currently bootstrapping -- this
# data will be buffered and output after the bootstrap is complete.
#
# In "sync" mode, maxwell stops the replication stream while it
# outputs bootstrap data.
#
# async mode keeps ops live while bootstrapping, but carries the possibility of
# data loss (due to buffering transactions).  sync mode is safer but you
# have to stop replication.
#bootstrapper=async [sync, async, none]

# output filename when using the "file" producer
#output_file=/path/to/file
