{
    "docs": [
        {
            "location": "/", 
            "text": "Maxwell = Mysql + Kafka\n\n\n\nThis is Maxwell's daemon, an application that reads MySQL binlogs and writes row updates to Kafka as JSON.\nIt's playing in the same space as \nmypipe\n and \ndatabus\n,\nbut differentiates itself with these features:\n\n\n\n\nWorks with an unpatched mysql\n\n\nParses ALTER/CREATE/DROP table statements, which allows Maxwell to always have a correct view of the mysql schema\n\n\nStores its replication position and needed data within the mysql server itself\n\n\nRequires no external dependencies (save Kafka, if used)\n\n\nEschews the complexity of Avro for plain old JSON.\n\n\nMinimal setup\n\n\n\n\nMaxwell is intended as a source for event-based readers, eg various ETL applications, search indexing,\nstat emitters.\n\n\n\nmysql\n insert into `test`.`maxwell` set id = 1, daemon = 'Stanlislaw Lem';\n\n\n- {\n     \ndatabase\n:\ntest\n,\n     \ntable\n:\nmaxwell\n,\n     \ntype\n:\ninsert\n,\n     \nts\n:1449786310,\n     \nxid\n:940753,\n     \ncommit\n:true,\n     \ndata\n:{ \nid\n:1, \ndaemon\n: \nStanlislaw Lem\n }\n   }\n\nmysql\n update test.maxwell set id = 11, daemon = 'firebus!  firebus!';\n\n\n- {\n     \ndatabase\n:\ntest\n,\n     \ntable\n:\nmaxwell\n,\n     \ntype\n:\nupdate\n,\n     \nts\n:1449786341,\n     \nxid\n:940786,\n     \ncommit\n:true,\n     \ndata\n:{\nid\n:11, \ndaemon\n:\nFirebus!  Firebus!\n},\n     \nold\n:{\nid\n:1, \ndaemon\n:\nStanlislaw Lem\n}\n   }\n\n\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"#maxwell-header\").append(\n      jQuery(\"<img alt='The Daemon, maybe' src='./img/cyberiad_1.jpg' style='float: left; height: 300px; padding-right: 30px;'>\")\n\n    )\n  });", 
            "title": "Overview"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Row based replication\n\n\nMaxwell can only operate if row-based replication is on.\n\n\n$ vi my.cnf\n\n[mysqld]\nserver-id=1\nlog-bin=master\nbinlog_format=row\n\n\n\n\nGrant permissions\n\n\nmysql\n GRANT ALL on maxwell.* to 'maxwell'@'%' identified by 'XXXXXX';\nmysql\n GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'%';\n\n# or for running maxwell locally:\n\nmysql\n GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'localhost' identified by 'XXXXXX';\nmysql\n GRANT ALL on maxwell.* to 'maxwell'@'localhost';\n\n\n\n\n\nInstall maxwell\n\n\nYou'll need a version 7 of a JVM.\n\n\ncurl -sLo - https://github.com/zendesk/maxwell/releases/download/v0.16.1-RC1/maxwell-0.16.1-RC1.tar.gz \\\n       | tar zxvf -\ncd maxwell-0.16.1-RC1\n\n\n\n\nRun with stdout producer\n\n\nUseful for smoke-testing the thing.\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout\n\n\n\n\nIf all goes well you'll see maxwell replaying your inserts:\n\n\nmysql\n insert into test.maxwell set id = 5, daemon = 'firebus!  firebus!';\nQuery OK, 1 row affected (0.04 sec)\n\n(maxwell)\n{\ntable\n:\nmaxwell\n,\ntype\n:\ninsert\n,\ndata\n:{\nid\n:5,\ndaemon\n:\nfirebus!  firebus!\n},\nts\n: 123456789}\n\n\n\n\nRun with kafka producer\n\n\nBoot kafka as described here:  \nhttp://kafka.apache.org/07/quickstart.html\n, then:\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n   --producer=kafka --kafka.bootstrap.servers=localhost:9092\n\n\n\n\nThis will start writing to the topic \"maxwell\".", 
            "title": "Quick Start"
        }, 
        {
            "location": "/config/", 
            "text": "Maxwell configuration\n\n\n\n\n\nMaxwell can be configured by command line or a java \"properties\" file.\n\n\nCommand line options\n\n\n\n\n\n\n\n\noption\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\n--config FILE\n\n\nlocation of \nconfig.properties\n file\n\n\n\n\n\n\n--log_level DEBUG\n\n\nINFO\n\n\n\n\n\n\n--user USER\n\n\nmysql username\n\n\n\n\n\n\n--password PASSWORD\n\n\nmysql password\n\n\n\n\n\n\n--host HOST\n\n\nmysql host\n\n\n\n\n\n\n--port PORT\n\n\nmysql port\n\n\n\n\n\n\n--producer stdout,kafka,file,profiler\n\n\nwhat type of producer to use.  default: stdout\n\n\n\n\n\n\n--output_file\n\n\npath of the file for the 'file' producer to write to\n\n\n\n\n\n\n--include_dbs PATTERN\n\n\nonly send updates from these databases\n\n\n\n\n\n\n--exclude_dbs PATTERN\n\n\nignore updates from these databases\n\n\n\n\n\n\n--include_tables PATTERN\n\n\nonly send updates from tables named like PATTERN\n\n\n\n\n\n\n--exclude_tables PATTERN\n\n\nignore updates from tables named like PATTERN\n\n\n\n\n\n\n--kafka.bootstrap.servers\n\n\nlist of kafka brokers, listed as HOST:PORT[,HOST:PORT].\n\n\n\n\n\n\n--kafka_topic\n\n\nprovide a topic for maxwell to write to. Default will be \"maxwell\".\n\n\n\n\n\n\n--max_schemas\n\n\nhow many old schemas maxwell should leave lying around in maxwell.schemas.  default: 5\n\n\n\n\n\n\n--init_position FILE:POSITION\n\n\nignore the information in maxwell.positions and start at the given binlog position.  Not available in config.properties.\n\n\n\n\n\n\n--replay\n\n\nenabled maxwell's read-only \"replay\" mode.  Not available in config.properties.\n\n\n\n\n\n\n\n\nConfiguration file\n\n\nIf maxwell finds the file \nconfig.properties\n in $PWD it will use it.  Any\ncommand line options (except init_position and replay) may be given as\n\"key=value\" pairs.\n\n\nAdditionally, any configuration file options prefixed with 'kafka.' will be\npassed into the kafka producer library, after having 'kafka.' stripped off the\nfront of the key.  So, for example if config.properties contains\n\n\nkafka.batch.size=16384\n\n\n\n\nMaxwell will send \nbatch.size=16384\n to the kafka producer library.\n\n\nrow filters\n\n\nThe options \ninclude_dbs\n, \nexclude_dbs\n, \ninclude_tables\n, and \nexclude_tables\n control whether\nMaxwell will send an update for a given row to its producer.  All the options take a single value PATTERN,\nwhich may either be a literal table/database name, given as \noption=name\n, or a regular expression,\ngiven as \noption=/regex/\n.  The options are evaluated as follows:\n\n\n\n\nonly accept databases in \ninclude_dbs\n if non-empty\n\n\nreject databases in \nexclude_dbs\n\n\nonly accept tables in \ninclude_tables\n if non-empty\n\n\nreject tables in \nexclude_tables\n\n\n\n\nSo an example like \n--include_dbs=/foo.*/ --exclude_tables=bar\n will include \nfooty.zab\n and exclude \nfooty.bar\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Configuration"
        }, 
        {
            "location": "/kafka/", 
            "text": "Kafka options\n\n\nAny options given to Maxwell that are prefixed with \nkafka.\n will be passed directly into the Kafka producer configuration\n(with \nkafka.\n stripped off).  We use the \"new producer\" configuration, as described here:\n\nhttp://kafka.apache.org/documentation.html#newproducerconfigs\n\n\nMaxwell sets the following Kafka options by default, but you can override them in \nconfig.properties\n.\n\n\n\n\nkafka.acks = 1\n\n\nkafka.compression.type = gzip\n\n\n\n\nMaxwell writes to a kafka topic named \"maxwell\" by default.  This can be changed with the \nkafka_topic\n option.\n\n\nKafka key\n\n\nMaxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format:\n\n\n{ \ndatabase\n:\ntest_tb\n,\ntable\n:\ntest_tbl\n,\npk.id\n:4,\npk.part2\n:\nhello\n}\n\n\n\n\nThis key is designed to co-operate with Kafka's log compaction, which will save the last-known\nvalue for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act\nas a source of truth.\n\n\nTopic and partitioning\n\n\nMaxwell enforces ordering on events within a logical mysql database (but not within a mysql server).  We enforce\nthis ordering by choosing a kafka partition based on an event's database name (\ndbName.hashCode() % numPartitions\n).\nThis means that you should create a kafka topic for Maxwell with at least as many partitions as you have logical databases:\n\n\nbin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\\n                    --topic maxwell --partitions 20 --replication-factor 2\n\n\n\n\nhttp://kafka.apache.org/documentation.html#quickstart\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Kafka"
        }, 
        {
            "location": "/dataformat/", 
            "text": "How Maxwell munges various datatypes\n\n\n\n\n\nstrings (varchar, text)\n\n\nMaxwell currently supports latin1 and utf-8 columns, and will convert both to UTF-8 before outputting as JSON.\n\n\n\n\nblob (+ binary encoded strings)\n\n\nMaxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding).\n\n\n\n\ndatetime\n\n\nDatetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings.  Note that mysql\nhas no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and\nMaxwell chooses to reproduce these invalid datetimes faithfully,\nfor lack of something better to do.\n\n\nmysql\n    create table test_datetime ( id int(11), dtcol datetime );\nmysql\n    insert into test_datetime set dtcol='0000-00-00 00:00:00';\n\n\nmaxwell  {\ntable\n:\ntest_datetime\n,\ntype\n:\ninsert\n,\ndata\n:{\ndtcol\n:\n0000-00-00 00:00:00\n}}\n\n\n\n\n\n\nsets\n\n\noutput as JSON arrays.\n\n\nmysql\n   create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') );\nmysql\n   insert into test_sets set setcol = 'b_val,c_val';\n\n\nmaxwell {\ntable\n:\ntest_sets\n,\ntype\n:\ninsert\n,\ndata\n:{\nsetcol\n:[\nb_val\n,\nc_val\n]}}", 
            "title": "Data Format"
        }, 
        {
            "location": "/compat/", 
            "text": "Maxwell Compatibility\n\n\n\nRequirements:\n\n\n\n\nJRE 7 or above\n\n\nmysql 5.1, 5.5, 5.6\n\n\nkafka 0.8.2 or greater\n\n\n\n\nUnsupported\n\n\n\n\nMysql 5.7 is untested with Maxwell, and in particular GTID replication is unsupported as of yet.\n\n\nbinlog_row_image=MINIMAL\n is not supported and will break Maxwell in a variety of amusing ways.\n\n\n\n\nMaster recovery\n\n\nCurrently Maxwell is not very smart about master recovery or detecting a promoted slave; if it determines\nthat the server_id has changed between runs, Maxwell will simply delete its old schema cache and binlog position\nand start again.  We plan on improving master recovery in future releases.\n\n\nIf you know the starting position of your new master, you can start the new Maxwell process with the\n\n--init_position\n flag, which will ensure that no gap appears in a master failover.", 
            "title": "Compat / Caveats"
        }
    ]
}