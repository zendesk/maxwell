{
    "docs": [
        {
            "location": "/", 
            "text": "This is Maxwell's daemon, an application that reads MySQL binlogs and writes\nrow updates to Kafka as JSON.  Maxwell has a low operational bar and produces a\nconsistent, easy to ingest stream of updates.  It allows you to easily \"bolt\non\" some of the benefits of stream processing systems without going through your\nentire code base to add (unreliable) instrumentation points.  Common use cases\ninclude ETL, cache building/expiring, metrics collection, and search indexing.\n\n\nadvanced features:\n\n\n\n\nCan do \nSELECT * from table\n (bootstrapping) initial loads of a table\n\n\nsupports automatic position recover on master promotion\n\n\nflexible partitioning schemes for Kakfa - by database, table, primary key, or column\n\n\nMaxwell pulls all this off by acting as a full mysql replica, including a SQL\n  parser for create/alter/drop statements (nope, there was no other way).\n\n\n\n\n Download:\n\nhttps://github.com/zendesk/maxwell/releases/download/v1.5.1/maxwell-1.5.1.tar.gz\n\n\n\n\n Source:\n\nhttps://github.com/zendesk/maxwell\n\n\n\n\n  mysql\n insert into `test`.`maxwell` set id = 1, daemon = 'Stanislaw Lem';\n  maxwell: {\n    \ndatabase\n: \ntest\n,\n    \ntable\n: \nmaxwell\n,\n    \ntype\n: \ninsert\n,\n    \nts\n: 1449786310,\n    \nxid\n: 940752,\n    \ncommit\n: true,\n    \ndata\n: { \nid\n:1, \ndaemon\n: \nStanislaw Lem\n }\n  }\n\n\n\n\n  mysql\n update test.maxwell set daemon = 'firebus!  firebus!' where id = 1;\n  maxwell: {\n    \ndatabase\n: \ntest\n,\n    \ntable\n: \nmaxwell\n,\n    \ntype\n: \nupdate\n,\n    \nts\n: 1449786341,\n    \nxid\n: 940786,\n    \ncommit\n: true,\n    \ndata\n: {\nid\n:1, \ndaemon\n: \nFirebus!  Firebus!\n},\n    \nold\n:  {\ndaemon\n: \nStanislaw Lem\n}\n  }\n\n\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"#maxwell-header\").append(\n      jQuery(\"<img alt='The Daemon, maybe' src='./img/cyberiad_1.jpg' id='maxwell-daemon-image'>\")\n    );\n    jQuery(\"pre\").addClass(\"home-code\");\n  });", 
            "title": "Overview"
        }, 
        {
            "location": "/quickstart/", 
            "text": "Download\n\n\n\n\n\n\nDownload binary distro: \nhttps://github.com/zendesk/maxwell/releases/download/v1.5.1/maxwell-1.5.1.tar.gz\n\n\nSources and bug tracking is available on github: \nhttps://github.com/zendesk/maxwell\n\n\nObligatory copy/paste to terminal:\n\n\n\n\ncurl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.5.1/maxwell-1.5.1.tar.gz \\\n       | tar zxvf -\ncd maxwell-1.5.1\n\n\n\n\nRow based replication\n\n\n\n\nMaxwell can only operate if row-based replication is on.\n\n\n$ vi my.cnf\n\n[mysqld]\nserver-id=1\nlog-bin=master\nbinlog_format=row\n\n\n\n\nOr on a running server:\n\n\nmysql\n set global binlog_row_image=FULL;\n\n\n\n\nnote\n: When changing the binlog format on a running server, currently connected mysql clients will generate binlog entries in STATEMENT format, leading\nto odd results.  In order to change to row-based replication in runtime, you must reconnect all active clients to the server.\n\n\nMysql permissions\n\n\n\n\nMaxwell stores all the state it needs within the mysql server itself, in the database called specified by the \nschema_database\n option. By default the database is named \nmaxwell\n.\n\n\nmysql\n GRANT ALL on maxwell.* to 'maxwell'@'%' identified by 'XXXXXX';\nmysql\n GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'%';\n\n# or for running maxwell locally:\n\nmysql\n GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'localhost' identified by 'XXXXXX';\nmysql\n GRANT ALL on maxwell.* to 'maxwell'@'localhost';\n\n\n\n\n\nSTDOUT producer\n\n\n\n\nUseful for smoke-testing the thing.\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout\n\n\n\n\nIf all goes well you'll see maxwell replaying your inserts:\n\n\nmysql\n insert into test.maxwell set id = 5, daemon = 'firebus!  firebus!';\nQuery OK, 1 row affected (0.04 sec)\n\n(maxwell)\n{\ntable\n:\nmaxwell\n,\ntype\n:\ninsert\n,\ndata\n:{\nid\n:5,\ndaemon\n:\nfirebus!  firebus!\n},\nts\n: 123456789}\n\n\n\n\nKafka producer\n\n\n\n\nBoot kafka as described here:  \nhttp://kafka.apache.org/documentation.html#quickstart\n, then:\n\n\nbin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n   --producer=kafka --kafka.bootstrap.servers=localhost:9092\n\n\n\n\nThis will start writing to the topic \"maxwell\".", 
            "title": "Quick Start"
        }, 
        {
            "location": "/quickstart/#download", 
            "text": "Download binary distro:  https://github.com/zendesk/maxwell/releases/download/v1.5.1/maxwell-1.5.1.tar.gz  Sources and bug tracking is available on github:  https://github.com/zendesk/maxwell  Obligatory copy/paste to terminal:   curl -sLo - https://github.com/zendesk/maxwell/releases/download/v1.5.1/maxwell-1.5.1.tar.gz \\\n       | tar zxvf -\ncd maxwell-1.5.1", 
            "title": "Download"
        }, 
        {
            "location": "/quickstart/#row-based-replication", 
            "text": "Maxwell can only operate if row-based replication is on.  $ vi my.cnf\n\n[mysqld]\nserver-id=1\nlog-bin=master\nbinlog_format=row  Or on a running server:  mysql  set global binlog_row_image=FULL;  note : When changing the binlog format on a running server, currently connected mysql clients will generate binlog entries in STATEMENT format, leading\nto odd results.  In order to change to row-based replication in runtime, you must reconnect all active clients to the server.", 
            "title": "Row based replication"
        }, 
        {
            "location": "/quickstart/#mysql-permissions", 
            "text": "Maxwell stores all the state it needs within the mysql server itself, in the database called specified by the  schema_database  option. By default the database is named  maxwell .  mysql  GRANT ALL on maxwell.* to 'maxwell'@'%' identified by 'XXXXXX';\nmysql  GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'%';\n\n# or for running maxwell locally:\n\nmysql  GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE on *.* to 'maxwell'@'localhost' identified by 'XXXXXX';\nmysql  GRANT ALL on maxwell.* to 'maxwell'@'localhost';", 
            "title": "Mysql permissions"
        }, 
        {
            "location": "/quickstart/#stdout-producer", 
            "text": "Useful for smoke-testing the thing.  bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' --producer=stdout  If all goes well you'll see maxwell replaying your inserts:  mysql  insert into test.maxwell set id = 5, daemon = 'firebus!  firebus!';\nQuery OK, 1 row affected (0.04 sec)\n\n(maxwell)\n{ table : maxwell , type : insert , data :{ id :5, daemon : firebus!  firebus! }, ts : 123456789}", 
            "title": "STDOUT producer"
        }, 
        {
            "location": "/quickstart/#kafka-producer", 
            "text": "Boot kafka as described here:   http://kafka.apache.org/documentation.html#quickstart , then:  bin/maxwell --user='maxwell' --password='XXXXXX' --host='127.0.0.1' \\\n   --producer=kafka --kafka.bootstrap.servers=localhost:9092  This will start writing to the topic \"maxwell\".", 
            "title": "Kafka producer"
        }, 
        {
            "location": "/config/", 
            "text": "Command line options\n\n\n\n\n\n\n\n\n\n\noption\n\n\nargument\n\n\ndescription\n\n\ndefault\n\n\n\n\n\n\n\n\n\n\ngeneral options\n\n\n\n\n\n\n\n\n\n\n\n\nconfig\n\n\nSTRING\n\n\nlocation of \nconfig.properties\n file\n\n\n\n\n\n\n\n\nlog_level\n\n\n[debug \n info \n warn \n error]\n\n\nlog level\n\n\nINFO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmysql options\n\n\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nSTRING\n\n\nmysql host\n\n\nlocalhost\n\n\n\n\n\n\nuser\n\n\nSTRING\n\n\nmysql username\n\n\n\n\n\n\n\n\npassword\n\n\nSTRING\n\n\nmysql password\n\n\n(no password)\n\n\n\n\n\n\nport\n\n\nINT\n\n\nmysql port\n\n\n3306\n\n\n\n\n\n\nschema_database\n\n\nSTRING\n\n\ndatabase to store schema and position in\n\n\nmaxwell\n\n\n\n\n\n\nclient_id\n\n\nSTRING\n\n\nunique text identifier for maxwell instance\n\n\nmaxwell\n\n\n\n\n\n\nreplica_server_id\n\n\nLONG\n\n\nunique numeric identifier for this maxwell instance\n\n\n6379 (see notes)\n\n\n\n\n\n\nmaster_recovery\n\n\nBOOLEAN\n\n\nenable experimental master recovery code\n\n\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreplication_host\n\n\nSTRING\n\n\nmysql host to replicate from.  Only specify if different from \nhost\n (see notes)\n\n\nschema-store host\n\n\n\n\n\n\nreplication_password\n\n\nSTRING\n\n\npassword on replication server\n\n\n(none)\n\n\n\n\n\n\nreplication_port\n\n\nINT\n\n\nport on replication server\n\n\n3306\n\n\n\n\n\n\nreplication_user\n\n\nSTRING\n\n\nuser on replication server\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nproducer options\n\n\n\n\n\n\n\n\n\n\n\n\nproducer\n\n\n[stdout \n kafka \n file \n profiler]\n\n\ntype of producer to use\n\n\nstdout\n\n\n\n\n\n\noutput_file\n\n\nSTRING\n\n\noutput file for \nfile\n producer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nkafka.bootstrap.servers\n\n\nSTRING\n\n\nkafka brokers, given as \nHOST:PORT[,HOST:PORT]\n\n\n\n\n\n\n\n\nkafka_topic\n\n\nSTRING\n\n\nkafka topic to write to. static string or variable replacement\n\n\nmaxwell\n\n\n\n\n\n\nkafka_partition_by\n\n\n[database \n table \n primary_key \n column]\n\n\ninput to kafka partition function\n\n\ndatabase\n\n\n\n\n\n\nkafka_partition_columns\n\n\nSTRING\n\n\nif partitioning by 'column', a comma separated list of columns\n\n\n\n\n\n\n\n\nkafka_partition_by_fallback\n\n\n[database \n table \n primary_key]\n\n\nrequired when kafka_partition_by=column.  Used when the column is missing\n\n\n\n\n\n\n\n\nkafka_partition_hash\n\n\n[default \n murmur3]\n\n\nhash function to use when hoosing kafka partition\n\n\ndefault\n\n\n\n\n\n\nddl_kafka_topic\n\n\nSTRING\n\n\nif output_ddl is true, kafka topic to write DDL changes to\n\n\nkafka_topic\n\n\n\n\n\n\nkafka_version\n\n\n[0.8 \n 0.9 \n 0.10]\n\n\nrun maxwell with kafka producer 0.8.2, 0.9.0 or 0.10.0.  Not available in config.properties.\n\n\n0.9.0\n\n\n\n\n\n\nformatting\n\n\n\n\n\n\n\n\n\n\n\n\noutput_binlog_position\n\n\nBOOLEAN\n\n\nshould produced records include binlog position\n\n\nfalse\n\n\n\n\n\n\noutput_commit_info\n\n\nBOOLEAN\n\n\nshould produced records include commit and xid\n\n\ntrue\n\n\n\n\n\n\noutput_nulls\n\n\nBOOLEAN\n\n\nproduced records include fields with NULL values\n\n\ntrue\n\n\n\n\n\n\noutput_server_id\n\n\nBOOLEAN\n\n\nproduced records include server_id\n\n\nfalse\n\n\n\n\n\n\noutput_thread_id\n\n\nBOOLEAN\n\n\nproduced records include thread_id\n\n\nfalse\n\n\n\n\n\n\noutput_ddl\n\n\nBOOLEAN\n\n\noutput DDL (table-alter, table-create, etc) events\n\n\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfiltering\n\n\n\n\n\n\n\n\n\n\n\n\ninclude_dbs\n\n\nPATTERN\n\n\nonly send updates from these databases\n\n\n\n\n\n\n\n\nexclude_dbs\n\n\nPATTERN\n\n\nignore updates from these databases\n\n\n\n\n\n\n\n\ninclude_tables\n\n\nPATTERN\n\n\nonly send updates from tables named like PATTERN\n\n\n\n\n\n\n\n\nexclude_tables\n\n\nPATTERN\n\n\nignore updates from tables named like PATTERN\n\n\n\n\n\n\n\n\nblacklist_dbs\n\n\nPATTERN\n\n\nignore updates AND schema changes from databases (see warnings below)\n\n\n\n\n\n\n\n\nblacklist_tables\n\n\nPATTERN\n\n\nignore updates AND schema changes from tables named like PATTERN (see warnings below)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\n\nbootstrapper\n\n\n[async \n sync \n none]\n\n\nbootstrapper type.  See bootstrapping docs.\n\n\nasync\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninit_position\n\n\nFILE:POSITION\n\n\nignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties.\n\n\n\n\n\n\n\n\nreplay\n\n\nBOOLEAN\n\n\nenable maxwell's read-only \"replay\" mode: don't store a binlog position or schema changes.  Not available in config.properties.\n\n\n\n\n\n\n\n\n\n\nProperties file\n\n\n\n\nIf maxwell finds the file \nconfig.properties\n in $PWD it will use it.  Any\ncommand line options (except \ninit_position\n, \nreplay\n, and \nkafka_version\n) may be given as\n\"key=value\" pairs.\n\n\nAdditionally, any configuration file options prefixed with 'kafka.' will be\npassed into the kafka producer library, after having 'kafka.' stripped off the\nfront of the key.  So, for example if config.properties contains\n\n\nkafka.batch.size=16384\n\n\n\n\nthen Maxwell will send \nbatch.size=16384\n to the kafka producer library.\n\n\nRunning against RDS\n\n\n\n\nTo run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following:\n\n\n\n\nset binlog_format to \"ROW\".  Do this in the \"parameter groups\" section.  For a Mysql-RDS instance this parameter will be\n  in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\".\n\n\nsetup RDS binlog retention as described \nhere\n.\n  The tl;dr is to execute \ncall mysql.rds_set_configuration('binlog retention hours', 24)\n on the server.\n\n\n\n\nFilters\n\n\n\n\nThe options \ninclude_dbs\n, \nexclude_dbs\n, \ninclude_tables\n, and \nexclude_tables\n control whether\nMaxwell will send an update for a given row to its producer.  All the options take a single value PATTERN,\nwhich may either be a literal table/database name, given as \noption=name\n, or a regular expression,\ngiven as \noption=/regex/\n.  The options are evaluated as follows:\n\n\n\n\nonly accept databases in \ninclude_dbs\n if non-empty\n\n\nreject databases in \nexclude_dbs\n\n\nonly accept tables in \ninclude_tables\n if non-empty\n\n\nreject tables in \nexclude_tables\n\n\n\n\nSo an example like \n--include_dbs=/foo.*/ --exclude_tables=bar\n will include \nfooty.zab\n and exclude \nfooty.bar\n\n\nThe option \nblacklist_tables\n and \nblacklist_dbs\n controls whether Maxwell will send updates for a table to its producer AND whether\nit captures schema changes for that table or database. Note that once Maxwell has been running with a table or database marked as blacklisted,\nyou \nmust\n continue to run Maxwell with that table or database blacklisted or else Maxwell will halt. If you want to stop\nblacklisting a table or database, you will have to drop the maxwell schema first.\n\n\nSchema storage host vs replica host\n\n\n\n\nMaxwell needs two sets of mysql permissions to operate properly: a mysql database in which to store schema snapshots,\nand a mysql host to replicate from.  The recommended configuration is that\nthese two functions are provided by a single mysql host.  In this case, just\nspecify \nhost\n, \nuser\n, etc.\n\n\nSome configurations, however, may need to write data to a different server than it replicates from.  In this case,\nthe host described by \nhost\n, \nuser\n, ..., will be used to write schema information to, and Maxwell will replicate\nevents from the host described by \nreplication_host\n, \nreplication_user\n, ...  Note that bootstrapping is not available\nin this configuration.\n\n\nrunning multiple instances of maxwell against the same master\n\n\n\n\nMaxwell can operate with multiple instances running against a single master, in\ndifferent configurations.  This can be useful if you wish to have producers\nrunning in different configurations, for example producing different groups of\ntables to different topics.  Each instance of Maxwell must be configured with a\nunique \nclient_id\n, in order to store unique binlog positions.\n\n\nmultiple instances on a 5.5 server\n\n\nWith MySQL 5.5 and below, each replicator (be it mysql, maxwell, whatever) must\nalso be configured with a unique \nreplica_server_id\n.  This is a 32-bit integer\nthat corresponds to mysql's \nserver_id\n parameter.  The value you configure\nshould be unique across all mysql and maxwell instances.\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Configuration"
        }, 
        {
            "location": "/config/#command-line-options", 
            "text": "option  argument  description  default      general options       config  STRING  location of  config.properties  file     log_level  [debug   info   warn   error]  log level  INFO          mysql options       host  STRING  mysql host  localhost    user  STRING  mysql username     password  STRING  mysql password  (no password)    port  INT  mysql port  3306    schema_database  STRING  database to store schema and position in  maxwell    client_id  STRING  unique text identifier for maxwell instance  maxwell    replica_server_id  LONG  unique numeric identifier for this maxwell instance  6379 (see notes)    master_recovery  BOOLEAN  enable experimental master recovery code  false          replication_host  STRING  mysql host to replicate from.  Only specify if different from  host  (see notes)  schema-store host    replication_password  STRING  password on replication server  (none)    replication_port  INT  port on replication server  3306    replication_user  STRING  user on replication server           producer options       producer  [stdout   kafka   file   profiler]  type of producer to use  stdout    output_file  STRING  output file for  file  producer           kafka.bootstrap.servers  STRING  kafka brokers, given as  HOST:PORT[,HOST:PORT]     kafka_topic  STRING  kafka topic to write to. static string or variable replacement  maxwell    kafka_partition_by  [database   table   primary_key   column]  input to kafka partition function  database    kafka_partition_columns  STRING  if partitioning by 'column', a comma separated list of columns     kafka_partition_by_fallback  [database   table   primary_key]  required when kafka_partition_by=column.  Used when the column is missing     kafka_partition_hash  [default   murmur3]  hash function to use when hoosing kafka partition  default    ddl_kafka_topic  STRING  if output_ddl is true, kafka topic to write DDL changes to  kafka_topic    kafka_version  [0.8   0.9   0.10]  run maxwell with kafka producer 0.8.2, 0.9.0 or 0.10.0.  Not available in config.properties.  0.9.0    formatting       output_binlog_position  BOOLEAN  should produced records include binlog position  false    output_commit_info  BOOLEAN  should produced records include commit and xid  true    output_nulls  BOOLEAN  produced records include fields with NULL values  true    output_server_id  BOOLEAN  produced records include server_id  false    output_thread_id  BOOLEAN  produced records include thread_id  false    output_ddl  BOOLEAN  output DDL (table-alter, table-create, etc) events  false          filtering       include_dbs  PATTERN  only send updates from these databases     exclude_dbs  PATTERN  ignore updates from these databases     include_tables  PATTERN  only send updates from tables named like PATTERN     exclude_tables  PATTERN  ignore updates from tables named like PATTERN     blacklist_dbs  PATTERN  ignore updates AND schema changes from databases (see warnings below)     blacklist_tables  PATTERN  ignore updates AND schema changes from tables named like PATTERN (see warnings below)           misc       bootstrapper  [async   sync   none]  bootstrapper type.  See bootstrapping docs.  async          init_position  FILE:POSITION  ignore the information in maxwell.positions and start at the given binlog position. Not available in config.properties.     replay  BOOLEAN  enable maxwell's read-only \"replay\" mode: don't store a binlog position or schema changes.  Not available in config.properties.", 
            "title": "Command line options"
        }, 
        {
            "location": "/config/#properties-file", 
            "text": "If maxwell finds the file  config.properties  in $PWD it will use it.  Any\ncommand line options (except  init_position ,  replay , and  kafka_version ) may be given as\n\"key=value\" pairs.  Additionally, any configuration file options prefixed with 'kafka.' will be\npassed into the kafka producer library, after having 'kafka.' stripped off the\nfront of the key.  So, for example if config.properties contains  kafka.batch.size=16384  then Maxwell will send  batch.size=16384  to the kafka producer library.", 
            "title": "Properties file"
        }, 
        {
            "location": "/config/#running-against-rds", 
            "text": "To run Maxwell against RDS, (either Aurora or Mysql) you will need to do the following:   set binlog_format to \"ROW\".  Do this in the \"parameter groups\" section.  For a Mysql-RDS instance this parameter will be\n  in a \"DB Parameter Group\", for Aurora it will be in a \"DB Cluster Parameter Group\".  setup RDS binlog retention as described  here .\n  The tl;dr is to execute  call mysql.rds_set_configuration('binlog retention hours', 24)  on the server.", 
            "title": "Running against RDS"
        }, 
        {
            "location": "/config/#filters", 
            "text": "The options  include_dbs ,  exclude_dbs ,  include_tables , and  exclude_tables  control whether\nMaxwell will send an update for a given row to its producer.  All the options take a single value PATTERN,\nwhich may either be a literal table/database name, given as  option=name , or a regular expression,\ngiven as  option=/regex/ .  The options are evaluated as follows:   only accept databases in  include_dbs  if non-empty  reject databases in  exclude_dbs  only accept tables in  include_tables  if non-empty  reject tables in  exclude_tables   So an example like  --include_dbs=/foo.*/ --exclude_tables=bar  will include  footy.zab  and exclude  footy.bar  The option  blacklist_tables  and  blacklist_dbs  controls whether Maxwell will send updates for a table to its producer AND whether\nit captures schema changes for that table or database. Note that once Maxwell has been running with a table or database marked as blacklisted,\nyou  must  continue to run Maxwell with that table or database blacklisted or else Maxwell will halt. If you want to stop\nblacklisting a table or database, you will have to drop the maxwell schema first.", 
            "title": "Filters"
        }, 
        {
            "location": "/config/#schema-storage-host-vs-replica-host", 
            "text": "Maxwell needs two sets of mysql permissions to operate properly: a mysql database in which to store schema snapshots,\nand a mysql host to replicate from.  The recommended configuration is that\nthese two functions are provided by a single mysql host.  In this case, just\nspecify  host ,  user , etc.  Some configurations, however, may need to write data to a different server than it replicates from.  In this case,\nthe host described by  host ,  user , ..., will be used to write schema information to, and Maxwell will replicate\nevents from the host described by  replication_host ,  replication_user , ...  Note that bootstrapping is not available\nin this configuration.", 
            "title": "Schema storage host vs replica host"
        }, 
        {
            "location": "/config/#running-multiple-instances-of-maxwell-against-the-same-master", 
            "text": "Maxwell can operate with multiple instances running against a single master, in\ndifferent configurations.  This can be useful if you wish to have producers\nrunning in different configurations, for example producing different groups of\ntables to different topics.  Each instance of Maxwell must be configured with a\nunique  client_id , in order to store unique binlog positions.", 
            "title": "running multiple instances of maxwell against the same master"
        }, 
        {
            "location": "/config/#multiple-instances-on-a-55-server", 
            "text": "With MySQL 5.5 and below, each replicator (be it mysql, maxwell, whatever) must\nalso be configured with a unique  replica_server_id .  This is a 32-bit integer\nthat corresponds to mysql's  server_id  parameter.  The value you configure\nshould be unique across all mysql and maxwell instances.  \n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "multiple instances on a 5.5 server"
        }, 
        {
            "location": "/kafka/", 
            "text": "Kafka options\n\n\n\n\nAny options given to Maxwell that are prefixed with \nkafka.\n will be passed directly into the Kafka producer configuration\n(with \nkafka.\n stripped off).  We use the \"new producer\" configuration, as described here:\n\nhttp://kafka.apache.org/documentation.html#newproducerconfigs\n\n\nThese are recommended properties. You can remove or update them in \nconfig.properties\n.\n- kafka.acks = 1\n- kafka.compression.type = snappy\n- kafka.metadata.fetch.timeout.ms=5000\n- kafka.retries=0\n\n\nMaxwell writes to a kafka topic named \"maxwell\" by default. It can be static, e.g. 'maxwell', or dynamic, e.g. \nnamespace_%{database}_%{table}\n. In the latter case 'database' and 'table' will be replaced with the values for the row being processed. This can be changed with the \nkafka_topic\n option.\n\n\nKafka key\n\n\n\n\nMaxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format:\n\n\n{ \ndatabase\n:\ntest_tb\n,\ntable\n:\ntest_tbl\n,\npk.id\n:4,\npk.part2\n:\nhello\n}\n\n\n\n\nThis key is designed to co-operate with Kafka's log compaction, which will save the last-known\nvalue for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act\nas a source of truth.\n\n\nPartitioning\n\n\n\n\nA binlog event's partition is determined by the selected hash function and hash string as follows\n\n\n  HASH_FUNCTION(HASH_STRING) % TOPIC.NUMBER_OF_PARTITIONS\n\n\n\n\nThe HASH_FUNCTION is either java's \nhashCode\n or \nmurmurhash3\n. The default HASH_FUNCTION is \nhashCode\n. Murmurhash3 may be set with the \nkafka_partition_hash\n option. The seed value for the murmurhash function is hardcoded to 25342 in the MaxwellKafkaPartitioner class.\n\n\nThe HASH_STRING may be (\ndatabase\n, \ntable\n, \nprimary_key\n, \ncolumn\n).  The default HASH_STRING is the \ndatabase\n. The partitioning field can be configured using the \nkafka_partition_by\n option.\n\n\nWhen using \nkafka_partition_by\n=\ncolumn\n you must set \nkafka_partition_columns\n with the column name(s) to partition by (e.g. \nkafka_partition_columns\n=user_id or \nkafka_partition_columns\n=user_id,create_date). You must also set \nkafka_partiton_by_fallback\n. This may be (\ndatabase\n, \ntable\n, \nprimary_key\n). It is used when the column(s) specified does not exist in the current row. The default is \ndatabase\n.\nWhen partitioning by \ncolumn\n Maxwell will treat the values for the specified columns as strings, concatenate them and use that value to partition the data. The above example, partitioning by user_id + create_date would have a partition key similar to \n1178532016-10-10 18:29:04\n.\n\n\nMaxwell will discover the number of partitions in its kafka topic upon boot.  This means that you should pre-create your kafka topics,\nand with at least as many partitions as you have logical databases:\n\n\nbin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\\n                    --topic maxwell --partitions 20 --replication-factor 2\n\n\n\n\nhttp://kafka.apache.org/documentation.html#quickstart\n\n\nKafka 0.9\n\n\n\n\nBy default, maxwell runs with kafka clients 0.9.0.1. There is a flag (--kafka_version) that allows maxwell to run with either 0.8.2.2, 0.9.0.1 or 0.10.0.1.\nNoteables:\n- Kafka clients 0.9.0.1 are not compatible with brokers running kafka 0.8. The exception below will show in logs when that is the case:\n\n\n14:53:06,033 ERROR Sender - Uncaught error in kafka producer I/O thread: \norg.apache.kafka.common.protocol.types.SchemaException: Error reading field 'throttle_time_ms': java.nio.BufferUnderflowException\n    at org.apache.kafka.common.protocol.types.Schema.read(Schema.java:71) ~[kafka-clients-0.9.0.1.jar:?]\n    at org.apache.kafka.clients.NetworkClient.handleCompletedReceives(NetworkClient.java:439) ~[kafka-clients-0.9.0.1.jar:?]\n    at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:265) ~[kafka-clients-0.9.0.1.jar:?]\n    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:216) ~[kafka-clients-0.9.0.1.jar:?]\n    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:128) [kafka-clients-0.9.0.1.jar:?]\n    at java.lang.Thread.run(Thread.java:745) [?:1.7.0_79]\n\n\n\n\n\n\nKafka clients 0.8 and 0.9 are compatible with brokers running kafka 0.8.\n\n\n0.10.0.x clients only support 0.10.0.x or later brokers.\n\n\nMixing Kafka 0.10 with other versions can lead to serious performance impacts:\nhttp://kafka.apache.org/0100/documentation.html#upgrade_10\n\n\nNotes to clients with version 0.9.0.0: Due to a bug introduced in 0.9.0.0, clients that depend on ZooKeeper (old Scala high-level Consumer and MirrorMaker if used with the old consumer) will not work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 before brokers are upgraded to 0.10.0.x. This step is not necessary for 0.8.X or 0.9.0.1 clients.\nhttp://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact\nThe message format in 0.10.0 includes a new timestamp field and uses relative offsets for compressed messages. The on disk message format can be configured through log.message.format.version in the server.properties file. The default on-disk message format is 0.10.0. If a consumer client is on a version before 0.10.0.0, it only understands message formats before 0.10.0. In this case, the broker is able to convert messages from the 0.10.0 format to an earlier format before sending the response to the consumer on an older version. However, the broker can't use zero-copy transfer in this case. Reports from the Kafka community on the performance impact have shown CPU utilization going from 20% before to 100% after an upgrade, which forced an immediate upgrade of all clients to bring performance back to normal. To avoid such message conversion before consumers are upgraded to 0.10.0.0, one can set log.message.format.version to 0.8.2 or 0.9.0 when upgrading the broker to 0.10.0.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once consumers are upgraded, one can change the message format to 0.10.0 on the broker and enjoy the new message format that includes new timestamp and improved compression. The conversion is supported to ensure compatibility and can be useful to support a few apps that have not updated to newer clients yet, but is impractical to support all consumer traffic on even an overprovisioned cluster. Therefore it is critical to avoid the message conversion as much as possible when brokers have been upgraded but the majority of clients have not.\nFor clients that are upgraded to 0.10.0.0, there is no performance impact.\nNote: By setting the message format version, one certifies that all existing messages are on or below that message format version. Otherwise consumers before 0.10.0.0 might break. In particular, after the message format is set to 0.10.0, one should not change it back to an earlier format as it may break consumers on versions before 0.10.0.0.\n\n\n\n\n\n\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Kafka"
        }, 
        {
            "location": "/kafka/#kafka-options", 
            "text": "Any options given to Maxwell that are prefixed with  kafka.  will be passed directly into the Kafka producer configuration\n(with  kafka.  stripped off).  We use the \"new producer\" configuration, as described here: http://kafka.apache.org/documentation.html#newproducerconfigs  These are recommended properties. You can remove or update them in  config.properties .\n- kafka.acks = 1\n- kafka.compression.type = snappy\n- kafka.metadata.fetch.timeout.ms=5000\n- kafka.retries=0  Maxwell writes to a kafka topic named \"maxwell\" by default. It can be static, e.g. 'maxwell', or dynamic, e.g.  namespace_%{database}_%{table} . In the latter case 'database' and 'table' will be replaced with the values for the row being processed. This can be changed with the  kafka_topic  option.", 
            "title": "Kafka options"
        }, 
        {
            "location": "/kafka/#kafka-key", 
            "text": "Maxwell generates keys for its Kafka messages based upon a mysql row's primary key in JSON format:  {  database : test_tb , table : test_tbl , pk.id :4, pk.part2 : hello }  This key is designed to co-operate with Kafka's log compaction, which will save the last-known\nvalue for a key, allowing Maxwell's Kafka stream to retain the last-known value for a row and act\nas a source of truth.", 
            "title": "Kafka key"
        }, 
        {
            "location": "/kafka/#partitioning", 
            "text": "A binlog event's partition is determined by the selected hash function and hash string as follows    HASH_FUNCTION(HASH_STRING) % TOPIC.NUMBER_OF_PARTITIONS  The HASH_FUNCTION is either java's  hashCode  or  murmurhash3 . The default HASH_FUNCTION is  hashCode . Murmurhash3 may be set with the  kafka_partition_hash  option. The seed value for the murmurhash function is hardcoded to 25342 in the MaxwellKafkaPartitioner class.  The HASH_STRING may be ( database ,  table ,  primary_key ,  column ).  The default HASH_STRING is the  database . The partitioning field can be configured using the  kafka_partition_by  option.  When using  kafka_partition_by = column  you must set  kafka_partition_columns  with the column name(s) to partition by (e.g.  kafka_partition_columns =user_id or  kafka_partition_columns =user_id,create_date). You must also set  kafka_partiton_by_fallback . This may be ( database ,  table ,  primary_key ). It is used when the column(s) specified does not exist in the current row. The default is  database .\nWhen partitioning by  column  Maxwell will treat the values for the specified columns as strings, concatenate them and use that value to partition the data. The above example, partitioning by user_id + create_date would have a partition key similar to  1178532016-10-10 18:29:04 .  Maxwell will discover the number of partitions in its kafka topic upon boot.  This means that you should pre-create your kafka topics,\nand with at least as many partitions as you have logical databases:  bin/kafka-topics.sh --zookeeper ZK_HOST:2181 --create \\\n                    --topic maxwell --partitions 20 --replication-factor 2  http://kafka.apache.org/documentation.html#quickstart", 
            "title": "Partitioning"
        }, 
        {
            "location": "/kafka/#kafka-09", 
            "text": "By default, maxwell runs with kafka clients 0.9.0.1. There is a flag (--kafka_version) that allows maxwell to run with either 0.8.2.2, 0.9.0.1 or 0.10.0.1.\nNoteables:\n- Kafka clients 0.9.0.1 are not compatible with brokers running kafka 0.8. The exception below will show in logs when that is the case:  14:53:06,033 ERROR Sender - Uncaught error in kafka producer I/O thread: \norg.apache.kafka.common.protocol.types.SchemaException: Error reading field 'throttle_time_ms': java.nio.BufferUnderflowException\n    at org.apache.kafka.common.protocol.types.Schema.read(Schema.java:71) ~[kafka-clients-0.9.0.1.jar:?]\n    at org.apache.kafka.clients.NetworkClient.handleCompletedReceives(NetworkClient.java:439) ~[kafka-clients-0.9.0.1.jar:?]\n    at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:265) ~[kafka-clients-0.9.0.1.jar:?]\n    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:216) ~[kafka-clients-0.9.0.1.jar:?]\n    at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:128) [kafka-clients-0.9.0.1.jar:?]\n    at java.lang.Thread.run(Thread.java:745) [?:1.7.0_79]   Kafka clients 0.8 and 0.9 are compatible with brokers running kafka 0.8.  0.10.0.x clients only support 0.10.0.x or later brokers.  Mixing Kafka 0.10 with other versions can lead to serious performance impacts:\nhttp://kafka.apache.org/0100/documentation.html#upgrade_10  Notes to clients with version 0.9.0.0: Due to a bug introduced in 0.9.0.0, clients that depend on ZooKeeper (old Scala high-level Consumer and MirrorMaker if used with the old consumer) will not work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 before brokers are upgraded to 0.10.0.x. This step is not necessary for 0.8.X or 0.9.0.1 clients.\nhttp://kafka.apache.org/0100/documentation.html#upgrade_10_performance_impact\nThe message format in 0.10.0 includes a new timestamp field and uses relative offsets for compressed messages. The on disk message format can be configured through log.message.format.version in the server.properties file. The default on-disk message format is 0.10.0. If a consumer client is on a version before 0.10.0.0, it only understands message formats before 0.10.0. In this case, the broker is able to convert messages from the 0.10.0 format to an earlier format before sending the response to the consumer on an older version. However, the broker can't use zero-copy transfer in this case. Reports from the Kafka community on the performance impact have shown CPU utilization going from 20% before to 100% after an upgrade, which forced an immediate upgrade of all clients to bring performance back to normal. To avoid such message conversion before consumers are upgraded to 0.10.0.0, one can set log.message.format.version to 0.8.2 or 0.9.0 when upgrading the broker to 0.10.0.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once consumers are upgraded, one can change the message format to 0.10.0 on the broker and enjoy the new message format that includes new timestamp and improved compression. The conversion is supported to ensure compatibility and can be useful to support a few apps that have not updated to newer clients yet, but is impractical to support all consumer traffic on even an overprovisioned cluster. Therefore it is critical to avoid the message conversion as much as possible when brokers have been upgraded but the majority of clients have not.\nFor clients that are upgraded to 0.10.0.0, there is no performance impact.\nNote: By setting the message format version, one certifies that all existing messages are on or below that message format version. Otherwise consumers before 0.10.0.0 might break. In particular, after the message format is set to 0.10.0, one should not change it back to an earlier format as it may break consumers on versions before 0.10.0.0.     \n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Kafka 0.9"
        }, 
        {
            "location": "/dataformat/", 
            "text": "So you ran some sql?\n\n\ncreate table test.e (\n  id int(10) not null primary key auto_increment,\n  m double,\n  c timestamp(6),\n  comment varchar(255) charset 'latin1'\n);\n\ninsert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.';\nupdate test.e set m = 5.444, c = now(3) where id = 1;\ndelete from test.e where id = 1;\nalter table test.e add column torvalds bigint unsigned after m;\ndrop table test.e;\n\n\n\nMaxwell will produce some output for that.  Let's look at it.\n\n\nINSERT\n\n\n\n\nmysql\n insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.';\n{\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \ntype\n:\ninsert\n,\n   \nts\n:1477053217,\n   \nxid\n:23396,\n   \ncommit\n:true,\n   \nposition\n:\nmaster.000006:800911\n,\n   \nserver_id\n:23042,\n   \nthread_id\n:108,\n   \ndata\n:{\n      \nid\n:1,\n      \nm\n:4.2341,\n      \nc\n:\n2016-10-21 05:33:37.523000\n,\n      \ncomment\n:\nI am a creature of light.\n\n   }\n}\n\n\n\n\n\nMost of the fields are self-explanatory, but a couple of them deserve mention:\n\n\n\u21b3 \n\"type\":\"insert\",\n\n\nMost commonly you will see insert/update/delete here.  If you're bootstrapping\na table, you will see \"bootstrap-insert\", and DDL statements (explained later)\nhave their own types.\n\n\n\u21b3 \n\"xid\":23396,\n\n\nThis is InnoDB's \"transaction ID\" for the transaction this row is associated\nwith.  It's unique within the lifetime of a server as near as I can tell.\n\n\n\u21b3 \n\"server_id\":23042,\n\n\nThe mysql server_id of the server that accepted this transaction.\n\n\n\u21b3 \n\"thread_id\":108,\n\n\nA thread_id is more or less a unique identifier of the client connection that\ngenerated the data.\n\n\n\u21b3 \n\"commit\":true,\n\n\nIf you need to re-assemble transactions in your stream processors, you can use\nthis field and \nxid\n to do so.  The data will look like:\n\n\n\n\nrow with no \ncommit\n, xid=142\n\n\nrow with no \ncommit\n, xid=142\n\n\nrow with \ncommit=true\n, xid=142\n\n\nrow with no \ncommit\n, xid=155\n\n\n...\n\n\n\n\nUPDATE\n\n\n\n\nmysql\n update test.e set m = 5.444, c = now(3) where id = 1;\n{\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \ntype\n:\nupdate\n,\n   \nts\n:1477053234,\n   ...\n   \ndata\n:{\n      \nid\n:1,\n      \nm\n:5.444,\n      \nc\n:\n2016-10-21 05:33:54.631000\n,\n      \ncomment\n:\nI am a creature of light.\n\n   },\n   \nold\n:{\n      \nm\n:4.2341,\n      \nc\n:\n2016-10-21 05:33:37.523000\n\n   }\n}\n\n\n\n\n\nWhat's important to note here is the \nold\n field, which stores old values for\nrows that changed.  So \ndata\n still has a complete copy of the row (just as\nwith the insert), but now you can reconstruct what the row \nwas\n by doing\n\ndata.merge(old)\n.\n\n\nDELETE\n\n\n\n\nmysql\n delete from test.e where id = 1;\n{\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \ntype\n:\ndelete\n,\n   ...\n   \ndata\n:{\n      \nid\n:1,\n      \nm\n:5.444,\n      \nc\n:\n2016-10-21 05:33:54.631000\n,\n      \ncomment\n:\nI am a creature of light.\n\n   }\n}\n\n\n\n\nafter a DELETE, \ndata\n contains a copy of the row, just before it shuffled off\nthis mortal coil.\n\n\nCREATE TABLE\n\n\n\n\ncreate table test.e ( ... )\n{\n   \ntype\n:\ntable-create\n,\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \ndef\n:{\n      \ndatabase\n:\ntest\n,\n      \ncharset\n:\nutf8mb4\n,\n      \ntable\n:\ne\n,\n      \ncolumns\n:[\n         {\n            \ntype\n:\nint\n,\n            \nname\n:\nid\n,\n            \nsigned\n:true\n         },\n         {\n            \ntype\n:\ndouble\n,\n            \nname\n:\nm\n\n         },\n         {\n            \ntype\n:\ntimestamp\n,\n            \nname\n:\nc\n,\n            \ncolumn-length\n:6\n         },\n         {\n            \ntype\n:\nvarchar\n,\n            \nname\n:\ncomment\n,\n            \ncharset\n:\nlatin1\n\n         }\n      ],\n      \nprimary-key\n:[\n         \nid\n\n      ]\n   },\n   \nts\n:1477053126000,\n   \nsql\n:\ncreate table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' )\n,\n   \nposition\n:\nmaster.000006:800050\n\n}\n\n\n\n\n\nYou only get this with \n--output_ddl\n.\n\n\n\u21b3 \n\"type\": \"table-create\"\n\nhere you have \ndatabase-create\n, \ndatabase-alter\n, \ndatabase-drop\n, \ntable-create\n, \ntable-alter\n, \ntable-drop\n.\n\n\n\u21b3 \n\"type\":\"int\",\n\nMostly here we preserve the inbound type of the column.  There's a couple of\nexceptions where we will change the column type, you could read about them in the\n\nunalias_type\n\nfunction if you so desired.\n\n\nALTER TABLE\n\n\nmysql\n alter table test.e add column torvalds bigint unsigned after m;\n{\n   \ntype\n:\ntable-alter\n,\n   \ndatabase\n:\ntest\n,\n   \ntable\n:\ne\n,\n   \nold\n:{\n      \ndatabase\n:\ntest\n,\n      \ncharset\n:\nutf8mb4\n,\n      \ntable\n:\ne\n,\n      \ncolumns\n:[\n         {\n            \ntype\n:\nint\n,\n            \nname\n:\nid\n,\n            \nsigned\n:true\n         },\n         {\n            \ntype\n:\ndouble\n,\n            \nname\n:\nm\n\n         },\n         {\n            \ntype\n:\ntimestamp\n,\n            \nname\n:\nc\n,\n            \ncolumn-length\n:6\n         },\n         {\n            \ntype\n:\nvarchar\n,\n            \nname\n:\ncomment\n,\n            \ncharset\n:\nlatin1\n\n         }\n      ],\n      \nprimary-key\n:[\n         \nid\n\n      ]\n   },\n   \ndef\n:{\n      \ndatabase\n:\ntest\n,\n      \ncharset\n:\nutf8mb4\n,\n      \ntable\n:\ne\n,\n      \ncolumns\n:[\n         {\n            \ntype\n:\nint\n,\n            \nname\n:\nid\n,\n            \nsigned\n:true\n         },\n         {\n            \ntype\n:\ndouble\n,\n            \nname\n:\nm\n\n         },\n         {\n            \ntype\n:\nbigint\n,\n            \nname\n:\ntorvalds\n,\n            \nsigned\n:false\n         },\n         {\n            \ntype\n:\ntimestamp\n,\n            \nname\n:\nc\n,\n            \ncolumn-length\n:6\n         },\n         {\n            \ntype\n:\nvarchar\n,\n            \nname\n:\ncomment\n,\n            \ncharset\n:\nlatin1\n\n         }\n      ],\n      \nprimary-key\n:[\n         \nid\n\n      ]\n   },\n   \nts\n:1477053308000,\n   \nsql\n:\nalter table test.e add column torvalds bigint unsigned after m\n,\n   \nposition\n:\nmaster.000006:804398\n\n}\n\n\n\n\nAs with the UPDATE, we have a complete image of the table before-and-after the alter\n\n\nblob (+ binary encoded strings)\n\n\n\n\nMaxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding).\n\n\ndatetime\n\n\n\n\nDatetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings.  Note that mysql\nhas no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and\nMaxwell chooses to reproduce these invalid datetimes faithfully,\nfor lack of something better to do.\n\n\nmysql\n    create table test_datetime ( id int(11), dtcol datetime );\nmysql\n    insert into test_datetime set dtcol='0000-00-00 00:00:00';\n\n\nmaxwell  { \ntable\n : \ntest_datetime\n, \ntype\n: \ninsert\n, \ndata\n: { \ndtcol\n: \n0000-00-00 00:00:00\n } }\n\n\n\n\nAs of 1.3.0, Maxwell supports microsecond precision datetime/timestamp/time columns.\n\n\nsets\n\n\n\n\noutput as JSON arrays.\n\n\nmysql\n   create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') );\nmysql\n   insert into test_sets set setcol = 'b_val,c_val';\n\n\nmaxwell { \ntable\n:\ntest_sets\n, \ntype\n:\ninsert\n, \ndata\n: { \nsetcol\n: [\nb_val\n, \nc_val\n] } }\n\n\n\n\nstrings (varchar, text)\n\n\n\n\nMaxwell will accept a variety of character encodings, but will always output UTF-8 strings.  The following table\ndescribes support for mysql's character sets:\n\n\n\n\n\n\n\n\ncharset\n\n\nstatus\n\n\n\n\n\n\n\n\n\n\nutf8\n\n\nsupported\n\n\n\n\n\n\nutf8mb4\n\n\nsupported\n\n\n\n\n\n\nlatin1\n\n\nsupported\n\n\n\n\n\n\nlatin2\n\n\nsupported\n\n\n\n\n\n\nascii\n\n\nsupported\n\n\n\n\n\n\nucs2\n\n\nsupported\n\n\n\n\n\n\nbinary\n\n\nsupported (as base64)\n\n\n\n\n\n\nutf16\n\n\nsupported, not tested in production\n\n\n\n\n\n\nutf32\n\n\nsupported, not tested in production\n\n\n\n\n\n\nbig5\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp850\n\n\nsupported, not tested in production\n\n\n\n\n\n\nsjis\n\n\nsupported, not tested in production\n\n\n\n\n\n\nhebrew\n\n\nsupported, not tested in production\n\n\n\n\n\n\ntis620\n\n\nsupported, not tested in production\n\n\n\n\n\n\neuckr\n\n\nsupported, not tested in production\n\n\n\n\n\n\ngb2312\n\n\nsupported, not tested in production\n\n\n\n\n\n\ngreek\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp1250\n\n\nsupported, not tested in production\n\n\n\n\n\n\ngbk\n\n\nsupported, not tested in production\n\n\n\n\n\n\nlatin5\n\n\nsupported, not tested in production\n\n\n\n\n\n\nmacroman\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp852\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp1251\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp866\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp1256\n\n\nsupported, not tested in production\n\n\n\n\n\n\ncp1257\n\n\nsupported, not tested in production\n\n\n\n\n\n\ndec8\n\n\nunsupported\n\n\n\n\n\n\nhp8\n\n\nunsupported\n\n\n\n\n\n\nkoi8r\n\n\nunsupported\n\n\n\n\n\n\nswe7\n\n\nunsupported\n\n\n\n\n\n\nujis\n\n\nunsupported\n\n\n\n\n\n\nkoi8u\n\n\nunsupported\n\n\n\n\n\n\narmscii8\n\n\nunsupported\n\n\n\n\n\n\nkeybcs2\n\n\nunsupported\n\n\n\n\n\n\nmacce\n\n\nunsupported\n\n\n\n\n\n\nlatin7\n\n\nunsupported\n\n\n\n\n\n\ngeostd8\n\n\nunsupported\n\n\n\n\n\n\ncp932\n\n\nunsupported\n\n\n\n\n\n\neucjpms\n\n\nunsupported\n\n\n\n\n\n\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Data Format"
        }, 
        {
            "location": "/dataformat/#so-you-ran-some-sql", 
            "text": "create table test.e (\n  id int(10) not null primary key auto_increment,\n  m double,\n  c timestamp(6),\n  comment varchar(255) charset 'latin1'\n);\n\ninsert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.';\nupdate test.e set m = 5.444, c = now(3) where id = 1;\ndelete from test.e where id = 1;\nalter table test.e add column torvalds bigint unsigned after m;\ndrop table test.e;  Maxwell will produce some output for that.  Let's look at it.", 
            "title": "So you ran some sql?"
        }, 
        {
            "location": "/dataformat/#insert", 
            "text": "mysql  insert into test.e set m = 4.2341, c = now(3), comment = 'I am a creature of light.';\n{\n    database : test ,\n    table : e ,\n    type : insert ,\n    ts :1477053217,\n    xid :23396,\n    commit :true,\n    position : master.000006:800911 ,\n    server_id :23042,\n    thread_id :108,\n    data :{\n       id :1,\n       m :4.2341,\n       c : 2016-10-21 05:33:37.523000 ,\n       comment : I am a creature of light. \n   }\n}  Most of the fields are self-explanatory, but a couple of them deserve mention:  \u21b3  \"type\":\"insert\",  Most commonly you will see insert/update/delete here.  If you're bootstrapping\na table, you will see \"bootstrap-insert\", and DDL statements (explained later)\nhave their own types.  \u21b3  \"xid\":23396,  This is InnoDB's \"transaction ID\" for the transaction this row is associated\nwith.  It's unique within the lifetime of a server as near as I can tell.  \u21b3  \"server_id\":23042,  The mysql server_id of the server that accepted this transaction.  \u21b3  \"thread_id\":108,  A thread_id is more or less a unique identifier of the client connection that\ngenerated the data.  \u21b3  \"commit\":true,  If you need to re-assemble transactions in your stream processors, you can use\nthis field and  xid  to do so.  The data will look like:   row with no  commit , xid=142  row with no  commit , xid=142  row with  commit=true , xid=142  row with no  commit , xid=155  ...", 
            "title": "INSERT"
        }, 
        {
            "location": "/dataformat/#update", 
            "text": "mysql  update test.e set m = 5.444, c = now(3) where id = 1;\n{\n    database : test ,\n    table : e ,\n    type : update ,\n    ts :1477053234,\n   ...\n    data :{\n       id :1,\n       m :5.444,\n       c : 2016-10-21 05:33:54.631000 ,\n       comment : I am a creature of light. \n   },\n    old :{\n       m :4.2341,\n       c : 2016-10-21 05:33:37.523000 \n   }\n}  What's important to note here is the  old  field, which stores old values for\nrows that changed.  So  data  still has a complete copy of the row (just as\nwith the insert), but now you can reconstruct what the row  was  by doing data.merge(old) .", 
            "title": "UPDATE"
        }, 
        {
            "location": "/dataformat/#delete", 
            "text": "mysql  delete from test.e where id = 1;\n{\n    database : test ,\n    table : e ,\n    type : delete ,\n   ...\n    data :{\n       id :1,\n       m :5.444,\n       c : 2016-10-21 05:33:54.631000 ,\n       comment : I am a creature of light. \n   }\n}  after a DELETE,  data  contains a copy of the row, just before it shuffled off\nthis mortal coil.", 
            "title": "DELETE"
        }, 
        {
            "location": "/dataformat/#create-table", 
            "text": "create table test.e ( ... )\n{\n    type : table-create ,\n    database : test ,\n    table : e ,\n    def :{\n       database : test ,\n       charset : utf8mb4 ,\n       table : e ,\n       columns :[\n         {\n             type : int ,\n             name : id ,\n             signed :true\n         },\n         {\n             type : double ,\n             name : m \n         },\n         {\n             type : timestamp ,\n             name : c ,\n             column-length :6\n         },\n         {\n             type : varchar ,\n             name : comment ,\n             charset : latin1 \n         }\n      ],\n       primary-key :[\n          id \n      ]\n   },\n    ts :1477053126000,\n    sql : create table test.e ( id int(10) not null primary key auto_increment, m double, c timestamp(6), comment varchar(255) charset 'latin1' ) ,\n    position : master.000006:800050 \n}  You only get this with  --output_ddl .  \u21b3  \"type\": \"table-create\" \nhere you have  database-create ,  database-alter ,  database-drop ,  table-create ,  table-alter ,  table-drop .  \u21b3  \"type\":\"int\", \nMostly here we preserve the inbound type of the column.  There's a couple of\nexceptions where we will change the column type, you could read about them in the unalias_type \nfunction if you so desired.", 
            "title": "CREATE TABLE"
        }, 
        {
            "location": "/dataformat/#alter-table", 
            "text": "mysql  alter table test.e add column torvalds bigint unsigned after m;\n{\n    type : table-alter ,\n    database : test ,\n    table : e ,\n    old :{\n       database : test ,\n       charset : utf8mb4 ,\n       table : e ,\n       columns :[\n         {\n             type : int ,\n             name : id ,\n             signed :true\n         },\n         {\n             type : double ,\n             name : m \n         },\n         {\n             type : timestamp ,\n             name : c ,\n             column-length :6\n         },\n         {\n             type : varchar ,\n             name : comment ,\n             charset : latin1 \n         }\n      ],\n       primary-key :[\n          id \n      ]\n   },\n    def :{\n       database : test ,\n       charset : utf8mb4 ,\n       table : e ,\n       columns :[\n         {\n             type : int ,\n             name : id ,\n             signed :true\n         },\n         {\n             type : double ,\n             name : m \n         },\n         {\n             type : bigint ,\n             name : torvalds ,\n             signed :false\n         },\n         {\n             type : timestamp ,\n             name : c ,\n             column-length :6\n         },\n         {\n             type : varchar ,\n             name : comment ,\n             charset : latin1 \n         }\n      ],\n       primary-key :[\n          id \n      ]\n   },\n    ts :1477053308000,\n    sql : alter table test.e add column torvalds bigint unsigned after m ,\n    position : master.000006:804398 \n}  As with the UPDATE, we have a complete image of the table before-and-after the alter", 
            "title": "ALTER TABLE"
        }, 
        {
            "location": "/dataformat/#blob-binary-encoded-strings", 
            "text": "Maxell will base64 encode BLOB, BINARY and VARBINARY columns (as well as varchar/string columns with a BINARY encoding).", 
            "title": "blob (+ binary encoded strings)"
        }, 
        {
            "location": "/dataformat/#datetime", 
            "text": "Datetime columns are output as \"YYYY-MM-DD hh:mm::ss\" strings.  Note that mysql\nhas no problem storing invalid datetimes like \"0000-00-00 00:00:00\", and\nMaxwell chooses to reproduce these invalid datetimes faithfully,\nfor lack of something better to do.  mysql     create table test_datetime ( id int(11), dtcol datetime );\nmysql     insert into test_datetime set dtcol='0000-00-00 00:00:00'; maxwell  {  table  :  test_datetime ,  type :  insert ,  data : {  dtcol :  0000-00-00 00:00:00  } }  As of 1.3.0, Maxwell supports microsecond precision datetime/timestamp/time columns.", 
            "title": "datetime"
        }, 
        {
            "location": "/dataformat/#sets", 
            "text": "output as JSON arrays.  mysql    create table test_sets ( id int(11), setcol set('a_val', 'b_val', 'c_val') );\nmysql    insert into test_sets set setcol = 'b_val,c_val'; maxwell {  table : test_sets ,  type : insert ,  data : {  setcol : [ b_val ,  c_val ] } }", 
            "title": "sets"
        }, 
        {
            "location": "/dataformat/#strings-varchar-text", 
            "text": "Maxwell will accept a variety of character encodings, but will always output UTF-8 strings.  The following table\ndescribes support for mysql's character sets:     charset  status      utf8  supported    utf8mb4  supported    latin1  supported    latin2  supported    ascii  supported    ucs2  supported    binary  supported (as base64)    utf16  supported, not tested in production    utf32  supported, not tested in production    big5  supported, not tested in production    cp850  supported, not tested in production    sjis  supported, not tested in production    hebrew  supported, not tested in production    tis620  supported, not tested in production    euckr  supported, not tested in production    gb2312  supported, not tested in production    greek  supported, not tested in production    cp1250  supported, not tested in production    gbk  supported, not tested in production    latin5  supported, not tested in production    macroman  supported, not tested in production    cp852  supported, not tested in production    cp1251  supported, not tested in production    cp866  supported, not tested in production    cp1256  supported, not tested in production    cp1257  supported, not tested in production    dec8  unsupported    hp8  unsupported    koi8r  unsupported    swe7  unsupported    ujis  unsupported    koi8u  unsupported    armscii8  unsupported    keybcs2  unsupported    macce  unsupported    latin7  unsupported    geostd8  unsupported    cp932  unsupported    eucjpms  unsupported     \n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "strings (varchar, text)"
        }, 
        {
            "location": "/bootstrapping/", 
            "text": "Using the maxwell-bootstrap utility\n\n\n\n\nYou can use the \nmaxwell-bootstrap\n utility to bootstrap tables from the command-line.\n\n\n\n\n\n\n\n\noption\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\n--log_level LOG_LEVEL\n\n\nlog level (DEBUG, INFO, WARN or ERROR)\n\n\n\n\n\n\n--user USER\n\n\nmysql username\n\n\n\n\n\n\n--password PASSWORD\n\n\nmysql password\n\n\n\n\n\n\n--host HOST\n\n\nmysql host\n\n\n\n\n\n\n--port PORT\n\n\nmysql port\n\n\n\n\n\n\n--database DATABASE\n\n\nmysql database containing the table to bootstrap\n\n\n\n\n\n\n--table TABLE\n\n\nmysql table to bootstrap\n\n\n\n\n\n\n\n\nUsing the maxwell.bootstrap table\n\n\n\n\nAlternatively you can insert a row in the \nmaxwell.bootstrap\n table to trigger a bootstrap.\n\n\nmysql\n insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable');\n\n\n\n\nAsync vs Sync bootstrapping\n\n\n\n\nThe Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time.\nWhen running Maxwell with \n--bootstrapper=sync\n, the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete.\nRunning Maxwell with \n--bootstrapper=async\n however, will make Maxwell spawn a separate thread for bootstrapping.\nIn this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process.\n\n\nBootstrapping Data Format\n\n\n\n\n\n\na bootstrap starts with a document with \ntype = \"bootstrap-start\"\n\n\nthen documents with \ntype = \"insert\"\n (one per row in the table)\n\n\nthen one document per \nINSERT\n, \nUPDATE\n or \nDELETE\n that occurred since the beginning of bootstrap\n\n\nfinally a document with \ntype = \"bootstrap-complete\"\n\n\n\n\nHere's a complete example:\n\n\nmysql\n create table fooDB.barTable(txt varchar(255));\nmysql\n insert into fooDB.barTable (txt) values (\nhello\n), (\nbootstrap!\n);\nmysql\n insert into maxwell.bootstrap (database_name, table_name) values (\nfooDB\n, \nbarTable\n);\n\n\n\n\nCorresponding replication stream output of table \nfooDB.barTable\n:\n\n\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\ninsert\n,\nts\n:1450557598,\nxid\n:13,\ndata\n:{\ntxt\n:\nhello\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\ninsert\n,\nts\n:1450557598,\nxid\n:13,\ndata\n:{\ntxt\n:\nbootstrap!\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-start\n,\nts\n:1450557744,\ndata\n:{}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-insert\n,\nts\n:1450557744,\ndata\n:{\ntxt\n:\nhello\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-insert\n,\nts\n:1450557744,\ndata\n:{\ntxt\n:\nbootstrap!\n}}\n{\ndatabase\n:\nfooDB\n,\ntable\n:\nbarTable\n,\ntype\n:\nbootstrap-complete\n,\nts\n:1450557744,\ndata\n:{}}\n\n\n\n\n\n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Bootstrapping"
        }, 
        {
            "location": "/bootstrapping/#using-the-maxwell-bootstrap-utility", 
            "text": "You can use the  maxwell-bootstrap  utility to bootstrap tables from the command-line.     option  description      --log_level LOG_LEVEL  log level (DEBUG, INFO, WARN or ERROR)    --user USER  mysql username    --password PASSWORD  mysql password    --host HOST  mysql host    --port PORT  mysql port    --database DATABASE  mysql database containing the table to bootstrap    --table TABLE  mysql table to bootstrap", 
            "title": "Using the maxwell-bootstrap utility"
        }, 
        {
            "location": "/bootstrapping/#using-the-maxwellbootstrap-table", 
            "text": "Alternatively you can insert a row in the  maxwell.bootstrap  table to trigger a bootstrap.  mysql  insert into maxwell.bootstrap (database_name, table_name) values ('fooDB', 'barTable');", 
            "title": "Using the maxwell.bootstrap table"
        }, 
        {
            "location": "/bootstrapping/#async-vs-sync-bootstrapping", 
            "text": "The Maxwell replicator is single threaded; events are captured by one thread from the binlog and replicated to Kafka one message at a time.\nWhen running Maxwell with  --bootstrapper=sync , the same thread is used to do bootstrapping, meaning that all binlog events are blocked until bootstrapping is complete.\nRunning Maxwell with  --bootstrapper=async  however, will make Maxwell spawn a separate thread for bootstrapping.\nIn this async mode, non-bootstrapped tables are replicated as normal by the main thread, while the binlog events for bootstrapped tables are queued and sent to the replication stream at the end of the bootstrap process.", 
            "title": "Async vs Sync bootstrapping"
        }, 
        {
            "location": "/bootstrapping/#bootstrapping-data-format", 
            "text": "a bootstrap starts with a document with  type = \"bootstrap-start\"  then documents with  type = \"insert\"  (one per row in the table)  then one document per  INSERT ,  UPDATE  or  DELETE  that occurred since the beginning of bootstrap  finally a document with  type = \"bootstrap-complete\"   Here's a complete example:  mysql  create table fooDB.barTable(txt varchar(255));\nmysql  insert into fooDB.barTable (txt) values ( hello ), ( bootstrap! );\nmysql  insert into maxwell.bootstrap (database_name, table_name) values ( fooDB ,  barTable );  Corresponding replication stream output of table  fooDB.barTable :  { database : fooDB , table : barTable , type : insert , ts :1450557598, xid :13, data :{ txt : hello }}\n{ database : fooDB , table : barTable , type : insert , ts :1450557598, xid :13, data :{ txt : bootstrap! }}\n{ database : fooDB , table : barTable , type : bootstrap-start , ts :1450557744, data :{}}\n{ database : fooDB , table : barTable , type : bootstrap-insert , ts :1450557744, data :{ txt : hello }}\n{ database : fooDB , table : barTable , type : bootstrap-insert , ts :1450557744, data :{ txt : bootstrap! }}\n{ database : fooDB , table : barTable , type : bootstrap-complete , ts :1450557744, data :{}}  \n  jQuery(document).ready(function () {\n    jQuery(\"table\").addClass(\"table table-condensed table-bordered table-hover\");\n  });", 
            "title": "Bootstrapping Data Format"
        }, 
        {
            "location": "/compat/", 
            "text": "Requirements:\n\n\n\n\n\n\nJRE 7 or above\n\n\nmysql 5.1, 5.5, 5.6\n\n\nkafka 0.8.2 or greater\n\n\n\n\nUnsupported configurations\n\n\n\n\n\n\nMysql 5.7 is untested with Maxwell.  GTID replication is known to not function.\n\n\n\n\nbinlog_row_image=MINIMAL\n\n\n\n\nAs of 0.16.2, Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want.  It will differ\nfrom normal Maxwell operation in that:\n\n\n\n\nINSERT statements will no longer output a column's default value\n\n\nUPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs,\n  but \ndata\n will only include what is needed to perform the update (generally, id columns and changed columns).\n  The \nold\n section may or may not be included, depending on the nature of the update.\n\n\nDELETE statements will be incomplete; generally they will only include the primary key.\n\n\n\n\nMaster recovery\n\n\n\n\nAs of 1.2.0, maxwell includes experimental support for master position recovery.  It works like this:\n\n\n\n\nmaxwell writes heartbeats into the binlogs (via the \npositions\n table)\n\n\nmaxwell reads its own heartbeats, using them as a secondary position guide\n\n\nif maxwell boots and can't find its position matching the \nserver_id\n it's\n  connecting to, it will look for a row in \nmaxwell.positions\n from a different\n  server_id.\n\n\nif it finds that row, it will scan backwards in the binary logs of the new\n  master until it finds that heartbeat.\n\n\n\n\nNotes:\n\n\n\n\nmaster recovery is not compatible with separate schema-store hosts and\n  replication-hosts, due to the heartbeat mechanism.\n\n\nthis code should be considered alpha-quality.\n\n\non highly active servers, as much as 1 second of data may be duplicated.", 
            "title": "Compat / Caveats"
        }, 
        {
            "location": "/compat/#requirements", 
            "text": "JRE 7 or above  mysql 5.1, 5.5, 5.6  kafka 0.8.2 or greater", 
            "title": "Requirements:"
        }, 
        {
            "location": "/compat/#unsupported-configurations", 
            "text": "Mysql 5.7 is untested with Maxwell.  GTID replication is known to not function.", 
            "title": "Unsupported configurations"
        }, 
        {
            "location": "/compat/#binlog_row_imageminimal", 
            "text": "As of 0.16.2, Maxwell supports binlog_row_image=MINIMAL, but it may not be what you want.  It will differ\nfrom normal Maxwell operation in that:   INSERT statements will no longer output a column's default value  UPDATE statements will be incomplete; Maxwell outputs as much of the row as given in the binlogs,\n  but  data  will only include what is needed to perform the update (generally, id columns and changed columns).\n  The  old  section may or may not be included, depending on the nature of the update.  DELETE statements will be incomplete; generally they will only include the primary key.", 
            "title": "binlog_row_image=MINIMAL"
        }, 
        {
            "location": "/compat/#master-recovery", 
            "text": "As of 1.2.0, maxwell includes experimental support for master position recovery.  It works like this:   maxwell writes heartbeats into the binlogs (via the  positions  table)  maxwell reads its own heartbeats, using them as a secondary position guide  if maxwell boots and can't find its position matching the  server_id  it's\n  connecting to, it will look for a row in  maxwell.positions  from a different\n  server_id.  if it finds that row, it will scan backwards in the binary logs of the new\n  master until it finds that heartbeat.   Notes:   master recovery is not compatible with separate schema-store hosts and\n  replication-hosts, due to the heartbeat mechanism.  this code should be considered alpha-quality.  on highly active servers, as much as 1 second of data may be duplicated.", 
            "title": "Master recovery"
        }, 
        {
            "location": "/changelog/", 
            "text": "Maxwell changelog\n\n\nv1.5.1\n: \"1.5.1 is just 1.5.1\"\n\n\nThis is a bugfix release.\n- fixes for bootstrapping with an alternative maxwell-schema name and an\n  \ninclude_database\n filter, thanks Lucian Jones\n- fixes for kafka 0.10 with lz4 compression, thanks Scott Ferguson\n- ignore the RDS table \nmysql.ha_health_check\n table\n- Get the bootstrapping process to output NULL values.\n- fix a quoting issue in the bootstrap code, thanks @mylesjao.\n\n\nv1.5.0\n: \"someone, somewhere, is still smoking cigarettes, damnit\"\n\n\n\n\nCHANGE: Kafka producer no longer ships with hard-coded defaults.\n  Please ensure you have \"compression.type\", \"metadata.fetch.timeout.ms\", and \"retries\"\n  configured to your liking.\n\n\nbugfix: fix a regression in handling \nALTER TABLE change c int after b\n statements\n\n\nwarn on servers with missing server_id\n\n\n\n\nv1.4.2\n: \"drawer cat is back\"\n\n\n\n\nkafka 0.10.0 support, as well as a re-working of the --kafka_version\n  command line option.\n\n\n\n\nv1.4.1\n: \"cat snores\"\n\n\n\n\nsupport per-table topics, Thanks @smferguson and @sschatts.\n\n\nfix a parser issue with DROP COLUMN CASCADE, thanks @smferguson\n\n\n\n\nv1.4.0\n: \"deep, insomniac character flaws\"\n\n\n1.4.0 brings us two nice new features:\n\n\n\n\npartition-by-column: see --kafka_partition_columns.  Thanks @smferguson\n\n\noutput schema changes as JSON: see --output_ddl.  Thanks @xmlking\n\n\nAs well as a fix around race conditions on shutdown.\n\n\n\n\nv1.3.0\n: \"yogg-saron\"\n\n\n\n\nsupport for fractional DATETIME, TIME, TIMESTAMP columns, thanks @Dagnan\n\n\nsupport for outputting server_id \n thread_id, thanks @sagiba\n\n\nfix a race condition in bootstrap support\n\n\n\n\nv1.2.2\n: \"bats wearing frog pajamas\"\n\n\n\n\nMaxwell will now include by default fields with NULL values (as null\nfields).  To disable this and restore the old functionality where fields\nwere omitted, pass \n--output_nulls=false\n\n\nFix an issue with multi-client support where two replicators would\nping-pong heartbeats at each other\n\n\nFix an issue where a client would attempt to recover a position from a\nmismatched client_id\n\n\nFix a bug when using CHANGE COLUMN on a primary key\n\n\n\n\nv1.2.1\n: \"point-ones are a sad and inevitable fact\"\n\n\nThis is a bugfix release.\n- fix a parser bug around ALTER TABLE CHARACTER SET\n- fix bin/maxwell to pull in the proper version of the kafka-clients\n  library\n\n\nv1.2.0\n: \"just here, not to talk to you\"\n\n\n1.2.0 is a major release of Maxwell that introduces master recovery\nfeatures; when a slave is promoted to master, Maxwell is now capable of\nrecovering the position.  See the \n--master_recovery\n flag for more\ndetails.\n\n\nIt also upgrades the kafka producer library to 0.9.  If you're using\nmaxwell with a kafka 0.8 server, you must now pass the \n--kafka0.8\n flag\nto maxwell.\n\n\nv1.1.6\n: \"pithy\"\n\n\n\n\nminor bugfix in which maxwell with --replay mode was trying to write\n  heartbeats\n\n\n\n\nv1.1.5\n: \"my brain is a polluted mess\"\n\n\n\n\n@dadah89 adds --output_binlog_position to optionally output the\n  position with the row\n\n\n@dadah89 adds --output_commit_info to turn off xid/commit fields\n\n\nmaxwell now supports tables with partitions\n\n\nmaxwell now supports N maxwells per-server.  see the client_id /\n  replica_server_id options.\n\n\ntwo parser fixes, for engine=\ninnodb\n and CHARSET ASCII\n\n\nlay the ground work for doing master recovery; we add a heartbeat into\n  the positions table that we can co-ordinate around.\n\n\n\n\nv1.1.4\n: \"george flunk\"\n\n\n\n\nadd support for a bunch more charsets (gbk, big5, notably)\n\n\nfix Maxwell's handling of kafka errors - previously we were trying to\n  crash Maxwell by throwing a RuntimeException out of the Kafka\n  Producer, but this was a failure.  Now we log and skip all errors.\n\n\n\n\nv1.1.3\n: \"the button I push to not have to go out\"\n\n\nThis is a bugfix release, which fixes:\n\n\n\n\nhttps://github.com/zendesk/maxwell/issues/376, a problem parsing\n  RENAME INDEX\n\n\nhttps://github.com/zendesk/maxwell/issues/371, a problem with the\n  SERIAL datatype\n\n\nhttps://github.com/zendesk/maxwell/issues/362, we now preserve the\n  original casing of columns\n\n\nhttps://github.com/zendesk/maxwell/issues/373, we were incorrectly\n  expecting heartbeats to work under 5.1\n\n\n\n\nv1.1.2\n: \"scribbled notes on red pages\"\n\n\n\n\npick up latest mysql-connector-j, fixes #369\n\n\nfix an issue where maxwell could skip ahead positions if a leader failed.\n\n\nrework buffering code to be much kinder to the GC and JVM heap in case\n  of very large transactions / rows inside transactions\n\n\nkinder, gentler help text when you specify an option incorrectly\n\n\n\n\nv1.1.1\n: scribbled notes on blue pages\n\n\n\n\nfixes a race condition setting the binlog position that would get\n  maxwell stuck\n\n\n\n\nv1.1.0\n: \"sleep away the afternoon\"\n\n\n\n\nmuch more efficient processing of schema updates storage, especially when dealing with large schemas.\n\n\n@lileeyao added --exclude-columns and the --jdbc_options features\n\n\n@lileeyao added --jdbc_options\n\n\ncan now blacklist entire databases\n\n\nnew kafka key format available, using a JSON array instead of an object\n\n\nbugfix: unsigned integer columns were captured incorrectly.  1.1 will\n  recapture the schema and attempt to correct the error.\n\n\n\n\nv1.1.0-pre4\n: \"buck buck buck buck buck buck-AH!\"\n\n\n\n\nEddie McLean gives some helpful patches around bootstrapping\n\n\nBugfixes for the patch-up-the-schema code around unsigned ints\n\n\n\n\nv1.1.0-pre3\n:\n\n\n\n\nforgot to include some updates that back-patch unsigned column\n  problems\n\n\n\n\nv1.1.0-pre2\n: \"yawn yawn\"\n\n\n\n\nfix performance issues when capturing schema in AWS Aurora\n\n\nfix a bug in capturing unsigned integer columns\n\n\n\n\nv1.0.1\n: \"bag of oversized daisies\"\n\n\n\n\nfixes a parsing bug with \nCURRENT_TIMESTAMP()\n\n\n\n\nv1.0.0\n: \"Maxwell learns to speak\"\n\n\nSince v0.17.0, Maxwell has gotten:\n- bootstrapping support\n- blacklisting for tables\n- flexible kafka partitioning\n- replication heartbeats\n- GEOMETRY columns\n- a whole lotta lotta bugfixes\n\n\nand I, Osheroff, think the damn thing is stable enough for a 1.0.  So\nthere.\n\n\nv1.0.0-RC3\n: \"C'mon and take it\"\n\n\npull in support for replication heartbeats.  helps in the flakier\nnetwork environs.\n\n\nv1.0.0-RC2\n: \"same thing, just without the v\"\n\n\n\n\nfixes the way ALTER DATABASE charset= was handled\n\n\nadds proper handling of ALTER TABLE CONVERT TO CHARSET\n\n\n\n\nv1.0.0-RC1\n: \"Richard Buckner's release\"\n\n\n\n\nmodifications to the way the bootstrap utility works\n\n\nfix a race condition crash bug in bootstrapping\n\n\nfix a parser bug\n\n\n\n\nv1.0.0-PRE2\n: \"an embarassment of riches\"\n\n\n1.0.0-PRE2 brings in a lot of changes that got merged while we were\ntesting out PRE1.  so, hey.\n\n\n\n\nConfigurable names for the \nmaxwell\n schema database (Kristian Kaufman)\n\n\nConfigurable key (primary key, id, database) into the kafka partition hash function (Kristian Kaufman)\n\n\nConfigurable Kafka partition hash function (java hashCode, murmur3) (Kristian Kaufman)\n\n\nsupport GEOMETRY columns, output as well-known-text\n\n\nadd \n--blacklist_tables\n option to fully ignore excessive schema changes (Nicolas Maquet)\n\n\nbootstrap rows now have 'bootstrap-insert' type\n\n\n\n\nv1.0.0-PRE1\n: \"drunk conversations with sober people\"\n\n\n\n\nHere we have the preview release of @nmaquet's excellent work around\n  bootstrapping initial versions of mysql tables.\n\n\n\n\nv0.17.0\n: \"wrists of William\"\n\n\nv0.17 is a large bugfix release with one new feature.\n\n\n\n\nFEATURE: allow specifying an alternative mysql schema-storage server and\n  replication server\n\n\nBUGFIX: properly handle case-sensitivity by aping the behavior of the\n  master server.  Fixes #230.\n\n\nBUGFIX: parse some forms of CHECK( ... ) statements.  Fixes #203.\n\n\nBUGFIX: many more SQL-parser fixes.  We are mostly through some\n  thousands of lines of SQL produced by mysql-test.\n\n\n\n\nv0.16.2\n: \"The best laid plans\"\n\n\nThis is a large-ish bugfix release.\n- Support, with reservations, binlog_row_image=MINIMAL\n- parser bug: handle table names that look like floating points\n- parser bug: fix for entity names that have '.', '\\', etc in them\n- handle UPPERCASE encoding names\n- support UCS2 (start trying to operate ok on the mysql-test suite)\n- use ObjectOutputStream.reset to fix memory leaks when buffering to disk\n\n\nv0.16.1\n: \"me and room service\"\n\n\nThis is a bug-fix-roundup release:\n- support ALTER DATABASE\n- fix a bunch of parse errors: we've started running mysql-test at\n  maxwell and are fixing up failures.\n- some modifications to the overflow-to-disk logic; we buffer the input\n  and output, and we fix a memory leak\n\n\nv0.16.0\n: \"Kristian Kaufmann's version\"\n\n\nVersion 0.16.0 introduces a feature where UPDATE statements will now\nshow both the new row image and the old values of the fields that\nchanged.  Thanks @kristiankaufmann\n\n\nv0.15.0\n: \"the littlest little city\"\n\n\n\n\nfix a parse problem with indices ordered by ASC/DESC\n\n\n\n\nv0.15.0-RC1\n: \"it's later than you think\"\n\n\n\n\nlarge transactions now buffer to disk instead of crushing maxwell.\n\n\nsupport ALGORITHM=[algo], LOCK=[lock] for 5.6 alters\n\n\n\n\nv0.14.6\n: \"It's about being American.  Sort of.\"\n\n\n\n\nfix TIME column support\n\n\nfix parsing on millisecond precision column defintions\n\n\nfix CREATE SCHEMA parsing\n\n\n\n\nv0.14.5\n: \"false is the new true\"\n\n\n\n\nhandle BOOLEAN columns with true/false defaults\n\n\n\n\nv0.14.4\n: \"You'd think we'd be at 1.0 by now, wouldn't you?\"\n\n\n\n\nfixes parsing of \"mysql comments\" (\n/*! .. */\n)\n\n\nMore performance improvements, another 10% in a tight loop.\n\n\n\n\nv0.14.3\n: \"Peanuts.  My girlfriend thinks about peanuts.\"\n\n\n\n\nfixes a regression in 0.14.2 that creates duplicate copies of the \"mysql\" database in the schema.\n\n\n\n\nv0.14.2\n: \"Maxwell Sandvik 88\"\n\n\n\n\ncapture the mysql database along with the rest of the schema.  Eliding it was a bad premature optimization that led to crashes when tables in the mysql database changed. \n\n\n\n\nv0.14.1\n: \"be liberal in what you accept.  Even if nonsensical.\"\n\n\n\n\nfixes a parser bug around named PRIMARY KEYs.\n\n\n\n\nv0.14.0\n: \"the slow but inevitable slide\"\n\n\nThis release introduces row filters, allowing you to include or exclude tables from maxwell's output based on names or regular expressions.  \n\n\nv0.13.1\n: \"well that was somewhat expected\"\n\n\nv0.13.1 is a bug fix of v0.13.0 -- fixes a bug where long rows were truncated. \n\n\nv0.13.0 contains:\n\n\n\n\nBig performance boost for maxwell: 75% faster in some benchmarks\n\n\n@davidsheldon contributed some nice bug fixes around \nCREATE TABLE ... IF NOT EXISTS\n, which were previously generating new, bogus copies of the schema.\n\n\nwe now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.\n\n\n\n\nv0.13.0\n: \"Malkovich Malkovich Malkovich Sheldon?\"\n\n\nLucky release number 13 brings some reasonably big changes:\n\n\n\n\nBig performance boost for maxwell: 75% faster in some benchmarks\n\n\n@davidsheldon contributed some nice bug fixes around \nCREATE TABLE ... IF NOT EXISTS\n, which were previously generating new, bogus copies of the schema.\n\n\nwe now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.\n\n\n\n\nThis release has a pretty bad bug.  do not use.\n\n\nv0.12.0\n: \"what do I call them?  Slippers?  Why, are you jealous?\"\n\n\n\n\nadd support for BIT columns.  \n\n\n\n\nv0.11.4\n: \"13 steps\"\n\n\nthis is another bugfix release that fixes a problem where the replication thread can die in the middle of processing a transaction event.  I really need to fix this at a lower level, ie the open-replicator level.\n\n\nv0.11.3\n: \".. and the other half is to take the bugs out\"\n\n\nthis is a bugfix release:\n- fix problems with table creation options inside alter statements ( \nALTER TABLE foo auto_increment=10\n )\n- fix a host of shutdown-procedure bugs\n\n\nthe test suite should also be way more reliable, not like you care.\n\n\nv0.11.2\n: \"savage acts of unprovoked violence are bad\"\n\n\nThis is a bugfix release.  It includes:\n- soft deletions of maxwell.schemas to fix A-\nB-\nA master swapping without creating intense replication delay\n- detect and fail early if we see \nbinlog_row_image=minimal\n\n- kill off maxwell if the position thread dies\n- fix a bug where maxwell could pick up a copy of schema from a different server_id (curse you operator precedence!)\n\n\nv0.11.1\n: \"dog snoring loudly\"\n\n\n\n\nmaxwell gets a very minimal pass at detecting when a master has changed, in which it will kill off schemas and positions from a server_id that no longer is valid.  this should prevent the worst of cases.\n\n\n\n\nv0.11.0\n: \"cat waving gently\"\n\n\nThis release of Maxwell preserves transaction information in the kafka stream by adding a \nxid\n key in the JSON object, as well as a \ncommit\n key for the final row inside the transaction.\n\n\nIt also contains a bugfix around server_id handling.\n\n\nv0.10.1\n: \"all 64 of your bases belong to... shut up, internet parrot.\"\n\n\n\n\nproper support for BLOB, BINARY, VARBINARY columns (base 64 encoded)\n\n\nfix a problem with the SQL parser where specifying encoding or collation in a string column in the wrong order would crash\n\n\nmake table option parsing more lenient\n\n\n\n\nv0.11.0-RC1\n: \"goin' faster than a rollercoaster\"\n\n\n\n\nmerge master fixes\n\n\n\n\nv0.10.0\n: \"The first word is French\"\n\n\n\n\nMysql 5.6 checksum support!\n\n\nsome more bugfixes with the SQL parser \n\n\n\n\nv0.11.0-PRE4\n: \"except for that other thing\"\n\n\n\n\nbugfix on v0.11.0-PRE3\n\n\n\n\nv0.11.0-PRE3\n: \"nothing like a good night's sleep\"\n\n\n\n\nhandle SAVEPOINT within transactions\n\n\ndowngrade unhandled SQL to a warning\n\n\n\n\nv0.11.0-PRE2\n: \"you really need to name a \nPRE\n release something cutesy?\"\n\n\n\n\nfixes for myISAM \"transactions\"\n\n\n\n\nv0.11.0-PRE1\n: \"A slow traffic jam towards the void\"\n\n\n\n\nfix a server_id bug (was always 1 in maxwell.schemas)\n\n\nJSON output now includes transaction IDs\n\n\n\n\nv0.10.0-RC4\n: \"Inspiring confidence\"\n\n\n\n\ndeal with BINARY flag in string column creation.\n\n\n\n\nv0.9.5\n: \"Long story short, that's why I'm late\"\n\n\n\n\nhandle the BINARY flag in column creation\n\n\n\n\nv0.10.0-RC3\n: \"Except for that one thing\"\n\n\n\n\nhandle \"TRUNCATE [TABLE_NAME]\" statements\n\n\n\n\nv0.10.0-RC2\n: \"RC2 is always a good sign.\"\n\n\n\n\nfixes a bug with checksum processing.\n\n\n\n\nv0.10.0-RC1\n: \"verify all the things\"\n\n\n\n\nupgrade to open-replicator 1.3.0-RC1, which brings binlog checksum (and thus easy 5.6.1) support to maxwell.\n\n\n\n\nv0.9.4\n: \"we've been here before\"\n\n\n\n\nallow a configurable number (including unlimited) of schemas to be stored\n\n\n\n\nv0.9.3\n: \"some days it's just better to stay in bed\"\n\n\n\n\nbump open-replicator to 1.2.3, which allows processing of single rows greater than 2^24 bytes\n\n\n\n\nv0.9.2\n: \"Cat's tongue\"\n\n\n\n\nbump open-replicator buffer to 50mb by default\n\n\nlog to STDERR, not STDOUT \n\n\n--output_file\n option for file producer\n\n\n\n\nv0.9.1\n: \"bugs, bugs, bugs, lies, statistics\"\n\n\n\n\nMaxwell is now aware that column names are case-insenstive\n\n\nfix a nasty bug in which maxwell would store the wrong position after it lost its connection to the master.\n\n\n\n\nv0.9.0\n: Vanchi says \"eat\"\n\n\nAlso, vanchi is so paranoid he's worried immediately about this. \n\n\n\n\nmysql 5.6 support (without checksum support, yet)\n\n\nfix a bunch of miscellaneous bugs @akshayi1 found (REAL, BOOL, BOOLEAN types, TRUNCATE TABLE)\n\n\n\n\nv0.8.1\n: \"Pascal says Bonjour\"\n\n\n\n\nminor bugfix release around mysql connections going away.\n\n\n\n\nv0.8.0\n: the cat never shuts up\n\n\n\n\nadd \"ts\" field to row output\n\n\nadd --config option for passing a different config file\n\n\nsupport int1, int2, int4, int8 columns\n\n\n\n\nv0.7.2\n: \"all the sql ladies\"\n\n\n\n\nhandle inline sql comments\n\n\nignore more user management SQL\n\n\n\n\nv0.7.1\n: \"not hoarders\"\n\n\n\n\nonly keep 5 most recent schemas\n\n\n\n\nv0.7.0\n: 0.7.0, \"alameda\"\n\n\n\n\nhandle CURRENT_TIMESTAMP parsing properly\n\n\nbetter binlog position sync behavior\n\n\n\n\nv0.6.3\n: 0.6.3\n\n\n\n\nbetter blacklist for CREATE TRIGGER\n\n\n\n\nv0.6.2\n: v0.6.2\n\n\n\n\nmaxwell now ignores SAVEPOINT statements.\n\n\n\n\nv0.6.1\n: v0.6.1\n\n\n\n\nfixes a bug with parsing length-limited indexes.\n\n\n\n\nv0.6.0\n: kafkakafkakafa\n\n\nVersion 0.6.0 has Maxwell outputting a JSON kafka key, so that one can use Kafka's neat \"store the last copy of a key\" retention policy.  It also fixes a couple of bugs in the query parsing path.\n\n\nv0.5.0\n: 0.5.0 -- \"People who put commas in column names deserve undefined behavior\"\n\n\n\n\nmaxwell now captures primary keys on tables.  We'll use this to form kafka key names later.\n\n\nmaxwell now outputs to a single topic, hashing the data by database name to keep a database's updates in order.\n\n\n\n\nv0.4.0\n: 0.4.0, \"unboxed cat\"\n\n\nv0.4.0 fixes some bugs with long-lived mysql connections by adding connection pooling support.\n\n\nv0.3.0\n: 0.3.0\n\n\nThis version fixes a fairly nasty bug in which the binlog-position flush thread was sharing a connection with the rest of the system, leading to crashes. \n\n\nIt also enables kafka gzip compression by default.\n\n\nv0.2.2\n: 0.2.2\n\n\nVersion 0.2.2 sets up the LANG environment variable, which fixes a bug in utf-8 handling. \n\n\nv0.2.1\n: v0.2.1\n\n\nversion 0.2.1 makes Maxwell ignore CREATE INDEX ddl statements and others.\n\n\nv0.2.0\n: 0.2.0\n\n\nThis release gets Maxwell storing the last-written binlog position inside the mysql master itself. \n\n\nv0.1.4\n: 0.1.4\n\n\nsupport --position_file param\n\n\nv0.1.3\n: 0.1.3\n\n\nAdds kafka command line options.\n\n\nv0.1.1\n: 0.1.1\n\n\nv0.1.1, a small bugfix release. \n\n\nv0.1\n: 0.1\n\n\nThis is the first possible release of Maxwell that might work.  It includes some exceedingly basic kafka support, and JSON output of binlog deltas.", 
            "title": "Changelog"
        }, 
        {
            "location": "/changelog/#maxwell-changelog", 
            "text": "", 
            "title": "Maxwell changelog"
        }, 
        {
            "location": "/changelog/#v151-151-is-just-151", 
            "text": "This is a bugfix release.\n- fixes for bootstrapping with an alternative maxwell-schema name and an\n   include_database  filter, thanks Lucian Jones\n- fixes for kafka 0.10 with lz4 compression, thanks Scott Ferguson\n- ignore the RDS table  mysql.ha_health_check  table\n- Get the bootstrapping process to output NULL values.\n- fix a quoting issue in the bootstrap code, thanks @mylesjao.", 
            "title": "v1.5.1: \"1.5.1 is just 1.5.1\""
        }, 
        {
            "location": "/changelog/#v150-someone-somewhere-is-still-smoking-cigarettes-damnit", 
            "text": "CHANGE: Kafka producer no longer ships with hard-coded defaults.\n  Please ensure you have \"compression.type\", \"metadata.fetch.timeout.ms\", and \"retries\"\n  configured to your liking.  bugfix: fix a regression in handling  ALTER TABLE change c int after b  statements  warn on servers with missing server_id", 
            "title": "v1.5.0: \"someone, somewhere, is still smoking cigarettes, damnit\""
        }, 
        {
            "location": "/changelog/#v142-drawer-cat-is-back", 
            "text": "kafka 0.10.0 support, as well as a re-working of the --kafka_version\n  command line option.", 
            "title": "v1.4.2: \"drawer cat is back\""
        }, 
        {
            "location": "/changelog/#v141-cat-snores", 
            "text": "support per-table topics, Thanks @smferguson and @sschatts.  fix a parser issue with DROP COLUMN CASCADE, thanks @smferguson", 
            "title": "v1.4.1: \"cat snores\""
        }, 
        {
            "location": "/changelog/#v140-deep-insomniac-character-flaws", 
            "text": "1.4.0 brings us two nice new features:   partition-by-column: see --kafka_partition_columns.  Thanks @smferguson  output schema changes as JSON: see --output_ddl.  Thanks @xmlking  As well as a fix around race conditions on shutdown.", 
            "title": "v1.4.0: \"deep, insomniac character flaws\""
        }, 
        {
            "location": "/changelog/#v130-yogg-saron", 
            "text": "support for fractional DATETIME, TIME, TIMESTAMP columns, thanks @Dagnan  support for outputting server_id   thread_id, thanks @sagiba  fix a race condition in bootstrap support", 
            "title": "v1.3.0: \"yogg-saron\""
        }, 
        {
            "location": "/changelog/#v122-bats-wearing-frog-pajamas", 
            "text": "Maxwell will now include by default fields with NULL values (as null\nfields).  To disable this and restore the old functionality where fields\nwere omitted, pass  --output_nulls=false  Fix an issue with multi-client support where two replicators would\nping-pong heartbeats at each other  Fix an issue where a client would attempt to recover a position from a\nmismatched client_id  Fix a bug when using CHANGE COLUMN on a primary key", 
            "title": "v1.2.2: \"bats wearing frog pajamas\""
        }, 
        {
            "location": "/changelog/#v121-point-ones-are-a-sad-and-inevitable-fact", 
            "text": "This is a bugfix release.\n- fix a parser bug around ALTER TABLE CHARACTER SET\n- fix bin/maxwell to pull in the proper version of the kafka-clients\n  library", 
            "title": "v1.2.1: \"point-ones are a sad and inevitable fact\""
        }, 
        {
            "location": "/changelog/#v120-just-here-not-to-talk-to-you", 
            "text": "1.2.0 is a major release of Maxwell that introduces master recovery\nfeatures; when a slave is promoted to master, Maxwell is now capable of\nrecovering the position.  See the  --master_recovery  flag for more\ndetails.  It also upgrades the kafka producer library to 0.9.  If you're using\nmaxwell with a kafka 0.8 server, you must now pass the  --kafka0.8  flag\nto maxwell.", 
            "title": "v1.2.0: \"just here, not to talk to you\""
        }, 
        {
            "location": "/changelog/#v116-pithy", 
            "text": "minor bugfix in which maxwell with --replay mode was trying to write\n  heartbeats", 
            "title": "v1.1.6: \"pithy\""
        }, 
        {
            "location": "/changelog/#v115-my-brain-is-a-polluted-mess", 
            "text": "@dadah89 adds --output_binlog_position to optionally output the\n  position with the row  @dadah89 adds --output_commit_info to turn off xid/commit fields  maxwell now supports tables with partitions  maxwell now supports N maxwells per-server.  see the client_id /\n  replica_server_id options.  two parser fixes, for engine= innodb  and CHARSET ASCII  lay the ground work for doing master recovery; we add a heartbeat into\n  the positions table that we can co-ordinate around.", 
            "title": "v1.1.5: \"my brain is a polluted mess\""
        }, 
        {
            "location": "/changelog/#v114-george-flunk", 
            "text": "add support for a bunch more charsets (gbk, big5, notably)  fix Maxwell's handling of kafka errors - previously we were trying to\n  crash Maxwell by throwing a RuntimeException out of the Kafka\n  Producer, but this was a failure.  Now we log and skip all errors.", 
            "title": "v1.1.4: \"george flunk\""
        }, 
        {
            "location": "/changelog/#v113-the-button-i-push-to-not-have-to-go-out", 
            "text": "This is a bugfix release, which fixes:   https://github.com/zendesk/maxwell/issues/376, a problem parsing\n  RENAME INDEX  https://github.com/zendesk/maxwell/issues/371, a problem with the\n  SERIAL datatype  https://github.com/zendesk/maxwell/issues/362, we now preserve the\n  original casing of columns  https://github.com/zendesk/maxwell/issues/373, we were incorrectly\n  expecting heartbeats to work under 5.1", 
            "title": "v1.1.3: \"the button I push to not have to go out\""
        }, 
        {
            "location": "/changelog/#v112-scribbled-notes-on-red-pages", 
            "text": "pick up latest mysql-connector-j, fixes #369  fix an issue where maxwell could skip ahead positions if a leader failed.  rework buffering code to be much kinder to the GC and JVM heap in case\n  of very large transactions / rows inside transactions  kinder, gentler help text when you specify an option incorrectly", 
            "title": "v1.1.2: \"scribbled notes on red pages\""
        }, 
        {
            "location": "/changelog/#v111-scribbled-notes-on-blue-pages", 
            "text": "fixes a race condition setting the binlog position that would get\n  maxwell stuck", 
            "title": "v1.1.1: scribbled notes on blue pages"
        }, 
        {
            "location": "/changelog/#v110-sleep-away-the-afternoon", 
            "text": "much more efficient processing of schema updates storage, especially when dealing with large schemas.  @lileeyao added --exclude-columns and the --jdbc_options features  @lileeyao added --jdbc_options  can now blacklist entire databases  new kafka key format available, using a JSON array instead of an object  bugfix: unsigned integer columns were captured incorrectly.  1.1 will\n  recapture the schema and attempt to correct the error.", 
            "title": "v1.1.0: \"sleep away the afternoon\""
        }, 
        {
            "location": "/changelog/#v110-pre4-buck-buck-buck-buck-buck-buck-ah", 
            "text": "Eddie McLean gives some helpful patches around bootstrapping  Bugfixes for the patch-up-the-schema code around unsigned ints", 
            "title": "v1.1.0-pre4: \"buck buck buck buck buck buck-AH!\""
        }, 
        {
            "location": "/changelog/#v110-pre3", 
            "text": "forgot to include some updates that back-patch unsigned column\n  problems", 
            "title": "v1.1.0-pre3:"
        }, 
        {
            "location": "/changelog/#v110-pre2-yawn-yawn", 
            "text": "fix performance issues when capturing schema in AWS Aurora  fix a bug in capturing unsigned integer columns", 
            "title": "v1.1.0-pre2: \"yawn yawn\""
        }, 
        {
            "location": "/changelog/#v101-bag-of-oversized-daisies", 
            "text": "fixes a parsing bug with  CURRENT_TIMESTAMP()", 
            "title": "v1.0.1: \"bag of oversized daisies\""
        }, 
        {
            "location": "/changelog/#v100-maxwell-learns-to-speak", 
            "text": "Since v0.17.0, Maxwell has gotten:\n- bootstrapping support\n- blacklisting for tables\n- flexible kafka partitioning\n- replication heartbeats\n- GEOMETRY columns\n- a whole lotta lotta bugfixes  and I, Osheroff, think the damn thing is stable enough for a 1.0.  So\nthere.", 
            "title": "v1.0.0: \"Maxwell learns to speak\""
        }, 
        {
            "location": "/changelog/#v100-rc3-cmon-and-take-it", 
            "text": "pull in support for replication heartbeats.  helps in the flakier\nnetwork environs.", 
            "title": "v1.0.0-RC3: \"C'mon and take it\""
        }, 
        {
            "location": "/changelog/#v100-rc2-same-thing-just-without-the-v", 
            "text": "fixes the way ALTER DATABASE charset= was handled  adds proper handling of ALTER TABLE CONVERT TO CHARSET", 
            "title": "v1.0.0-RC2: \"same thing, just without the v\""
        }, 
        {
            "location": "/changelog/#v100-rc1-richard-buckners-release", 
            "text": "modifications to the way the bootstrap utility works  fix a race condition crash bug in bootstrapping  fix a parser bug", 
            "title": "v1.0.0-RC1: \"Richard Buckner's release\""
        }, 
        {
            "location": "/changelog/#v100-pre2-an-embarassment-of-riches", 
            "text": "1.0.0-PRE2 brings in a lot of changes that got merged while we were\ntesting out PRE1.  so, hey.   Configurable names for the  maxwell  schema database (Kristian Kaufman)  Configurable key (primary key, id, database) into the kafka partition hash function (Kristian Kaufman)  Configurable Kafka partition hash function (java hashCode, murmur3) (Kristian Kaufman)  support GEOMETRY columns, output as well-known-text  add  --blacklist_tables  option to fully ignore excessive schema changes (Nicolas Maquet)  bootstrap rows now have 'bootstrap-insert' type", 
            "title": "v1.0.0-PRE2: \"an embarassment of riches\""
        }, 
        {
            "location": "/changelog/#v100-pre1-drunk-conversations-with-sober-people", 
            "text": "Here we have the preview release of @nmaquet's excellent work around\n  bootstrapping initial versions of mysql tables.", 
            "title": "v1.0.0-PRE1: \"drunk conversations with sober people\""
        }, 
        {
            "location": "/changelog/#v0170-wrists-of-william", 
            "text": "v0.17 is a large bugfix release with one new feature.   FEATURE: allow specifying an alternative mysql schema-storage server and\n  replication server  BUGFIX: properly handle case-sensitivity by aping the behavior of the\n  master server.  Fixes #230.  BUGFIX: parse some forms of CHECK( ... ) statements.  Fixes #203.  BUGFIX: many more SQL-parser fixes.  We are mostly through some\n  thousands of lines of SQL produced by mysql-test.", 
            "title": "v0.17.0: \"wrists of William\""
        }, 
        {
            "location": "/changelog/#v0162-the-best-laid-plans", 
            "text": "This is a large-ish bugfix release.\n- Support, with reservations, binlog_row_image=MINIMAL\n- parser bug: handle table names that look like floating points\n- parser bug: fix for entity names that have '.', '\\', etc in them\n- handle UPPERCASE encoding names\n- support UCS2 (start trying to operate ok on the mysql-test suite)\n- use ObjectOutputStream.reset to fix memory leaks when buffering to disk", 
            "title": "v0.16.2: \"The best laid plans\""
        }, 
        {
            "location": "/changelog/#v0161-me-and-room-service", 
            "text": "This is a bug-fix-roundup release:\n- support ALTER DATABASE\n- fix a bunch of parse errors: we've started running mysql-test at\n  maxwell and are fixing up failures.\n- some modifications to the overflow-to-disk logic; we buffer the input\n  and output, and we fix a memory leak", 
            "title": "v0.16.1: \"me and room service\""
        }, 
        {
            "location": "/changelog/#v0160-kristian-kaufmanns-version", 
            "text": "Version 0.16.0 introduces a feature where UPDATE statements will now\nshow both the new row image and the old values of the fields that\nchanged.  Thanks @kristiankaufmann", 
            "title": "v0.16.0: \"Kristian Kaufmann's version\""
        }, 
        {
            "location": "/changelog/#v0150-the-littlest-little-city", 
            "text": "fix a parse problem with indices ordered by ASC/DESC", 
            "title": "v0.15.0: \"the littlest little city\""
        }, 
        {
            "location": "/changelog/#v0150-rc1-its-later-than-you-think", 
            "text": "large transactions now buffer to disk instead of crushing maxwell.  support ALGORITHM=[algo], LOCK=[lock] for 5.6 alters", 
            "title": "v0.15.0-RC1: \"it's later than you think\""
        }, 
        {
            "location": "/changelog/#v0146-its-about-being-american-sort-of", 
            "text": "fix TIME column support  fix parsing on millisecond precision column defintions  fix CREATE SCHEMA parsing", 
            "title": "v0.14.6: \"It's about being American.  Sort of.\""
        }, 
        {
            "location": "/changelog/#v0145-false-is-the-new-true", 
            "text": "handle BOOLEAN columns with true/false defaults", 
            "title": "v0.14.5: \"false is the new true\""
        }, 
        {
            "location": "/changelog/#v0144-youd-think-wed-be-at-10-by-now-wouldnt-you", 
            "text": "fixes parsing of \"mysql comments\" ( /*! .. */ )  More performance improvements, another 10% in a tight loop.", 
            "title": "v0.14.4: \"You'd think we'd be at 1.0 by now, wouldn't you?\""
        }, 
        {
            "location": "/changelog/#v0143-peanuts-my-girlfriend-thinks-about-peanuts", 
            "text": "fixes a regression in 0.14.2 that creates duplicate copies of the \"mysql\" database in the schema.", 
            "title": "v0.14.3: \"Peanuts.  My girlfriend thinks about peanuts.\""
        }, 
        {
            "location": "/changelog/#v0142-maxwell-sandvik-88", 
            "text": "capture the mysql database along with the rest of the schema.  Eliding it was a bad premature optimization that led to crashes when tables in the mysql database changed.", 
            "title": "v0.14.2: \"Maxwell Sandvik 88\""
        }, 
        {
            "location": "/changelog/#v0141-be-liberal-in-what-you-accept-even-if-nonsensical", 
            "text": "fixes a parser bug around named PRIMARY KEYs.", 
            "title": "v0.14.1: \"be liberal in what you accept.  Even if nonsensical.\""
        }, 
        {
            "location": "/changelog/#v0140-the-slow-but-inevitable-slide", 
            "text": "This release introduces row filters, allowing you to include or exclude tables from maxwell's output based on names or regular expressions.", 
            "title": "v0.14.0: \"the slow but inevitable slide\""
        }, 
        {
            "location": "/changelog/#v0131-well-that-was-somewhat-expected", 
            "text": "v0.13.1 is a bug fix of v0.13.0 -- fixes a bug where long rows were truncated.   v0.13.0 contains:   Big performance boost for maxwell: 75% faster in some benchmarks  @davidsheldon contributed some nice bug fixes around  CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema.  we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.", 
            "title": "v0.13.1: \"well that was somewhat expected\""
        }, 
        {
            "location": "/changelog/#v0130-malkovich-malkovich-malkovich-sheldon", 
            "text": "Lucky release number 13 brings some reasonably big changes:   Big performance boost for maxwell: 75% faster in some benchmarks  @davidsheldon contributed some nice bug fixes around  CREATE TABLE ... IF NOT EXISTS , which were previously generating new, bogus copies of the schema.  we now include a \"scavenger thread\" that will lazily clean out old, deleted schemas.   This release has a pretty bad bug.  do not use.", 
            "title": "v0.13.0: \"Malkovich Malkovich Malkovich Sheldon?\""
        }, 
        {
            "location": "/changelog/#v0120-what-do-i-call-them-slippers-why-are-you-jealous", 
            "text": "add support for BIT columns.", 
            "title": "v0.12.0: \"what do I call them?  Slippers?  Why, are you jealous?\""
        }, 
        {
            "location": "/changelog/#v0114-13-steps", 
            "text": "this is another bugfix release that fixes a problem where the replication thread can die in the middle of processing a transaction event.  I really need to fix this at a lower level, ie the open-replicator level.", 
            "title": "v0.11.4: \"13 steps\""
        }, 
        {
            "location": "/changelog/#v0113-and-the-other-half-is-to-take-the-bugs-out", 
            "text": "this is a bugfix release:\n- fix problems with table creation options inside alter statements (  ALTER TABLE foo auto_increment=10  )\n- fix a host of shutdown-procedure bugs  the test suite should also be way more reliable, not like you care.", 
            "title": "v0.11.3: \".. and the other half is to take the bugs out\""
        }, 
        {
            "location": "/changelog/#v0112-savage-acts-of-unprovoked-violence-are-bad", 
            "text": "This is a bugfix release.  It includes:\n- soft deletions of maxwell.schemas to fix A- B- A master swapping without creating intense replication delay\n- detect and fail early if we see  binlog_row_image=minimal \n- kill off maxwell if the position thread dies\n- fix a bug where maxwell could pick up a copy of schema from a different server_id (curse you operator precedence!)", 
            "title": "v0.11.2: \"savage acts of unprovoked violence are bad\""
        }, 
        {
            "location": "/changelog/#v0111-dog-snoring-loudly", 
            "text": "maxwell gets a very minimal pass at detecting when a master has changed, in which it will kill off schemas and positions from a server_id that no longer is valid.  this should prevent the worst of cases.", 
            "title": "v0.11.1: \"dog snoring loudly\""
        }, 
        {
            "location": "/changelog/#v0110-cat-waving-gently", 
            "text": "This release of Maxwell preserves transaction information in the kafka stream by adding a  xid  key in the JSON object, as well as a  commit  key for the final row inside the transaction.  It also contains a bugfix around server_id handling.", 
            "title": "v0.11.0: \"cat waving gently\""
        }, 
        {
            "location": "/changelog/#v0101-all-64-of-your-bases-belong-to-shut-up-internet-parrot", 
            "text": "proper support for BLOB, BINARY, VARBINARY columns (base 64 encoded)  fix a problem with the SQL parser where specifying encoding or collation in a string column in the wrong order would crash  make table option parsing more lenient", 
            "title": "v0.10.1: \"all 64 of your bases belong to... shut up, internet parrot.\""
        }, 
        {
            "location": "/changelog/#v0110-rc1-goin-faster-than-a-rollercoaster", 
            "text": "merge master fixes", 
            "title": "v0.11.0-RC1: \"goin' faster than a rollercoaster\""
        }, 
        {
            "location": "/changelog/#v0100-the-first-word-is-french", 
            "text": "Mysql 5.6 checksum support!  some more bugfixes with the SQL parser", 
            "title": "v0.10.0: \"The first word is French\""
        }, 
        {
            "location": "/changelog/#v0110-pre4-except-for-that-other-thing", 
            "text": "bugfix on v0.11.0-PRE3", 
            "title": "v0.11.0-PRE4: \"except for that other thing\""
        }, 
        {
            "location": "/changelog/#v0110-pre3-nothing-like-a-good-nights-sleep", 
            "text": "handle SAVEPOINT within transactions  downgrade unhandled SQL to a warning", 
            "title": "v0.11.0-PRE3: \"nothing like a good night's sleep\""
        }, 
        {
            "location": "/changelog/#v0110-pre2-you-really-need-to-name-a-pre-release-something-cutesy", 
            "text": "fixes for myISAM \"transactions\"", 
            "title": "v0.11.0-PRE2: \"you really need to name a PRE release something cutesy?\""
        }, 
        {
            "location": "/changelog/#v0110-pre1-a-slow-traffic-jam-towards-the-void", 
            "text": "fix a server_id bug (was always 1 in maxwell.schemas)  JSON output now includes transaction IDs", 
            "title": "v0.11.0-PRE1: \"A slow traffic jam towards the void\""
        }, 
        {
            "location": "/changelog/#v0100-rc4-inspiring-confidence", 
            "text": "deal with BINARY flag in string column creation.", 
            "title": "v0.10.0-RC4: \"Inspiring confidence\""
        }, 
        {
            "location": "/changelog/#v095-long-story-short-thats-why-im-late", 
            "text": "handle the BINARY flag in column creation", 
            "title": "v0.9.5: \"Long story short, that's why I'm late\""
        }, 
        {
            "location": "/changelog/#v0100-rc3-except-for-that-one-thing", 
            "text": "handle \"TRUNCATE [TABLE_NAME]\" statements", 
            "title": "v0.10.0-RC3: \"Except for that one thing\""
        }, 
        {
            "location": "/changelog/#v0100-rc2-rc2-is-always-a-good-sign", 
            "text": "fixes a bug with checksum processing.", 
            "title": "v0.10.0-RC2: \"RC2 is always a good sign.\""
        }, 
        {
            "location": "/changelog/#v0100-rc1-verify-all-the-things", 
            "text": "upgrade to open-replicator 1.3.0-RC1, which brings binlog checksum (and thus easy 5.6.1) support to maxwell.", 
            "title": "v0.10.0-RC1: \"verify all the things\""
        }, 
        {
            "location": "/changelog/#v094-weve-been-here-before", 
            "text": "allow a configurable number (including unlimited) of schemas to be stored", 
            "title": "v0.9.4: \"we've been here before\""
        }, 
        {
            "location": "/changelog/#v093-some-days-its-just-better-to-stay-in-bed", 
            "text": "bump open-replicator to 1.2.3, which allows processing of single rows greater than 2^24 bytes", 
            "title": "v0.9.3: \"some days it's just better to stay in bed\""
        }, 
        {
            "location": "/changelog/#v092-cats-tongue", 
            "text": "bump open-replicator buffer to 50mb by default  log to STDERR, not STDOUT   --output_file  option for file producer", 
            "title": "v0.9.2: \"Cat's tongue\""
        }, 
        {
            "location": "/changelog/#v091-bugs-bugs-bugs-lies-statistics", 
            "text": "Maxwell is now aware that column names are case-insenstive  fix a nasty bug in which maxwell would store the wrong position after it lost its connection to the master.", 
            "title": "v0.9.1: \"bugs, bugs, bugs, lies, statistics\""
        }, 
        {
            "location": "/changelog/#v090-vanchi-says-eat", 
            "text": "Also, vanchi is so paranoid he's worried immediately about this.    mysql 5.6 support (without checksum support, yet)  fix a bunch of miscellaneous bugs @akshayi1 found (REAL, BOOL, BOOLEAN types, TRUNCATE TABLE)", 
            "title": "v0.9.0: Vanchi says \"eat\""
        }, 
        {
            "location": "/changelog/#v081-pascal-says-bonjour", 
            "text": "minor bugfix release around mysql connections going away.", 
            "title": "v0.8.1: \"Pascal says Bonjour\""
        }, 
        {
            "location": "/changelog/#v080-the-cat-never-shuts-up", 
            "text": "add \"ts\" field to row output  add --config option for passing a different config file  support int1, int2, int4, int8 columns", 
            "title": "v0.8.0: the cat never shuts up"
        }, 
        {
            "location": "/changelog/#v072-all-the-sql-ladies", 
            "text": "handle inline sql comments  ignore more user management SQL", 
            "title": "v0.7.2: \"all the sql ladies\""
        }, 
        {
            "location": "/changelog/#v071-not-hoarders", 
            "text": "only keep 5 most recent schemas", 
            "title": "v0.7.1: \"not hoarders\""
        }, 
        {
            "location": "/changelog/#v070-070-alameda", 
            "text": "handle CURRENT_TIMESTAMP parsing properly  better binlog position sync behavior", 
            "title": "v0.7.0: 0.7.0, \"alameda\""
        }, 
        {
            "location": "/changelog/#v063-063", 
            "text": "better blacklist for CREATE TRIGGER", 
            "title": "v0.6.3: 0.6.3"
        }, 
        {
            "location": "/changelog/#v062-v062", 
            "text": "maxwell now ignores SAVEPOINT statements.", 
            "title": "v0.6.2: v0.6.2"
        }, 
        {
            "location": "/changelog/#v061-v061", 
            "text": "fixes a bug with parsing length-limited indexes.", 
            "title": "v0.6.1: v0.6.1"
        }, 
        {
            "location": "/changelog/#v060-kafkakafkakafa", 
            "text": "Version 0.6.0 has Maxwell outputting a JSON kafka key, so that one can use Kafka's neat \"store the last copy of a key\" retention policy.  It also fixes a couple of bugs in the query parsing path.", 
            "title": "v0.6.0: kafkakafkakafa"
        }, 
        {
            "location": "/changelog/#v050-050-people-who-put-commas-in-column-names-deserve-undefined-behavior", 
            "text": "maxwell now captures primary keys on tables.  We'll use this to form kafka key names later.  maxwell now outputs to a single topic, hashing the data by database name to keep a database's updates in order.", 
            "title": "v0.5.0: 0.5.0 -- \"People who put commas in column names deserve undefined behavior\""
        }, 
        {
            "location": "/changelog/#v040-040-unboxed-cat", 
            "text": "v0.4.0 fixes some bugs with long-lived mysql connections by adding connection pooling support.", 
            "title": "v0.4.0: 0.4.0, \"unboxed cat\""
        }, 
        {
            "location": "/changelog/#v030-030", 
            "text": "This version fixes a fairly nasty bug in which the binlog-position flush thread was sharing a connection with the rest of the system, leading to crashes.   It also enables kafka gzip compression by default.", 
            "title": "v0.3.0: 0.3.0"
        }, 
        {
            "location": "/changelog/#v022-022", 
            "text": "Version 0.2.2 sets up the LANG environment variable, which fixes a bug in utf-8 handling.", 
            "title": "v0.2.2: 0.2.2"
        }, 
        {
            "location": "/changelog/#v021-v021", 
            "text": "version 0.2.1 makes Maxwell ignore CREATE INDEX ddl statements and others.", 
            "title": "v0.2.1: v0.2.1"
        }, 
        {
            "location": "/changelog/#v020-020", 
            "text": "This release gets Maxwell storing the last-written binlog position inside the mysql master itself.", 
            "title": "v0.2.0: 0.2.0"
        }, 
        {
            "location": "/changelog/#v014-014", 
            "text": "support --position_file param", 
            "title": "v0.1.4: 0.1.4"
        }, 
        {
            "location": "/changelog/#v013-013", 
            "text": "Adds kafka command line options.", 
            "title": "v0.1.3: 0.1.3"
        }, 
        {
            "location": "/changelog/#v011-011", 
            "text": "v0.1.1, a small bugfix release.", 
            "title": "v0.1.1: 0.1.1"
        }, 
        {
            "location": "/changelog/#v01-01", 
            "text": "This is the first possible release of Maxwell that might work.  It includes some exceedingly basic kafka support, and JSON output of binlog deltas.", 
            "title": "v0.1: 0.1"
        }
    ]
}